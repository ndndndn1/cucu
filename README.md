# BareMetal-SGEMM

> CUDA Driver APIì™€ Inline PTXë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ SGEMM ë¼ì´ë¸ŒëŸ¬ë¦¬

**ëª©í‘œ: cuBLAS ì„±ëŠ¥ì˜ 90% ì´ìƒ ë‹¬ì„±**

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœí•œ ì½”ë”© ì—°ìŠµì´ ì•„ë‹ˆë¼, NVIDIAì˜ **ì‹œìŠ¤í…œ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´**ë‚˜ **GPU ì•„í‚¤í…íŠ¸**ê°€ ì‹¤ì œë¡œ ìˆ˜í–‰í•˜ëŠ” "í•˜ë“œì›¨ì–´ íŠ¹ì„± ë¶„ì„ ë° ìµœì í™”(Characterization & Optimization)" ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
baremetal-sgemm/
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ cuda_driver_wrapper.hpp   # Driver API ë˜í¼ í´ë˜ìŠ¤
â”‚   â””â”€â”€ sgemm_kernels.hpp         # SGEMM ì»¤ë„ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ driver/
â”‚   â”‚   â”œâ”€â”€ cuda_driver.cpp       # Driver API êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ kernel_launcher.cpp   # ì»¤ë„ ëŸ°ì²˜
â”‚   â”‚   â””â”€â”€ memory_manager.cpp    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ kernels/
â”‚   â”‚   â”œâ”€â”€ sgemm_naive.cu        # Level 0: ê¸°ë³¸ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ sgemm_coalesced.cu    # Level 1: ë©”ëª¨ë¦¬ ì½”ì–¼ë ˆì‹±
â”‚   â”‚   â”œâ”€â”€ sgemm_tiled.cu        # Level 2: ê³µìœ  ë©”ëª¨ë¦¬ íƒ€ì¼ë§
â”‚   â”‚   â”œâ”€â”€ sgemm_register_blocking.cu  # Level 3: ë ˆì§€ìŠ¤í„° ë¸”ë¡œí‚¹
â”‚   â”‚   â”œâ”€â”€ sgemm_double_buffer.cu      # Level 4: ë”ë¸” ë²„í¼ë§
â”‚   â”‚   â””â”€â”€ sgemm_async_copy.cu         # Level 5: cp.async (Ampere+)
â”‚   â””â”€â”€ benchmark/
â”‚       â”œâ”€â”€ main.cpp              # ë²¤ì¹˜ë§ˆí¬ ë©”ì¸
â”‚       â”œâ”€â”€ benchmark_runner.cpp  # ë²¤ì¹˜ë§ˆí¬ ëŸ¬ë„ˆ
â”‚       â”œâ”€â”€ cublas_reference.cu   # cuBLAS ë ˆí¼ëŸ°ìŠ¤
â”‚       â””â”€â”€ test_correctness.cpp  # ì •í™•ì„± í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ptx/                          # ìƒì„±ëœ PTX íŒŒì¼
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build.sh                  # ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ compile_ptx.sh            # PTX ì»´íŒŒì¼ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ANALYSIS_GUIDE.md         # Nsight Compute ë¶„ì„ ê°€ì´ë“œ
â””â”€â”€ CMakeLists.txt
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ìš”êµ¬ì‚¬í•­
- CUDA Toolkit 11.0+ (Ampereì˜ cp.async ê¸°ëŠ¥ ì‚¬ìš©ì‹œ 11.1+)
- CMake 3.18+
- GCC 9+ ë˜ëŠ” Clang 10+
- NVIDIA GPU (Ampere ì´ìƒ ê¶Œì¥)

### ë¹Œë“œ
```bash
# ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
chmod +x scripts/build.sh
./scripts/build.sh Release 80  # SM 8.0 (A100, RTX 30 series)

# ë˜ëŠ” ì§ì ‘ CMake ì‚¬ìš©
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
make -j$(nproc)
```

### ì‹¤í–‰
```bash
# ì „ì²´ ë²¤ì¹˜ë§ˆí¬
./build/sgemm_benchmark -m 4096 -n 4096 -k 4096 -c

# íŠ¹ì • ìµœì í™” ë ˆë²¨ë§Œ í…ŒìŠ¤íŠ¸
./build/sgemm_benchmark -l 5 -c  # AsyncCopyë§Œ

# ì •í™•ì„± í…ŒìŠ¤íŠ¸
./build/sgemm_test

# ë‹¤ì–‘í•œ í¬ê¸° í…ŒìŠ¤íŠ¸
./build/sgemm_benchmark -s -c
```

## ğŸ“Š ìµœì í™” ë‹¨ê³„ë³„ ì„¤ëª…

### Level 0: Naive (ê¸°ë³¸ êµ¬í˜„)
```cuda
// ê° ìŠ¤ë ˆë“œê°€ Cì˜ í•œ ì›ì†Œë¥¼ ê³„ì‚°
C[row][col] = sum(A[row][k] * B[k][col])
```
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~1-5%
- ë¬¸ì œì : ë©”ëª¨ë¦¬ ì½”ì–¼ë ˆì‹± ì—†ìŒ, ë°ì´í„° ì¬ì‚¬ìš© ì—†ìŒ

### Level 1: Coalesced (ë©”ëª¨ë¦¬ ì½”ì–¼ë ˆì‹±)
```cuda
// float4 ë²¡í„° íƒ€ì…ìœ¼ë¡œ 128-bit ë¡œë“œ
float4 a_vec = FETCH_FLOAT4(A[row * lda + k * 4]);
```
- SASS í™•ì¸: `LDG.E.128` ëª…ë ¹ì–´ ìƒì„± í™•ì¸
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~5-15%

### Level 2: Tiled (ê³µìœ  ë©”ëª¨ë¦¬ íƒ€ì¼ë§)
```cuda
__shared__ float As[BLOCK_SIZE][BLOCK_SIZE + 1];  // +1ì€ ë±…í¬ ì¶©ëŒ ë°©ì§€
// íƒ€ì¼ ë¡œë“œ í›„ ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ ì—°ì‚°
```
- SASS í™•ì¸: `LDS.128`, Bank Conflict ë©”íŠ¸ë¦­
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~20-40%

### Level 3: Register Blocking (ë ˆì§€ìŠ¤í„° ë¸”ë¡œí‚¹)
```cuda
// ê° ìŠ¤ë ˆë“œê°€ 8x8 = 64ê°œ ê²°ê³¼ ê³„ì‚°
float rC[TM][TN] = {{0.0f}};
// ë ˆì§€ìŠ¤í„°ì—ì„œ ë°ì´í„° ì¬ì‚¬ìš©
```
- í•µì‹¬: Arithmetic Intensity ì¦ê°€ (0.25 â†’ 2.0 FLOP/byte)
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~50-70%

### Level 4: Double Buffering (ë”ë¸” ë²„í¼ë§)
```cuda
// í˜„ì¬ íƒ€ì¼ ì—°ì‚° ì¤‘ ë‹¤ìŒ íƒ€ì¼ ë¡œë“œ
compute_tile(buffer[i % 2]);
load_tile(buffer[(i+1) % 2], next_tile);
```
- íš¨ê³¼: ë©”ëª¨ë¦¬ ë ˆì´í„´ì‹œ ìˆ¨ê¸°ê¸°
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~70-85%

### Level 5: Async Copy (ë¹„ë™ê¸° ë³µì‚¬, Ampere+)
```cuda
// cp.asyncë¡œ ë ˆì§€ìŠ¤í„° ìš°íšŒí•˜ì—¬ ì§ì ‘ ë³µì‚¬
asm volatile("cp.async.cg.shared.global [%0], [%1], 16;"
             :: "r"(smem_addr), "l"(gmem_ptr));
```
- SASS í™•ì¸: `LDGSTS` ëª…ë ¹ì–´ ìƒì„± í™•ì¸
- ì˜ˆìƒ ì„±ëŠ¥: cuBLASì˜ ~85-95%

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ë¹„êµ

| Matrix Size | Naive | Coalesced | Tiled | RegBlock | DblBuffer | AsyncCopy | cuBLAS |
|-------------|-------|-----------|-------|----------|-----------|-----------|--------|
| 1024Â³       | 50    | 200       | 800   | 2000     | 4000      | 7000      | 8000   |
| 2048Â³       | 60    | 250       | 1000  | 2500     | 5000      | 8500      | 10000  |
| 4096Â³       | 70    | 300       | 1200  | 3000     | 6000      | 9500      | 11000  |

*(ë‹¨ìœ„: GFLOP/s, A100 ê¸°ì¤€ ì˜ˆìƒì¹˜)*

## ğŸ”¬ Nsight Compute ë¶„ì„ ê°€ì´ë“œ

ìì„¸í•œ ë¶„ì„ ë°©ë²•ì€ [`docs/ANALYSIS_GUIDE.md`](docs/ANALYSIS_GUIDE.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### í•µì‹¬ ë©”íŠ¸ë¦­

1. **Speed of Light (SOL)**
   - SM Throughput: ì—°ì‚° ìœ ë‹› í™œìš©ë„
   - Memory Throughput: ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë„

2. **Stall ì›ì¸**
   - Long Scoreboard: ë©”ëª¨ë¦¬ ë°ì´í„° ëŒ€ê¸° â†’ ë²„í¼ë§ í•„ìš”
   - Math Pipe Throttle: ì—°ì‚° í¬í™” â†’ ìµœì  ìƒíƒœ!

3. **SASS ëª…ë ¹ì–´ í™•ì¸**
   - `LDG.E.128`: ë²¡í„°í™”ëœ ê¸€ë¡œë²Œ ë¡œë“œ âœ“
   - `LDGSTS`: cp.async ì‚¬ìš© âœ“
   - `LDL/STL`: ë ˆì§€ìŠ¤í„° ìŠ¤í•„ë§ âœ—

## ğŸ¯ í•™ìŠµ ëª©í‘œ

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ìŠµë“í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ :

1. **Driver API ë§ˆìŠ¤í„°ë¦¬**
   - cuInit, cuCtxCreate, cuModuleLoadDataEx
   - ëŸ°íƒ€ì„ APIì˜ "ë§ˆë²•"ì„ ì´í•´í•˜ê³  ì§ì ‘ ì œì–´

2. **ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡° ì´í•´**
   - ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ì½”ì–¼ë ˆì‹±
   - ê³µìœ  ë©”ëª¨ë¦¬ ë±…í¬ ì¶©ëŒ
   - ë ˆì§€ìŠ¤í„° vs ê³µìœ  ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„

3. **íŒŒì´í”„ë¼ì´ë‹ ê¸°ë²•**
   - Software prefetching
   - Double/Triple buffering
   - Ampere cp.async

4. **ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ ì‚¬ìš©**
   - Nsight Compute SASS ë¶„ì„
   - SOL ë¶„ì„ ë° ë³‘ëª© ì§€ì  íŒŒì•…
   - ë ˆì§€ìŠ¤í„° ì••ë°• ë° ìŠ¤í•„ë§ ê°ì§€

## ğŸ“ ì°¸ê³  ìë£Œ

- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Nsight Compute Documentation](https://docs.nvidia.com/nsight-compute/)
- [CUTLASS: CUDA Templates for Linear Algebra Subroutines](https://github.com/NVIDIA/cutlass)
- [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance](https://siboehm.com/articles/22/CUDA-MMM)

## ğŸ“œ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ í•™ìŠµ ë° ìˆ˜ì •ì— í™œìš©í•˜ì„¸ìš”.

---

*ì´ í”„ë¡œì íŠ¸ëŠ” NVIDIA GPU ì•„í‚¤í…ì²˜ì™€ CUDA ìµœì í™”ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.*
