# 모듈 1: 사전 지식 - GPU 아키텍처

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다:

- GPU와 CPU의 구조적 차이를 설명할 수 있다
- GPU 메모리 계층(레지스터, 공유 메모리, L1/L2 캐시, 글로벌 메모리)의 특성을 구분할 수 있다
- 스레드, 워프, 블록, 그리드의 계층 구조를 이해한다
- SM(Streaming Multiprocessor)의 역할을 설명할 수 있다
- 점유율(Occupancy)의 개념과 영향 요소를 이해한다

---

## 2. 사전 복습

이 모듈은 커리큘럼의 첫 번째 모듈이므로, 사전 복습 대신 아래의 사전 지식 확인 질문으로 대체한다. 각 질문에 자신 있게 답할 수 있어야 이후 내용을 원활하게 따라갈 수 있다.

**사전 지식 확인 질문:**

1. **C++ 포인터**: `float* ptr = new float[1024];`에서 `ptr[512]`가 가리키는 메모리 주소는 `ptr`로부터 몇 바이트 떨어져 있는가?
2. **병렬 처리 개념**: 1000개의 독립적인 덧셈 작업이 있을 때, 코어가 1개인 경우와 100개인 경우의 이론적 속도 차이는 어떻게 되는가?
3. **행렬 개념**: M x K 행렬과 K x N 행렬을 곱했을 때 결과 행렬의 크기는 무엇이며, 결과의 한 원소를 계산하는 데 필요한 곱셈-덧셈 연산 횟수는?

---

## 3. 개념 설명

이 절에서는 CRA(Concrete-Representational-Abstract) 프레임워크와 이중 코딩(Dual Coding) 기법을 적용하여 GPU 아키텍처의 핵심 개념을 단계적으로 설명한다.

### 3.1 CPU vs GPU 비교

#### Concrete (구체적 비유)

CPU는 **스포츠카**와 같다. 개별 차량의 속도는 매우 빠르지만 한 번에 소수의 승객만 태울 수 있다. GPU는 **버스 수백 대로 구성된 대중교통 시스템**과 같다. 개별 버스의 속도는 스포츠카보다 느리지만, 수백 명을 동시에 수송할 수 있다.

행렬곱 연산은 "수만 명의 승객을 동시에 목적지로 보내야 하는 상황"과 유사하다. 스포츠카 몇 대로는 한계가 있지만, 수백 대의 버스가 동시에 움직이면 전체 처리량(throughput)에서 압도적인 차이가 난다.

#### Representational (구조적 표현)

```
=== CPU 구조 ===                     === GPU 구조 ===

+------------------+                 +----+----+----+----+----+----+
|   Core 0 (ALU)   | <-- 강력       | SM | SM | SM | SM | SM | SM |
|   [큰 캐시]      |                +----+----+----+----+----+----+
+------------------+                | SM | SM | SM | SM | SM | SM |
|   Core 1 (ALU)   | <-- 강력       +----+----+----+----+----+----+
|   [큰 캐시]      |                | SM | SM | SM | SM | SM | SM |
+------------------+                +----+----+----+----+----+----+
|   ...            |                | ...  (수십~수백 개 SM)       |
+------------------+                +----+----+----+----+----+----+
|  [매우 큰 L3 캐시]|                |  각 SM 안에 수십~수백 코어    |
+------------------+                +-------------------------------+

코어 수: 8~64개                      코어 수: 수천~수만 개
클럭: ~5 GHz                         클럭: ~2 GHz
특징: 복잡한 제어 로직, 분기 예측     특징: 단순한 코어, 대규모 병렬
```

#### Abstract (추상적 실행 모델)

GPU는 **SIMT(Single Instruction, Multiple Threads)** 실행 모델을 따른다. 하나의 명령어가 다수의 스레드에서 동시에 실행된다. 이는 SIMD(Single Instruction, Multiple Data)의 확장으로, 각 스레드가 독립적인 프로그램 카운터를 가질 수 있지만 하드웨어 수준에서는 **워프(Warp)** 단위(32개 스레드)로 동일한 명령어를 실행한다.

| 특성 | CPU | GPU |
|------|-----|-----|
| 설계 철학 | 지연시간 최소화 (Latency-oriented) | 처리량 최대화 (Throughput-oriented) |
| 코어 수 | 수십 개 (복잡한 코어) | 수천 개 (단순한 코어) |
| 캐시 크기 | 코어당 수 MB | SM당 수백 KB |
| 분기 예측 | 정교한 분기 예측기 | 없음 (워프 divergence 발생) |
| 문맥 전환 | 비용이 큼 | 거의 비용 없음 (하드웨어 스케줄링) |
| 적합한 작업 | 순차적, 분기 많은 작업 | 데이터 병렬 작업 (행렬곱 등) |

---

### 3.2 GPU 메모리 계층

GPU의 메모리는 계층적으로 구성되어 있으며, 상위 계층일수록 빠르고 작고, 하위 계층일수록 크고 느리다. 이 계층 구조를 이해하는 것이 SGEMM 최적화의 핵심이다.

#### 메모리 계층 비교 표

| 메모리 | 지연시간 | 대역폭 | 범위 (Scope) | 크기 (H100 기준) |
|--------|---------|--------|-------------|-----------------|
| 레지스터 | ~1 cycle | ~수십 TB/s | 스레드 전용 | ~256 KB/SM |
| 공유 메모리 | ~20-30 cycles | ~수 TB/s/SM | 블록 내 공유 | ~164 KB/SM (설정 가능) |
| L1 캐시 | ~30 cycles | - | SM 내 | 공유 메모리와 하드웨어 공유 |
| L2 캐시 | ~200 cycles | - | 전체 GPU | ~40 MB |
| 글로벌 메모리 (HBM) | ~400-600 cycles | ~1.5 TB/s | 전체 GPU | ~40-80 GB |

#### 메모리 계층 다이어그램

```
+===========================================================================+
|                            GPU 전체                                       |
|                                                                           |
|  +-----------------------------+    +-----------------------------+       |
|  |          SM 0               |    |          SM 1               |       |
|  |                             |    |                             |       |
|  |  +-------+ +-------+       |    |  +-------+ +-------+       |       |
|  |  |Thread0| |Thread1| ...   |    |  |Thread0| |Thread1| ...   |       |
|  |  |[레지스터]| |[레지스터]|       |    |  |[레지스터]| |[레지스터]|       |       |
|  |  +-------+ +-------+       |    |  +-------+ +-------+       |       |
|  |         |        |          |    |         |        |          |       |
|  |  +-------------------------+|    |  +-------------------------+|       |
|  |  |   공유 메모리 (~164KB)   ||    |  |   공유 메모리 (~164KB)   ||       |
|  |  +-------------------------+|    |  +-------------------------+|       |
|  |  |   L1 캐시               ||    |  |   L1 캐시               ||       |
|  |  +-------------------------+|    |  +-------------------------+|       |
|  +-----------------------------+    +-----------------------------+       |
|                                                                           |
|  +---------------------------------------------------------------------+ |
|  |                     L2 캐시 (~40 MB)                                 | |
|  +---------------------------------------------------------------------+ |
|                                                                           |
|  +---------------------------------------------------------------------+ |
|  |               글로벌 메모리 / HBM (~40-80 GB)                        | |
|  +---------------------------------------------------------------------+ |
+===========================================================================+
```

지연시간의 차이를 직관적으로 이해하자면:

- **레지스터**: 내 주머니에서 물건을 꺼내는 것 (즉시)
- **공유 메모리**: 같은 방에 있는 책상 서랍에서 꺼내는 것 (몇 걸음)
- **L2 캐시**: 같은 건물의 다른 층 창고에서 가져오는 것 (엘리베이터 탑승)
- **글로벌 메모리**: 다른 도시의 물류 창고에서 배송받는 것 (택배 대기)

#### 코드 참조: DeviceInfo 구조체

프로젝트에서 GPU 메모리 관련 하드웨어 속성을 어떻게 조회하는지 확인하자:

```cpp
// include/cuda_driver_wrapper.hpp:54-79
struct DeviceInfo {
    int device_id;
    std::string name;
    int compute_major;
    int compute_minor;
    size_t total_memory;           // 글로벌 메모리 총 크기
    int sm_count;                  // SM 개수
    int max_threads_per_block;
    int max_threads_per_sm;
    int max_registers_per_block;   // 블록당 최대 레지스터
    int max_registers_per_sm;      // SM당 최대 레지스터
    int max_shared_memory_per_block;  // 블록당 최대 공유 메모리
    int max_shared_memory_per_sm;     // SM당 최대 공유 메모리
    int warp_size;                 // 워프 크기 (항상 32)
    int memory_bus_width;          // 메모리 버스 폭
    int memory_clock_rate;         // 메모리 클럭 (kHz)
    int l2_cache_size;             // L2 캐시 크기

    bool supports_async_copy;      // Ampere 이상 여부

    double peak_memory_bandwidth_gb() const {
        // 메모리 대역폭 = 클럭 * 버스폭 * 2(DDR) / 8(bits->bytes)
        return (memory_clock_rate * 1e3 * memory_bus_width * 2.0) / 8.0 / 1e9;
    }
};
```

이 구조체의 각 필드가 메모리 계층 표의 어떤 항목과 대응하는지 주목하라. `total_memory`는 글로벌 메모리, `max_shared_memory_per_sm`은 SM당 공유 메모리, `max_registers_per_sm`은 SM당 레지스터 총량, `l2_cache_size`는 L2 캐시에 해당한다.

---

### 3.3 SM(Streaming Multiprocessor) 구조

SM은 GPU의 기본 연산 단위이다. 하나의 GPU는 수십에서 수백 개의 SM으로 구성되며, 각 SM은 독립적으로 스레드 블록을 실행한다.

#### 워프(Warp) 개념

GPU 실행의 가장 기본적인 단위는 **워프(Warp)**이다. 하나의 워프는 **32개의 스레드**로 구성되며, 이 32개의 스레드는 **록스텝(lockstep)**으로, 즉 동일한 명령어를 동시에 실행한다.

```cpp
// include/sgemm_kernels.hpp:42
constexpr int WARP_SIZE = 32;
```

워프가 중요한 이유:
- GPU 하드웨어는 워프 단위로 명령어를 발급(issue)한다
- 워프 내 분기 발생 시 **워프 다이버전스(warp divergence)**가 일어나 양쪽 경로를 순차 실행한다
- 메모리 접근도 워프 단위로 합쳐(coalesce)져야 최적의 대역폭을 달성한다

#### 워프 스케줄러

각 SM에는 워프 스케줄러가 내장되어 있다. 워프 스케줄러는 매 사이클마다 실행 준비가 된 워프 중 하나를 선택하여 명령어를 발급한다.

```
+------------------------------------------------------+
|                    SM 내부 구조                        |
|                                                      |
|  [워프 스케줄러 0] [워프 스케줄러 1] ...              |
|        |                |                             |
|        v                v                             |
|  +-----------+   +-----------+                        |
|  | 워프 0    |   | 워프 1    |   ...  (활성 워프들)    |
|  | (32 스레드)|   | (32 스레드)|                        |
|  +-----------+   +-----------+                        |
|                                                      |
|  [INT32 유닛] [FP32 유닛] [FP64 유닛] [텐서 코어]     |
|                                                      |
|  [공유 메모리 / L1 캐시]                              |
|  [레지스터 파일]                                      |
+------------------------------------------------------+
```

하나의 워프가 글로벌 메모리 접근 등으로 대기 상태에 놓이면, 워프 스케줄러는 지연 없이 다른 준비된 워프로 즉시 전환한다. 이것이 GPU가 메모리 지연시간을 **은닉(hide)**하는 핵심 메커니즘이다.

---

### 3.4 스레드 계층 구조

GPU의 스레드는 4단계 계층으로 구성된다:

```
Grid (그리드)
  |
  +-- Block (0,0)     Block (1,0)     Block (2,0)   ...
  |     |
  |     +-- Warp 0: Thread  0 ~ Thread 31
  |     +-- Warp 1: Thread 32 ~ Thread 63
  |     +-- Warp 2: Thread 64 ~ Thread 95
  |     +-- ...
  |
  +-- Block (0,1)     Block (1,1)     ...
        |
        +-- Warp 0: Thread  0 ~ Thread 31
        +-- ...
```

| 계층 | 단위 | 특징 |
|------|------|------|
| **Thread (스레드)** | 1개 실행 흐름 | 고유 레지스터, threadIdx로 식별 |
| **Warp (워프)** | 32개 스레드 | 록스텝 실행, 하드웨어 스케줄링 단위 |
| **Block (블록)** | 여러 워프 | 공유 메모리 공유, 블록 내 동기화 가능 |
| **Grid (그리드)** | 여러 블록 | 하나의 커널 호출 전체, 블록 간 독립적 |

#### 코드 참조: LaunchConfig 구조체

```cpp
// include/cuda_driver_wrapper.hpp:295-307
struct LaunchConfig {
    dim3 grid;              // 그리드 크기 (블록의 배치)
    dim3 block;             // 블록 크기 (스레드의 배치)
    size_t shared_mem_bytes = 0;   // 동적 공유 메모리 크기
    CUstream stream = nullptr;

    LaunchConfig(dim3 g, dim3 b, size_t smem = 0, CUstream s = nullptr)
        : grid(g), block(b), shared_mem_bytes(smem), stream(s) {}

    int total_threads() const {
        return grid.x * grid.y * grid.z * block.x * block.y * block.z;
    }
};
```

`dim3 grid`는 그리드의 3차원 크기(블록 수)를, `dim3 block`은 블록의 3차원 크기(스레드 수)를 정의한다. SGEMM에서는 보통 2차원 그리드와 2차원 블록을 사용하여, 결과 행렬의 각 영역을 각 블록이 담당하도록 한다.

`total_threads()` 메서드는 전체 스레드 수를 계산한다: `(grid.x * grid.y * grid.z) * (block.x * block.y * block.z)`. 이 값은 결과 행렬의 원소 수 이상이어야 모든 원소가 계산된다.

---

### 3.5 점유율 (Occupancy)

#### 정의

**점유율(Occupancy)**은 SM에서 동시에 활성화된 워프 수를 해당 SM이 지원하는 최대 워프 수로 나눈 비율이다.

```
점유율 = 활성 워프 수 / SM당 최대 워프 수
```

예를 들어 SM이 최대 64개의 워프를 지원하는데, 현재 32개의 워프만 활성화되어 있다면 점유율은 50%이다.

#### 점유율을 제한하는 세 가지 요소

1. **레지스터 사용량**: 스레드당 사용하는 레지스터가 많을수록 SM에 동시에 올릴 수 있는 스레드(워프)가 줄어든다.
2. **공유 메모리 사용량**: 블록당 사용하는 공유 메모리가 클수록 SM에 동시에 배치할 수 있는 블록이 줄어든다.
3. **블록 크기**: SM당 최대 블록 수와 최대 스레드 수 제한에 의해 점유율이 제한된다.

#### 코드 참조: get_occupancy()

```cpp
// src/driver/kernel_launcher.cpp:137-149
float SgemmLauncher::get_occupancy(int M, int N, int K) const {
    auto config = get_launch_config(M, N, K);
    int block_size = config.block.x * config.block.y * config.block.z;

    int blocks_per_sm = module_->get_max_active_blocks_per_sm(
        kernel_func_, block_size, config.shared_mem_bytes);

    // Calculate theoretical occupancy
    int max_threads_per_sm = ctx_.info().max_threads_per_sm;
    int active_threads = blocks_per_sm * block_size;

    return static_cast<float>(active_threads) / max_threads_per_sm;
}
```

이 함수의 동작 과정:
1. 커널의 실행 설정(그리드, 블록 크기)을 가져온다
2. CUDA API를 통해 SM당 동시에 활성화 가능한 최대 블록 수를 조회한다
3. `활성 스레드 = SM당 블록 수 * 블록당 스레드 수`를 계산한다
4. 이를 `SM당 최대 스레드 수`로 나누어 점유율을 반환한다

#### 중요 통찰: 높은 점유율이 항상 최적은 아니다

이 프로젝트의 분석 가이드에서 명시하고 있듯이:

```
일반적인 오해: 높은 점유율 = 높은 성능

실제:
- 레지스터 blocking은 낮은 점유율로도 높은 성능 달성 가능
- ILP(Instruction-Level Parallelism)가 높으면 25-50% 점유율로 충분
- 메모리 바운드 커널은 높은 점유율이 유리
```
-- docs/ANALYSIS_GUIDE.md:157-163

스레드당 더 많은 레지스터를 사용하면 점유율은 낮아지지만, 각 스레드가 더 많은 데이터를 레지스터에 보관하여 글로벌 메모리 접근을 줄일 수 있다. SGEMM과 같은 연산 집약적(compute-bound) 커널에서는 이 전략이 오히려 더 높은 성능을 달성하는 경우가 많다.

---

## 4. 코드 분석

이 절에서는 프로젝트의 실제 코드를 통해 GPU 하드웨어 속성이 어떻게 조회되고 활용되는지 분석한다.

### DeviceInfo 구조체 분석

```cpp
// include/cuda_driver_wrapper.hpp:54-79
struct DeviceInfo {
    int device_id;
    std::string name;
    int compute_major;              // (1) 컴퓨트 능력 메이저 버전
    int compute_minor;              // (1) 컴퓨트 능력 마이너 버전
    size_t total_memory;            // (2) 글로벌 메모리 총 크기 (bytes)
    int sm_count;                   // (3) SM 개수
    int max_threads_per_block;      // (4) 블록당 최대 스레드 수
    int max_threads_per_sm;         // (5) SM당 최대 스레드 수
    int max_registers_per_block;    // (6) 블록당 최대 레지스터 수
    int max_registers_per_sm;       // (7) SM당 최대 레지스터 수
    int max_shared_memory_per_block;// (8) 블록당 최대 공유 메모리
    int max_shared_memory_per_sm;   // (9) SM당 최대 공유 메모리
    int warp_size;                  // (10) 워프 크기
    int memory_bus_width;           // (11) 메모리 버스 폭 (bits)
    int memory_clock_rate;          // (12) 메모리 클럭 (kHz)
    int l2_cache_size;              // (13) L2 캐시 크기

    bool supports_async_copy;       // (14) 비동기 복사 지원 여부
    ...
};
```

각 필드의 역할과 연결 관계:

**(1) compute_major / compute_minor**: GPU 세대를 나타낸다. Ampere는 8.x, Hopper는 9.x이다. 이 값에 따라 사용 가능한 기능이 달라진다. 예를 들어 `supports_async_copy`는 `compute_major >= 8`일 때 `true`이다. 이는 Ampere 아키텍처에서 도입된 `cp.async` 명령어(공유 메모리로의 비동기 복사)를 지원하는지 여부를 나타낸다.

**(3) sm_count**: GPU의 SM 개수. 블록이 SM에 분배되므로, 이 값이 병렬 처리 능력의 상한을 결정한다.

**(5) max_threads_per_sm**: 점유율 계산의 분모에 해당한다. `get_occupancy()` 함수에서 이 값을 직접 참조한다.

**(6-9) 레지스터/공유 메모리 제한**: 점유율을 제한하는 핵심 자원이다. SM 수준과 블록 수준 모두에서 제한이 적용된다.

### 피크 메모리 대역폭 계산

```cpp
// include/cuda_driver_wrapper.hpp:73-76
double peak_memory_bandwidth_gb() const {
    return (memory_clock_rate * 1e3 * memory_bus_width * 2.0) / 8.0 / 1e9;
}
```

이 계산을 분해하면:

1. `memory_clock_rate * 1e3` -- kHz를 Hz로 변환
2. `* memory_bus_width` -- 버스 폭(bits)을 곱함
3. `* 2.0` -- DDR(Double Data Rate)이므로 실효 전송률 2배
4. `/ 8.0` -- bits를 bytes로 변환
5. `/ 1e9` -- bytes를 GB로 변환

예시: H100의 경우 `memory_clock_rate = 1593000 kHz`, `memory_bus_width = 5120 bits`이면:

```
(1593000 * 1000 * 5120 * 2) / 8 / 1e9 = 약 2039 GB/s
```

이 값은 SGEMM 성능 분석에서 **연산 강도(Arithmetic Intensity)**와 함께 **루프라인 모델(Roofline Model)**을 구성하는 데 사용된다.

### WARP_SIZE와의 연결

```cpp
// include/sgemm_kernels.hpp:42
constexpr int WARP_SIZE = 32;
constexpr int WARPS_PER_BLOCK_M = 4;  // M 방향 워프 수
constexpr int WARPS_PER_BLOCK_N = 2;  // N 방향 워프 수
```

`WARP_SIZE`는 하드웨어가 결정하는 상수이며, `DeviceInfo::warp_size`를 통해 런타임에도 조회할 수 있다. 현재 모든 NVIDIA GPU에서 이 값은 32이다. 커널 코드에서는 이 값을 `constexpr`로 선언하여 컴파일 시점에 최적화가 적용되도록 한다.

`WARPS_PER_BLOCK_M = 4`, `WARPS_PER_BLOCK_N = 2`는 하나의 블록이 M 방향으로 4개, N 방향으로 2개, 총 8개의 워프(= 256개 스레드)를 사용함을 의미한다.

---

## 5. 왜 이것이 작동하는가? (Elaborative Interrogation)

아래 질문들은 단순 암기를 넘어 GPU 아키텍처 설계의 근본적인 이유를 탐구하기 위한 것이다. 각 질문에 대해 스스로 답해 본 후, 제시된 설명을 확인하라.

### 질문 1: 왜 GPU는 큰 캐시 대신 수천 개의 작은 코어를 선택했는가?

CPU는 소수의 스레드가 빠르게 실행되도록 큰 캐시와 복잡한 분기 예측기를 갖추었다. 반면 GPU는 행렬곱, 그래픽 렌더링 등 **데이터 병렬(data-parallel)** 작업에 최적화되었다. 이러한 작업에서는 수만 개의 독립적인 연산이 동시에 발생하므로, 트랜지스터를 캐시에 투자하는 것보다 연산 유닛(코어)에 투자하는 것이 전체 처리량(throughput) 관점에서 더 효율적이다.

또한 GPU는 캐시 대신 **대규모 스레드 병렬성**으로 메모리 지연시간을 은닉한다. 하나의 워프가 메모리를 기다리는 동안 다른 워프가 실행되므로, 개별 메모리 접근의 지연시간이 전체 성능에 미치는 영향이 크게 줄어든다.

### 질문 2: 공유 메모리가 L1 캐시와 같은 하드웨어인데 왜 별도로 관리하는가?

NVIDIA GPU에서 공유 메모리와 L1 캐시는 물리적으로 동일한 SRAM을 공유한다 (용량 배분을 설정할 수 있다). 그럼에도 별도로 관리하는 이유는 **프로그래머의 명시적 제어**를 가능하게 하기 위해서이다.

- **L1 캐시**: 하드웨어가 자동으로 관리. 어떤 데이터가 캐시에 올라가고 내려갈지 프로그래머가 직접 제어할 수 없다.
- **공유 메모리**: 프로그래머가 명시적으로 어떤 데이터를 올릴지, 어떤 패턴으로 접근할지 결정한다.

SGEMM 최적화에서는 글로벌 메모리의 타일을 공유 메모리로 명시적으로 로드한 후 반복 재사용하는 **타일링(tiling)** 기법이 핵심이다. 이 경우 캐시의 자동 관리보다 프로그래머의 명시적 관리가 훨씬 효율적이다.

### 질문 3: 워프 크기가 정확히 32인 이유는 무엇인가?

워프 크기 32는 하드웨어 설계의 여러 절충안의 결과이다:

- **메모리 합치기(Coalescing)**: 32개 스레드가 연속된 4바이트 float를 접근하면 정확히 128바이트 = 1개의 캐시 라인이 된다. 이는 메모리 버스의 효율적 활용에 최적이다.
- **SIMT 효율성**: 워프가 너무 크면 다이버전스 발생 시 낭비가 커지고, 너무 작으면 스케줄링 오버헤드가 커진다. 32는 이 사이의 적절한 균형점이다.
- **하드웨어 구현**: 32비트 마스크로 워프 내 각 스레드의 활성 상태를 표현할 수 있어 하드웨어 구현이 효율적이다.

### 질문 4: 점유율 100%가 항상 최적이 아닌 이유는?

점유율을 높이려면 스레드당 자원(레지스터, 공유 메모리)을 줄여야 한다. 그런데 SGEMM처럼 **레지스터 블로킹(register blocking)**을 활용하는 커널에서는 스레드당 많은 레지스터를 사용하여 중간 결과를 유지하는 것이 성능에 결정적이다.

- 점유율 25%이지만 스레드당 128개 레지스터를 사용하여 8x8 타일을 레지스터에 유지하는 커널이, 점유율 100%이지만 스레드당 32개 레지스터만 사용하여 매번 공유 메모리에서 데이터를 읽어오는 커널보다 훨씬 빠를 수 있다.
- 이는 높은 **ILP(Instruction-Level Parallelism)**가 워프 수준 병렬성(TLP)을 보상할 수 있기 때문이다.

---

## 6. 시뮬레이션 1: Thread-to-Hardware 매핑

이 시뮬레이션을 통해 소프트웨어 스레드가 하드웨어에 어떻게 매핑되는지 직접 계산해 본다.

### 주어진 조건

- 행렬 크기: 256 x 256
- 스레드 블록 크기: 16 x 16 (= 256 스레드/블록)
- 각 스레드가 결과 행렬의 원소 하나를 계산한다고 가정
- GPU: SM 108개 (예: A100)

### 단계 1: 그리드 크기 계산

```
grid.x = ceil(256 / 16) = 16
grid.y = ceil(256 / 16) = 16
그리드 크기: (16, 16)
```

### 단계 2: 전체 블록 수

```
전체 블록 수 = grid.x * grid.y = 16 * 16 = 256 블록
```

### 단계 3: 블록당 워프 수

```
블록당 스레드 = 16 * 16 = 256
블록당 워프 수 = 256 / 32 = 8 워프
```

### 단계 4: SM 배정

```
SM 수: 108
전체 블록: 256

초기 배정: 256 / 108 = 2.37...
--> 대부분의 SM에 2개 블록 배정
--> 일부 SM에 3개 블록 배정

정확한 분배:
  - 108 * 2 = 216 블록을 108개 SM에 2개씩
  - 나머지 40 블록을 40개 SM에 추가 1개씩
  --> 40개 SM: 3 블록, 68개 SM: 2 블록
```

### 단계 5: 블록-SM 매핑 표 (처음 10개 블록)

| 블록 ID | 블록 좌표 (bx, by) | 배정 SM (라운드 로빈 가정) | 블록 내 워프 수 |
|---------|-------------------|--------------------------|----------------|
| 0 | (0, 0) | SM 0 | 8 |
| 1 | (1, 0) | SM 1 | 8 |
| 2 | (2, 0) | SM 2 | 8 |
| 3 | (3, 0) | SM 3 | 8 |
| 4 | (4, 0) | SM 4 | 8 |
| 5 | (5, 0) | SM 5 | 8 |
| 6 | (6, 0) | SM 6 | 8 |
| 7 | (7, 0) | SM 7 | 8 |
| 8 | (8, 0) | SM 8 | 8 |
| 9 | (9, 0) | SM 9 | 8 |

참고: 실제 하드웨어에서 블록-SM 매핑 순서는 드라이버가 결정하며 반드시 라운드 로빈이 아닐 수 있다. 위 표는 이해를 위한 단순화이다.

### 단계 6: SM당 점유율 추정

```
SM당 블록 2개인 경우:
  활성 스레드 = 2 * 256 = 512
  활성 워프 = 512 / 32 = 16
  점유율 = 16 / 64 = 25% (SM당 최대 64 워프 가정)

SM당 블록 3개인 경우:
  활성 스레드 = 3 * 256 = 768
  활성 워프 = 768 / 32 = 24
  점유율 = 24 / 64 = 37.5%
```

이 결과는 256x256 행렬이 108개 SM GPU에 비해 상대적으로 작아서 점유율이 낮다는 것을 보여준다. 더 큰 행렬(4096x4096)을 사용하면 블록 수가 65536개로 늘어나 SM당 훨씬 많은 블록이 배정될 수 있다.

---

## 7. 핵심 정리

1. **CPU는 지연시간 최적화, GPU는 처리량 최적화** 설계이다. GPU는 수천 개의 단순한 코어로 데이터 병렬 작업에서 압도적인 성능을 낸다.

2. **GPU 메모리 계층은 5단계**로 구성된다: 레지스터(~1 cycle) > 공유 메모리(~20-30 cycles) > L1 캐시(~30 cycles) > L2 캐시(~200 cycles) > 글로벌 메모리(~400-600 cycles). 상위 계층을 최대한 활용하는 것이 최적화의 핵심이다.

3. **워프(32스레드)는 GPU 실행의 기본 단위**이다. 워프 내 스레드는 동일한 명령어를 록스텝으로 실행하며, 워프 스케줄러가 지연시간을 은닉한다.

4. **스레드 계층은 Thread -> Warp -> Block -> Grid**이다. 블록 내에서는 공유 메모리와 동기화가 가능하고, 블록 간에는 독립적으로 실행된다.

5. **SM(Streaming Multiprocessor)**은 블록을 실행하는 하드웨어 단위이다. 각 SM은 자체 레지스터 파일, 공유 메모리, 워프 스케줄러를 가진다.

6. **점유율은 SM의 자원 활용도**를 나타내며, 레지스터 사용량, 공유 메모리 사용량, 블록 크기에 의해 제한된다.

7. **높은 점유율이 항상 최적 성능을 의미하지 않는다.** 레지스터 블로킹 등으로 ILP를 높이면 낮은 점유율에서도 높은 성능을 달성할 수 있다.

---

## 8. 퀴즈

### 문제 1 (Remember)

**질문**: GPU에서 하나의 워프를 구성하는 스레드 수는 몇 개인가?

<details>
<summary>정답 보기</summary>

**32개**. 이 값은 모든 NVIDIA GPU에서 동일하며, `include/sgemm_kernels.hpp:42`에서 `constexpr int WARP_SIZE = 32;`로 정의되어 있다.

</details>

---

### 문제 2 (Remember)

**질문**: 글로벌 메모리(HBM) 접근의 지연시간은 약 몇 사이클인가?

<details>
<summary>정답 보기</summary>

**약 400-600 사이클**. 이는 레지스터 접근(~1 사이클) 대비 수백 배 느리며, 이 지연시간을 줄이는 것이 SGEMM 최적화의 핵심 동기이다.

</details>

---

### 문제 3 (Understand)

**질문**: 레지스터와 공유 메모리의 범위(scope) 차이를 설명하라.

<details>
<summary>정답 보기</summary>

**레지스터**는 **스레드 전용(thread-private)**이다. 하나의 스레드만이 자신의 레지스터를 읽고 쓸 수 있으며, 다른 스레드는 접근할 수 없다.

**공유 메모리**는 **블록 내 공유(block-shared)**이다. 같은 블록에 속한 모든 스레드가 동일한 공유 메모리 공간을 읽고 쓸 수 있다. 이를 통해 블록 내 스레드 간 데이터 교환이 가능하다.

</details>

---

### 문제 4 (Understand)

**질문**: GPU가 CPU보다 행렬곱 연산에 유리한 이유를 두 가지 이상 서술하라.

<details>
<summary>정답 보기</summary>

1. **높은 데이터 병렬성**: 행렬곱의 각 결과 원소는 독립적으로 계산할 수 있다. M x N 결과 행렬의 모든 원소를 동시에 계산하는 것이 가능하며, GPU의 수천 개 코어가 이를 병렬로 처리한다.

2. **높은 연산 대 메모리 비율**: 행렬곱의 연산 강도(Arithmetic Intensity)는 O(N^3) 연산 / O(N^2) 데이터로, 행렬이 클수록 연산 비중이 높아진다. GPU는 높은 부동소수점 처리량을 가지고 있어 이러한 연산 집약적 작업에 적합하다.

3. **메모리 접근 패턴**: 적절한 타일링을 적용하면 행렬곱의 메모리 접근이 규칙적이고 예측 가능하여, GPU의 메모리 합치기(coalescing) 메커니즘을 최대한 활용할 수 있다.

</details>

---

### 문제 5 (Apply)

**질문**: 4096 x 4096 행렬에 대해 16 x 16 스레드 블록을 사용할 경우, 필요한 그리드 크기(dim3)는?

<details>
<summary>정답 보기</summary>

```
grid.x = 4096 / 16 = 256
grid.y = 4096 / 16 = 256
그리드 크기: dim3(256, 256)
총 블록 수: 256 * 256 = 65,536 블록
총 스레드 수: 65,536 * 256 = 16,777,216 스레드
```

</details>

---

### 문제 6 (Apply)

**질문**: SM당 최대 레지스터 수가 65,536개이고, 커널이 스레드당 128개의 레지스터를 사용한다면, SM당 동시에 실행 가능한 최대 스레드 수는?

<details>
<summary>정답 보기</summary>

```
SM당 최대 스레드 수 (레지스터 기준) = 65,536 / 128 = 512 스레드
이는 512 / 32 = 16 워프에 해당한다.
```

만약 SM의 하드웨어 최대 스레드 수가 2048이라면, 레지스터 제한으로 인해 실제로는 512 스레드만 활성화 가능하다. 이 경우 점유율은 `512 / 2048 = 25%`이다.

</details>

---

### 문제 7 (Analyze)

**질문**: 어떤 SGEMM 커널의 점유율이 25%인데도 높은 성능을 보이는 경우가 있다. 이 현상의 원인을 분석하라.

<details>
<summary>정답 보기</summary>

점유율이 25%로 낮지만 높은 성능을 보이는 주된 이유는 **레지스터 블로킹에 의한 높은 ILP(Instruction-Level Parallelism)**이다.

- 스레드당 많은 레지스터를 사용하여 결과 행렬의 큰 타일(예: 8x8)을 레지스터에 유지한다.
- 이를 통해 매 반복마다 64개의 독립적인 FMA(Fused Multiply-Add) 연산을 발급할 수 있다.
- 이 FMA 연산들은 서로 의존성이 없으므로, 파이프라인에서 동시에 실행될 수 있다.
- 결과적으로 워프 수준 병렬성(TLP)이 낮아도 명령어 수준 병렬성(ILP)이 이를 보상한다.

또한 레지스터에 데이터를 유지함으로써 공유 메모리와 글로벌 메모리 접근 횟수가 줄어들어, 메모리 대역폭 병목이 완화된다.

ANALYSIS_GUIDE.md:157-163에서 언급하듯, 이는 "높은 점유율 = 높은 성능"이라는 일반적인 오해를 깨는 대표적인 사례이다.

</details>

---

### 문제 8 (Understand)

**질문**: `DeviceInfo` 구조체에서 `supports_async_copy`가 `compute_major >= 8`일 때 `true`가 되는 이유는 무엇인가?

<details>
<summary>정답 보기</summary>

`compute_major >= 8`은 **Ampere 아키텍처 이상**을 의미한다 (Ampere = 8.x, Hopper = 9.x).

Ampere 아키텍처에서 `cp.async` 명령어가 도입되었다. 이 명령어는 글로벌 메모리에서 공유 메모리로의 데이터 복사를 **비동기적으로** 수행하여, 데이터가 전송되는 동안 스레드가 다른 연산을 계속 수행할 수 있게 한다.

이전 아키텍처(Volta = 7.x 등)에서는 글로벌 메모리 데이터를 먼저 레지스터로 읽어온 뒤 다시 공유 메모리에 저장하는 2단계 과정이 필요했다. `cp.async`는 이 과정을 하드웨어 수준에서 1단계로 줄여 레지스터 사용량도 절약하고 지연시간 은닉도 개선한다.

따라서 이 플래그를 확인함으로써, 커널 구현에서 `cp.async`를 활용한 최적화 경로를 선택적으로 사용할 수 있다.

</details>

---

## 9. 다음 단계 미리보기

이 모듈에서는 GPU 아키텍처의 하드웨어 구조와 메모리 계층을 학습했다. 이제 이 하드웨어 위에서 실제로 코드를 작성하고 실행하는 방법을 배울 차례이다.

**모듈 2: CUDA 프로그래밍 기본**에서는 다음을 다룬다:

- CUDA 커널 함수의 작성과 호출 방법 (`__global__`, `<<<grid, block>>>`)
- 스레드 인덱스 계산 (`threadIdx`, `blockIdx`, `blockDim`)
- GPU 메모리 할당과 호스트-디바이스 간 데이터 전송 (`cudaMalloc`, `cudaMemcpy`)
- 첫 번째 나이브(naive) SGEMM 커널 구현

모듈 1에서 배운 "블록은 SM에 매핑되고, 블록 내 스레드는 워프 단위로 실행된다"는 개념이 실제 코드에서 어떻게 구체화되는지 확인하게 될 것이다. 특히 나이브 커널의 성능을 측정한 후, 왜 이 커널이 이론적 피크 대비 낮은 성능을 보이는지를 이 모듈에서 학습한 메모리 계층 관점으로 분석하는 것이 모듈 2의 핵심 학습 활동이다.
