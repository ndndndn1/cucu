# 모듈 4: Level 0 -- 나이브 SGEMM 커널 구현

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다.

| 목표 | Bloom 수준 |
|------|-----------|
| 나이브 SGEMM 커널의 코드를 라인별로 해석할 수 있다 | 이해(Understand) |
| 스레드-원소 매핑 방식을 설명할 수 있다 | 이해(Understand) |
| 이 커널이 느린 3가지 이유를 구별할 수 있다 | 분석(Analyze) |
| 실행 구성(launch configuration)을 계산할 수 있다 | 적용(Apply) |

---

## 2. 사전 복습

다음 3개 문항을 **보지 않고** 답한 뒤 모듈 1-3의 내용과 대조해 본다.

1. SGEMM에서 M x N 출력 행렬을 계산할 때 총 FLOP 수의 공식은?
2. Row-major 저장 방식에서 2차원 배열 원소 `A[i][j]`의 1차원 메모리 주소 수식은?
3. 글로벌 메모리(DRAM)의 접근 지연시간은 대략 몇 사이클인가?

<details>
<summary>정답 확인</summary>

1. `2 * M * N * K` (K번의 곱셈과 K번의 덧셈이 M*N개 원소 각각에 대해 발생)
2. `base_addr + (i * num_cols + j) * sizeof(element)`, 즉 SGEMM에서는 `A + i * lda + j`
3. 약 400-800 사이클 (GPU 아키텍처에 따라 다름)

</details>

---

## 3. 개념 설명

### 3.1 1스레드 = 1원소 전략

나이브 SGEMM 커널의 핵심 아이디어는 단순하다. 출력 행렬 C의 원소 하나를 스레드 하나가 전담한다.

**Concrete (구체적 비유)**

100명의 학생이 100x100 크기의 곱셈표를 만든다고 하자. 각 학생이 표의 한 칸을 담당하면 총 10,000명의 학생이 필요하다. 각 학생은 자기 칸에 해당하는 행과 열을 쭉 훑으며 곱해서 더한다. 다른 학생과 협력할 필요가 없으므로 구현은 간단하지만, 같은 행이나 열을 여러 학생이 중복해서 읽는 비효율이 발생한다.

**Representational (표상적 -- 다이어그램)**

```
Grid (블록들의 2D 배열)
+--------+--------+--------+
| Block  | Block  | Block  |
|(0,0)   |(1,0)   |(2,0)   |
+--------+--------+--------+
| Block  | Block  | Block  |
|(0,1)   |(1,1)   |(2,1)   |
+--------+--------+--------+

Block (bx, by) 내부의 스레드 배치:
+----+----+----+----+
|t0,0|t1,0|t2,0|... | ty=0
+----+----+----+----+
|t0,1|t1,1|t2,1|... | ty=1
+----+----+----+----+
|... |... |... |... |
+----+----+----+----+

스레드 (tx, ty)가 담당하는 C 원소:

       col = bx * blockDim.x + tx
        |
        v
row --> C[row][col]
 = by * blockDim.y + ty
```

**Abstract (추상적 -- 코드)**

```cuda
const int row = blockIdx.y * blockDim.y + threadIdx.y;
const int col = blockIdx.x * blockDim.x + threadIdx.x;
```

이 두 줄이 스레드의 글로벌 인덱스를 결정한다. `blockIdx`는 그리드 내에서 블록의 위치, `threadIdx`는 블록 내에서 스레드의 위치, `blockDim`은 블록의 크기(여기서는 16x16)이다.

### 3.2 내적(Dot Product) 계산

각 스레드는 A의 한 행(row)과 B의 한 열(col)에 대한 내적을 계산한다.

```
C[row][col] = sum_{k=0}^{K-1} A[row][k] * B[k][col]
```

이를 코드로 표현하면 K번의 곱셈-누적(multiply-accumulate) 반복이다.

```cuda
float sum = 0.0f;
for (int k = 0; k < K; ++k) {
    sum += A[row * lda + k] * B[k * ldb + col];
}
```

K=4096이면 한 스레드가 4096번의 곱셈과 4096번의 덧셈, 총 8192회의 부동소수점 연산을 수행한다.

---

## 4. 코드 분석 (완전 워크스루)

소스 파일: `src/kernels/sgemm_naive.cu`

전체 커널 코드를 아래에 복사하고, 각 부분을 라인별로 해설한다.

```cuda
extern "C" __global__ void sgemm_naive(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    const int M, const int N, const int K,
    const int lda, const int ldb, const int ldc,
    const float alpha, const float beta)
{
    const int row = blockIdx.y * blockDim.y + threadIdx.y;
    const int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k) {
            sum += A[row * lda + k] * B[k * ldb + col];
        }
        C[row * ldc + col] = alpha * sum + beta * C[row * ldc + col];
    }
}
```

### 라인 20-31: 함수 시그니처

```cuda
extern "C" __global__ void sgemm_naive(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    const int M, const int N, const int K,
    const int lda, const int ldb, const int ldc,
    const float alpha, const float beta)
```

| 키워드 | 의미 |
|--------|------|
| `extern "C"` | C++ 이름 맹글링(name mangling)을 비활성화한다. CUDA Driver API로 커널을 로드할 때 PTX/cubin에서 심볼 이름이 그대로 `sgemm_naive`가 되어야 하므로 필요하다. |
| `__global__` | 이 함수가 GPU에서 실행되는 커널(kernel)임을 선언한다. 호스트(CPU)에서 호출하고, 디바이스(GPU)에서 실행된다. |
| `__restrict__` | 포인터 A, B, C가 서로 메모리 영역이 겹치지 않음(별칭 없음, no aliasing)을 컴파일러에게 보장한다. 이를 통해 컴파일러가 로드/스토어 순서를 더 자유롭게 최적화할 수 있다. |

매개변수 설명:

| 매개변수 | 의미 |
|----------|------|
| `A` | 입력 행렬 A의 포인터 (M x K) |
| `B` | 입력 행렬 B의 포인터 (K x N) |
| `C` | 출력 행렬 C의 포인터 (M x N) |
| `M, N, K` | 행렬 차원: C(M x N) = A(M x K) * B(K x N) |
| `lda, ldb, ldc` | Leading dimension. Row-major에서 각 행렬의 열 수를 의미한다. 부분 행렬(submatrix) 연산을 지원하기 위해 별도 매개변수로 전달한다. |
| `alpha, beta` | BLAS 표준 스케일링 계수: `C = alpha * A * B + beta * C` |

### 라인 35-36: 글로벌 인덱스 계산

```cuda
const int row = blockIdx.y * blockDim.y + threadIdx.y;
const int col = blockIdx.x * blockDim.x + threadIdx.x;
```

이 두 줄이 스레드-원소 매핑의 핵심이다.

- `blockIdx.y`: 현재 블록이 그리드에서 y축 몇 번째에 위치하는가 (행 방향)
- `blockDim.y`: 블록의 y축 크기 (여기서는 16)
- `threadIdx.y`: 블록 내에서 현재 스레드의 y축 위치 (0~15)

예시: Block(3, 7)의 Thread(5, 11)인 경우
- `row = 7 * 16 + 11 = 123`
- `col = 3 * 16 + 5 = 53`
- 이 스레드는 `C[123][53]`을 계산한다.

### 라인 38: 경계 검사

```cuda
if (row < M && col < N) {
```

행렬 크기가 블록 크기(16)의 배수가 아닐 수 있다. 예를 들어 M=1000이면 `ceil(1000/16) = 63`개의 블록이 필요하고, 마지막 블록의 일부 스레드는 `row >= 1000`이 된다. 이런 스레드는 유효한 C 원소가 없으므로 아무 작업도 하지 않아야 한다.

경계 검사가 없으면 행렬 범위 밖의 메모리에 접근하여 **잘못된 결과**나 **세그멘테이션 폴트**가 발생한다.

### 라인 40: 축적 변수 초기화

```cuda
float sum = 0.0f;
```

내적 결과를 누적할 레지스터 변수를 0으로 초기화한다. 이 변수는 레지스터에 저장되므로 접근 속도가 가장 빠르다(1 사이클).

### 라인 42-45: K 루프 -- 내적 계산

```cuda
for (int k = 0; k < K; ++k) {
    // A[row, k] * B[k, col]
    sum += A[row * lda + k] * B[k * ldb + col];
}
```

이것이 커널의 핵심 연산 루프이다.

- `A[row * lda + k]`: A 행렬의 row번째 행, k번째 열 원소. Row-major 저장이므로 1차원 인덱스는 `row * lda + k`이다.
- `B[k * ldb + col]`: B 행렬의 k번째 행, col번째 열 원소. 1차원 인덱스는 `k * ldb + col`이다.
- `sum += ...`: 곱셈-누적 연산(FMA, Fused Multiply-Add). 하드웨어적으로 하나의 FMA 명령어로 실행될 수 있다.

K=4096이면 이 루프가 4096번 반복되며, 매 반복마다 글로벌 메모리에서 2개의 float를 로드하고 1번의 FMA를 수행한다.

### 라인 48: alpha/beta 스케일링 및 결과 저장

```cuda
C[row * ldc + col] = alpha * sum + beta * C[row * ldc + col];
```

BLAS 표준 SGEMM 공식 `C = alpha * A * B + beta * C`를 구현한다.

- `alpha * sum`: 계산된 내적에 alpha를 곱한다.
- `beta * C[row * ldc + col]`: 기존 C 값에 beta를 곱한다.
- 두 결과를 더하여 C에 저장한다.

`alpha=1.0, beta=0.0`이면 단순히 `C = A * B`가 된다.

### 구체적 실행 추적: 4x4 행렬 예제

이해를 돕기 위해 아주 작은 크기의 행렬로 실행 과정을 추적한다.

**설정**: A = 4x4, B = 4x4, C = 4x4, block = 2x2 (설명을 위해 축소)

```
블록 배치: grid = dim3(2, 2) = 4개 블록

         col 0  col 1  col 2  col 3
        +------+------+------+------+
row 0   |      Block(0,0)    | Block(1,0)    |
row 1   |                    |               |
        +------+------+------+------+
row 2   |      Block(0,1)    | Block(1,1)    |
row 3   |                    |               |
        +------+------+------+------+
```

**Block(0,0)의 스레드들이 계산하는 C 원소:**

| 스레드 | tx | ty | row | col | 계산 대상 |
|--------|----|----|-----|-----|----------|
| Thread(0,0) | 0 | 0 | 0 | 0 | C[0][0] |
| Thread(1,0) | 1 | 0 | 0 | 1 | C[0][1] |
| Thread(0,1) | 0 | 1 | 1 | 0 | C[1][0] |
| Thread(1,1) | 1 | 1 | 1 | 1 | C[1][1] |

**Thread(0,0) in Block(0,0)의 K 루프 상세 추적**:

이 스레드는 `row=0, col=0`이므로 `C[0][0]`을 계산한다. `lda = ldb = 4`이다.

```
k=0: A[0 * 4 + 0] * B[0 * 4 + 0] = A[0]  * B[0]
k=1: A[0 * 4 + 1] * B[1 * 4 + 0] = A[1]  * B[4]
k=2: A[0 * 4 + 2] * B[2 * 4 + 0] = A[2]  * B[8]
k=3: A[0 * 4 + 3] * B[3 * 4 + 0] = A[3]  * B[12]
```

메모리 주소 관점에서 보면:

```
A 행렬 (row-major, 4x4):
인덱스: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]
 행 0:   A00 A01 A02 A03
 행 1:                   A10 A11 A12 A13
 행 2:                                   A20 A21 A22  A23
 행 3:                                                      A30  A31  A32  A33

Thread(0,0)은 A[0], A[1], A[2], A[3]을 읽는다 -> 연속 주소 (행 0 전체)

B 행렬 (row-major, 4x4):
인덱스: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15]
 행 0:   B00 B01 B02 B03
 행 1:                   B10 B11 B12 B13
 행 2:                                   B20 B21 B22  B23
 행 3:                                                      B30  B31  B32  B33

Thread(0,0)은 B[0], B[4], B[8], B[12]를 읽는다 -> ldb(=4)만큼 떨어진 주소 (열 0)
```

### 실행 구성 (Launch Configuration)

소스 파일: `src/driver/kernel_launcher.cpp:75-81`

```cpp
case OptLevel::Naive:
    block = dim3(BLOCK_SIZE, BLOCK_SIZE);  // BLOCK_SIZE = 16
    grid = dim3((N + BLOCK_SIZE - 1) / BLOCK_SIZE,
               (M + BLOCK_SIZE - 1) / BLOCK_SIZE);
    break;
```

| 항목 | 값 | 설명 |
|------|-----|------|
| `block` | `dim3(16, 16)` | 256 스레드/블록 |
| `grid.x` | `ceil(N / 16)` | N 방향(열) 블록 수 |
| `grid.y` | `ceil(M / 16)` | M 방향(행) 블록 수 |

M=N=4096일 때의 계산:

```
block = dim3(16, 16)          = 256 스레드/블록
grid  = dim3(4096/16, 4096/16)
      = dim3(256, 256)        = 65,536 블록
총 스레드 수 = 256 * 65,536   = 16,777,216 스레드
출력 원소 수 = 4096 * 4096    = 16,777,216 원소
```

총 스레드 수와 출력 원소 수가 정확히 일치한다. 이것이 "1스레드 = 1원소" 전략의 본질이다.

---

## 5. 왜 이것이 작동하는가? + 왜 느린가?

커널은 정확한 결과를 낸다. 그러나 cuBLAS 대비 1-5%에 불과한 성능이다. 왜 이렇게 느린가? 세 가지 근본적인 문제가 있다.

### 문제 1: 메모리 코얼레싱(Coalescing) 부재

코얼레싱이란 동일 워프의 32개 스레드가 연속된 메모리 주소에 접근할 때, 하드웨어가 이를 하나의 메모리 트랜잭션으로 묶어서 처리하는 것이다.

16x16 블록에서 워프는 어떻게 구성되는가? CUDA에서 워프는 연속된 `threadIdx.x` 값을 가진 32개 스레드로 구성된다. 16x16 블록에서 첫 번째 워프는 `(tx=0..15, ty=0)`과 `(tx=0..15, ty=1)`의 32개 스레드이다.

**B 행렬 접근 패턴 (비교적 양호):**

K 루프의 특정 반복 `k`에서 같은 워프의 스레드들이 B에서 접근하는 주소를 살펴보자.

```
Thread(tx=0, ty=0): B[k * ldb + 0]   -> 주소: B + k*ldb + 0
Thread(tx=1, ty=0): B[k * ldb + 1]   -> 주소: B + k*ldb + 1
Thread(tx=2, ty=0): B[k * ldb + 2]   -> 주소: B + k*ldb + 2
...
Thread(tx=15,ty=0): B[k * ldb + 15]  -> 주소: B + k*ldb + 15
```

이 16개 스레드는 연속된 주소에 접근한다. 같은 128바이트 캐시 라인 내에 위치하므로 코얼레싱이 된다.

**A 행렬 접근 패턴 (문제):**

같은 워프에서 A에 접근하는 패턴을 보자. 워프의 처음 16개 스레드(`ty=0`)는 모두 같은 row를 가지므로 같은 주소를 읽는다(브로드캐스트 가능). 그러나 워프의 나머지 16개 스레드(`ty=1`)는 다른 row를 가진다.

```
Thread(tx=0, ty=0): A[row0 * lda + k]  -> 주소: A + row0*lda + k
Thread(tx=0, ty=1): A[row1 * lda + k]  -> 주소: A + row1*lda + k

row0 != row1이면 두 주소의 차이 = lda * sizeof(float) = lda * 4 바이트
```

lda=4096이면 주소 차이가 `4096 * 4 = 16,384` 바이트이다. 캐시 라인 크기(128바이트)보다 훨씬 크므로 별도의 메모리 트랜잭션이 필요하다.

코드 내 분석 주석(`src/kernels/sgemm_naive.cu:56-60` 참조):

```
A: Threads in the same warp access different rows
     -> Non-coalesced reads (bad!)
B: Threads in the same warp access the same column but different rows
     -> Strided access pattern (bad!)
```

### 문제 2: 데이터 재사용 부재

코드 내 분석 주석(`src/kernels/sgemm_naive.cu:62-64` 참조):

- `A[row][k]`는 같은 row를 가진 **모든 col에 대해** 다시 로드된다. 즉, N번 중복 로드된다.
- `B[k][col]`는 같은 col을 가진 **모든 row에 대해** 다시 로드된다. 즉, M번 중복 로드된다.

M=N=K=4096일 때:

```
A의 한 원소가 로드되는 총 횟수: N = 4096번
B의 한 원소가 로드되는 총 횟수: M = 4096번
```

공유 메모리나 레지스터에 캐싱하지 않기 때문에, 매번 느린 글로벌 메모리에서 다시 읽어야 한다. 이것은 엄청난 메모리 대역폭 낭비이다.

### 문제 3: 낮은 산술 강도(Arithmetic Intensity)

산술 강도는 전송된 바이트당 수행하는 부동소수점 연산 수(FLOP/byte)이다.

나이브 커널에서 K 루프 한 반복의 산술 강도를 계산해 보자.

```
연산: 1 FMA = 2 FLOP (곱셈 1회 + 덧셈 1회)
로드: A에서 float 1개(4바이트) + B에서 float 1개(4바이트) = 8바이트

산술 강도 = 2 FLOP / 8 bytes = 0.25 FLOP/byte
```

이 수치는 극히 낮다. A100 GPU의 루프라인 모델(Roofline Model)에서 연산 바운드가 되려면 약 `19,500 GFLOP/s / 2,039 GB/s = ~9.6 FLOP/byte` 이상이 필요하다. 0.25 FLOP/byte는 이에 한참 못 미치므로, 이 커널은 **완전히 메모리 바운드(memory bound)** 이다. 연산 유닛은 대부분의 시간을 메모리에서 데이터가 도착하기를 기다리며 놀고 있다.

### Nsight Compute 예상 프로파일링 결과

이 커널을 Nsight Compute로 프로파일링하면 다음과 같은 결과를 예상할 수 있다.

| 지표 | 예상 값 | 의미 |
|------|---------|------|
| SM SOL (Speed Of Light) | ~5% | SM 연산 유닛 활용률이 극히 낮다 |
| Memory SOL | ~20% | 메모리 대역폭도 효율적으로 사용하지 못한다 |
| 주요 스톨 원인 | Long Scoreboard > 80% | 글로벌 메모리 로드 완료를 기다리는 시간이 지배적이다 |
| SASS 명령어 | `LDG.E` (32-bit loads) | 32비트(4바이트) 단위로 로드하여 대역폭 활용률이 낮다 |

SM과 Memory SOL이 모두 낮으면서 Long Scoreboard 스톨이 높은 것은 **레이턴시 바운드(latency bound)** 상태를 나타낸다. 메모리 대역폭 자체를 포화시키지도 못하고, 연산도 하지 못하고, 그저 데이터 도착을 기다리고 있는 상태이다.

---

## 6. 시뮬레이션 4: 메모리 접근 패턴 시각화

이 시뮬레이션에서는 종이 위에 메모리 접근 패턴을 직접 그려보고, 코얼레싱 여부를 판별한다.

### 설정

8x8 행렬, 블록 크기 16x16 (실제로는 블록 1개로 전체 행렬을 커버). lda = ldb = 8로 설정한다.

워프 0은 처음 32개 스레드로 구성된다. 16x16 블록에서 워프 0은 다음과 같다.

```
워프 0의 스레드 배치 (16x16 블록에서):
tx:  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
ty=0: T0 T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T15
ty=1: T16 T17 T18 T19 T20 T21 T22 T23 T24 T25 T26 T27 T28 T29 T30 T31
```

### B 행렬 접근 패턴 (k=0일 때)

```
B 행렬 메모리 레이아웃 (8x8, row-major):
주소:    0    1    2    3    4    5    6    7
행 0: B[0] B[1] B[2] B[3] B[4] B[5] B[6] B[7]    <- 캐시 라인 A (32B) | 캐시 라인 B (32B)

주소:    8    9   10   11   12   13   14   15
행 1: B[8] B[9] B[10] B[11] B[12] B[13] B[14] B[15]

...이하 생략...
```

k=0일 때 각 스레드의 B 접근:

```
Thread  tx  ty  col  접근 주소           캐시 라인
------  --  --  ---  -----------------   ---------
T0       0   0    0  B[0*8 + 0] = B[0]   Line 0
T1       1   0    1  B[0*8 + 1] = B[1]   Line 0
T2       2   0    2  B[0*8 + 2] = B[2]   Line 0
T3       3   0    3  B[0*8 + 3] = B[3]   Line 0
T4       4   0    4  B[0*8 + 4] = B[4]   Line 0
T5       5   0    5  B[0*8 + 5] = B[5]   Line 0
T6       6   0    6  B[0*8 + 6] = B[6]   Line 0
T7       7   0    7  B[0*8 + 7] = B[7]   Line 0
T8       8   0    8  -- (col=8 >= N=8, 경계 밖)
...
T16      0   1    0  B[0*8 + 0] = B[0]   Line 0  (T0과 동일)
T17      1   1    1  B[0*8 + 1] = B[1]   Line 0  (T1과 동일)
...
```

B에 대해: ty=0인 스레드들은 `B[0]`~`B[7]`에 연속 접근한다. 이것은 코얼레싱이 가능하다. ty=1인 스레드들도 같은 k 값에서 같은 row의 B 원소를 접근하므로, 이미 캐시에 있는 데이터를 읽을 수 있다(브로드캐스트).

**결론: B 접근은 코얼레싱된다.**

### A 행렬 접근 패턴 (k=0일 때)

```
Thread  tx  ty  row  접근 주소             캐시 라인
------  --  --  ---  -------------------   ---------
T0       0   0    0  A[0*8 + 0] = A[0]    Line 0
T1       1   0    0  A[0*8 + 0] = A[0]    Line 0  (T0과 동일! -> 브로드캐스트)
T2       2   0    0  A[0*8 + 0] = A[0]    Line 0  (동일)
...
T15     15   0    0  A[0*8 + 0] = A[0]    Line 0  (동일)
T16      0   1    1  A[1*8 + 0] = A[8]    Line 1  (다른 행!)
T17      1   1    1  A[1*8 + 0] = A[8]    Line 1  (T16과 동일)
...
T31     15   1    1  A[1*8 + 0] = A[8]    Line 1  (동일)
```

A에 대해: 같은 ty를 가진 16개 스레드는 모두 같은 주소를 읽는다(브로드캐스트). 그러나 ty=0과 ty=1은 서로 `lda * sizeof(float)` = `8 * 4 = 32`바이트만큼 떨어진 주소에 접근한다. 이 예제에서는 32바이트 차이이므로 두 번째 캐시 라인이 필요하다.

실제 커널에서 lda=4096이면 차이가 `4096 * 4 = 16,384`바이트가 되어, 128바이트 캐시 라인 기준으로 완전히 별개의 캐시 라인이다.

### 트랜잭션 수 비교

32바이트 섹터(sector) 단위로 계산할 때:

| 행렬 | k=0에서 워프 0의 접근 | 필요한 섹터 수 | 비고 |
|------|----------------------|---------------|------|
| B | B[0]~B[7], 연속 32바이트 | 1 섹터 | 코얼레싱: 효율적 |
| A | A[0]과 A[8], 32바이트 간격 | 2 섹터 | 비연속: 비효율적 |

실제 4096x4096 행렬에서는 A의 접근이 16,384바이트 간격이 되므로, 워프 하나가 A를 읽기 위해 2개의 서로 먼 캐시 라인을 요청해야 한다. 이런 비효율이 K 루프 전체에 걸쳐 반복된다.

---

## 7. 핵심 정리

- **1 스레드 = 1 출력 원소** 전략을 사용하며, 블록 크기는 16x16 (256 스레드)이다.
- `row = blockIdx.y * blockDim.y + threadIdx.y`, `col = blockIdx.x * blockDim.x + threadIdx.x`로 글로벌 인덱스를 계산한다.
- 각 스레드는 K번의 곱셈-누적으로 A의 한 행과 B의 한 열의 내적을 계산한다.
- **3가지 성능 문제**:
  1. **코얼레싱 부재**: A 행렬 접근이 워프 내에서 비연속적이다.
  2. **재사용 부재**: 같은 데이터를 수천 번 글로벌 메모리에서 중복 로드한다.
  3. **낮은 산술 강도**: 0.25 FLOP/byte로 완전히 메모리 바운드이다.
- cuBLAS 대비 **1-5% 성능**에 불과하다.
- 이 커널은 **출발점**이지 목표가 아니다. 이후 모듈에서 단계적으로 최적화해 나간다.

---

## 8. 퀴즈

### Q1. (Remember)
나이브 커널에서 한 스레드가 계산하는 C 원소의 수는?

<details>
<summary>정답</summary>
1개. 1스레드 = 1원소 전략을 사용한다.
</details>

### Q2. (Understand)
`blockIdx.y`와 `threadIdx.y`의 역할을 각각 설명하시오.

<details>
<summary>정답</summary>

- `blockIdx.y`: 그리드 내에서 현재 블록의 y축 위치(행 방향 블록 인덱스)를 나타낸다. 어떤 블록이 행렬의 어느 행 범위를 담당하는지를 결정한다.
- `threadIdx.y`: 블록 내에서 현재 스레드의 y축 위치를 나타낸다. 블록이 담당하는 행 범위 내에서 구체적으로 몇 번째 행을 이 스레드가 계산하는지를 결정한다.
- 둘을 결합하여 `row = blockIdx.y * blockDim.y + threadIdx.y`로 글로벌 행 인덱스를 계산한다.

</details>

### Q3. (Apply)
M=1024, N=1024, 블록 크기 16x16일 때 총 스레드 수를 계산하시오.

<details>
<summary>정답</summary>

```
grid = dim3(1024/16, 1024/16) = dim3(64, 64) = 4,096 블록
스레드/블록 = 16 * 16 = 256
총 스레드 수 = 4,096 * 256 = 1,048,576
```

이것은 출력 행렬 C의 원소 수(1024 * 1024 = 1,048,576)와 동일하다.
</details>

### Q4. (Analyze)
같은 워프에서 ty가 다른 Thread(tx=0, ty=0)과 Thread(tx=0, ty=1)이 A에서 접근하는 주소의 차이는?

<details>
<summary>정답</summary>

`lda * sizeof(float)` 바이트.

- Thread(tx=0, ty=0): `A[row0 * lda + k]`, 여기서 `row0 = blockIdx.y * 16 + 0`
- Thread(tx=0, ty=1): `A[row1 * lda + k]`, 여기서 `row1 = blockIdx.y * 16 + 1`
- 주소 차이: `(row1 - row0) * lda * sizeof(float) = 1 * lda * 4` 바이트
- lda=4096이면: `4096 * 4 = 16,384` 바이트

이 간격은 128바이트 캐시 라인보다 훨씬 크므로 코얼레싱이 불가능하다.
</details>

### Q5. (Analyze)
데이터 재사용 관점에서 이 커널의 산술 강도가 낮은 이유를 설명하시오.

<details>
<summary>정답</summary>

각 스레드가 K 루프를 독립적으로 실행하면서, 같은 A 행과 B 열의 원소를 다른 스레드와 공유하지 않고 매번 글로벌 메모리에서 새로 로드한다. 구체적으로:

- A[row][k]는 같은 row를 가진 모든 col에 대해(N번) 중복 로드된다.
- B[k][col]는 같은 col을 가진 모든 row에 대해(M번) 중복 로드된다.

만약 공유 메모리에 타일을 캐싱하면 다수의 스레드가 같은 데이터를 재사용할 수 있어 실제 글로벌 메모리 전송량이 줄어들고, 산술 강도가 높아진다. 나이브 커널에는 이런 재사용 메커니즘이 없으므로, 루프 한 반복당 2 FLOP / 8 bytes = 0.25 FLOP/byte라는 매우 낮은 산술 강도가 된다.
</details>

### Q6. (Understand)
`__restrict__` 키워드의 목적은 무엇인가?

<details>
<summary>정답</summary>

`__restrict__`는 해당 포인터가 가리키는 메모리 영역이 다른 포인터의 메모리 영역과 겹치지 않음(별칭 없음, no aliasing)을 컴파일러에게 보장한다. 이를 통해 컴파일러는:

1. 로드/스토어 순서를 더 자유롭게 재배치할 수 있다.
2. 불필요한 재로드를 제거할 수 있다.
3. 더 공격적인 최적화(예: 명령어 스케줄링)를 수행할 수 있다.

예를 들어, `__restrict__`가 없으면 컴파일러는 C에 쓴 값이 A나 B의 다음 읽기에 영향을 줄 수 있다고 보수적으로 가정해야 한다.
</details>

### Q7. (Apply)
K=4096일 때 한 스레드의 총 글로벌 메모리 로드 횟수를 계산하시오.

<details>
<summary>정답</summary>

K 루프 내에서 매 반복마다:
- A에서 1회 로드: `A[row * lda + k]`
- B에서 1회 로드: `B[k * ldb + col]`

K=4096이므로: `2 * 4096 = 8,192`회의 글로벌 메모리 로드.

추가로 라인 48에서 C를 읽고 쓰므로 +2회(로드 1회, 스토어 1회)가 있지만, K 루프에 비하면 무시할 수 있는 수준이다.

총 로드 횟수: 약 8,194회 (K 루프 8,192회 + C 읽기 1회 + C 쓰기 1회).
</details>

### Q8. (Evaluate)
이 커널이 cuBLAS의 1-5%에 불과한 근본 이유 3가지를 요약하시오.

<details>
<summary>정답</summary>

1. **메모리 코얼레싱 부재**: 같은 워프의 스레드들이 A 행렬에 접근할 때 lda만큼 떨어진 비연속 주소를 읽어 다수의 메모리 트랜잭션이 발생한다. 이는 메모리 대역폭의 효율적 활용을 방해한다.

2. **데이터 재사용 부재**: 같은 A, B 원소가 수천 번씩 글로벌 메모리에서 중복 로드된다. 공유 메모리나 레지스터에 캐싱하지 않으므로, 매번 400-800 사이클의 글로벌 메모리 레이턴시를 지불한다.

3. **낮은 산술 강도**: 0.25 FLOP/byte로 완전히 메모리 바운드이다. GPU의 연산 유닛은 대부분 유휴 상태이며, 메모리에서 데이터가 도착하기를 기다리는 Long Scoreboard 스톨이 전체 실행 시간의 80% 이상을 차지한다. cuBLAS는 타일링, 레지스터 블로킹, 비동기 복사 등을 통해 산술 강도를 수십 FLOP/byte 이상으로 끌어올린다.
</details>

---

## 9. 다음 단계 미리보기

이 모듈에서는 나이브 커널의 구조와 세 가지 성능 병목을 이해했다. 특히 SASS에서 `LDG.E`(32비트 로드)만 사용되어 메모리 대역폭을 비효율적으로 활용하고 있음을 확인했다.

다음 모듈(Level 1: 메모리 코얼레싱)에서는 **float4 벡터화 로드**를 도입하여 한 번의 메모리 명령으로 128비트(16바이트)를 로드하고, 메모리 접근 패턴을 코얼레싱에 유리하게 재배치한다. 이를 통해 메모리 대역폭 활용도를 개선하고, SASS에서 `LDG.E.128` 명령어가 생성되는 것을 확인할 것이다.

---

*이 문서의 소스 코드 참조: `src/kernels/sgemm_naive.cu`, `src/driver/kernel_launcher.cpp:75-81`*
