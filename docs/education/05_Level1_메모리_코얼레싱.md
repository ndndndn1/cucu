# 모듈 5: Level 1 -- 메모리 코얼레싱과 벡터화 로드

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다.

1. 메모리 코얼레싱(Memory Coalescing)의 개념과 GPU 성능에 미치는 영향을 설명할 수 있다.
2. `float4` 벡터화 로드의 원리와 동작 방식을 이해한다.
3. `FETCH_FLOAT4` 매크로를 분석하고, SASS 코드에서 `LDG.E.128` 명령어를 식별할 수 있다.
4. B 행렬 접근 패턴이 여전히 비효율적인 이유를 메모리 트랜잭션 관점에서 분석할 수 있다.

---

## 2. 사전 복습

이전 모듈(모듈 1, 4)에서 배운 내용을 능동적으로 떠올려 보자. 답을 먼저 생각한 뒤
확인한다.

1. **워프(Warp)의 크기는 몇 개의 스레드인가?**
2. **나이브 커널(Level 0)이 느린 3가지 이유는 무엇인가?**
3. **GPU 글로벌 메모리의 이론적 대역폭은 대략 얼마인가?** (A100 기준)

> **힌트**: (1) 32개, (2) 비코얼레싱 접근 / 데이터 재사용 없음 / 낮은 산술 강도,
> (3) A100 HBM2e 기준 약 2TB/s

---

## 3. 개념 설명

### 3.1 메모리 코얼레싱 개념

#### Concrete (일상 비유)

32명의 사람이 자판기 앞에 서 있다고 상상하자. 1번부터 32번 슬롯에 들어 있는
음료를 **순서대로** 요청하면, 기계는 컨베이어 벨트를 한 번 돌려 한꺼번에 모든
음료를 내보낼 수 있다. 그러나 각 사람이 **랜덤한 슬롯**을 요청하면, 기계는
슬롯을 하나하나 찾아서 따로 처리해야 한다.

GPU의 메모리 시스템도 정확히 같은 원리이다. 워프 내 32개 스레드가 **연속된
주소**를 요청하면 하드웨어가 한 번에 처리하지만, 흩어진 주소를 요청하면 각각
별도의 트랜잭션이 필요하다.

#### Representational (다이어그램)

아래 다이어그램은 워프 내 32개 스레드가 메모리에 접근하는 두 가지 패턴을
비교한다.

```
[코얼레싱된 접근 -- 1 트랜잭션]

  스레드:    T0   T1   T2   T3   T4   ...  T31
  주소:     0x00 0x04 0x08 0x0C 0x10  ...  0x7C
            |____|____|____|____|____|...|____|
                         |
                  128바이트 연속 영역
                         |
            =============================
            |  하나의 메모리 트랜잭션   |
            =============================


[비코얼레싱 접근 -- 최대 32 트랜잭션]

  스레드:    T0     T1     T2     T3     ...
  주소:     0x400  0x004  0x800  0x108   ...
            |      |      |      |
            v      v      v      v
            각각 별도의 캐시 라인 접근
            ====   ====   ====   ====
            |T1|   |T2|   |T3|   |T4|   ...
            ====   ====   ====   ====
            (최대 32개의 독립 트랜잭션)
```

핵심은 **32-byte 캐시 섹터** 단위이다. GPU의 L1 캐시는 128-byte 캐시 라인을
4개의 32-byte 섹터로 분할하여 관리한다. 워프의 메모리 요청이 동일한 섹터들에
집중될수록 필요한 트랜잭션 수가 줄어든다.

```
128-byte 캐시 라인 구조:

|--- 섹터 0 ---|--- 섹터 1 ---|--- 섹터 2 ---|--- 섹터 3 ---|
| 32 bytes     | 32 bytes     | 32 bytes     | 32 bytes     |
| (8 floats)   | (8 floats)   | (8 floats)   | (8 floats)   |

- 코얼레싱: 32 스레드 x 4 bytes = 128 bytes = 4 섹터 = 1 캐시 라인 요청
- 비코얼레싱: 각 스레드가 서로 다른 캐시 라인에 접근하면 최대 32개 요청
```

#### Abstract (추상적 정의)

**메모리 코얼레싱**: 워프 내 연속 스레드(thread 0, 1, 2, ..., 31)가 연속 메모리
주소에 접근하면, 하드웨어가 이를 하나의 넓은 메모리 트랜잭션으로 **병합(coalesce)**
한다. 이를 통해 메모리 대역폭 활용률이 극대화된다.

수식으로 표현하면:

```
코얼레싱 조건:
  Thread i가 접근하는 주소 = base_addr + i * sizeof(element)

이상적인 트랜잭션 수:
  트랜잭션 = ceil(32 * sizeof(element) / 32)  (32-byte 섹터 기준)

  float(4 bytes)의 경우:
  트랜잭션 = ceil(32 * 4 / 32) = 4 섹터 = 1 캐시 라인 요청
```

### 3.2 float4 벡터화

`float4`는 CUDA가 제공하는 **벡터 타입**으로, 4개의 `float` 값을 하나의
구조체로 묶는다. 크기는 16바이트(128비트)이다.

```
float4 구조:
+-------+-------+-------+-------+
|   x   |   y   |   z   |   w   |
| 4B    | 4B    | 4B    | 4B    |
+-------+-------+-------+-------+
|<------------- 16 bytes ------->|
                128 bits
```

**FETCH_FLOAT4 매크로**는 이 벡터화 로드를 간결하게 수행하기 위한 도구이다.

```cuda
#define FETCH_FLOAT4(pointer) (reinterpret_cast<const float4*>(&(pointer))[0])
```

이 매크로를 단계별로 분해하면:

1. `&(pointer)` -- `pointer` 변수의 주소를 구한다. 여기서 `pointer`는 배열
   원소(예: `A[row * lda + k * 4]`)이므로, 해당 원소의 메모리 주소가 된다.
2. `reinterpret_cast<const float4*>(...)` -- 그 주소를 `float4*` 포인터로
   재해석한다. 즉, "이 주소부터 16바이트를 `float4`로 읽겠다"는 의미이다.
3. `[0]` -- 포인터 역참조. `float4` 값 하나를 로드한다.

결과적으로, 하나의 `float` 주소에서 시작하여 **연속된 4개의 float를 한 번에
128비트로 로드**한다.

**SASS 레벨에서의 차이**:

```
스칼라 로드 (32-bit):     LDG.E     Rd, [Ra]         // 4 bytes
벡터화 로드 (128-bit):    LDG.E.128 Rd, [Ra]         // 16 bytes (4x 더 효율적)
```

`LDG.E.128`이 SASS에서 관찰되면, 벡터화 로드가 성공적으로 생성된 것이다.
만약 `LDG.E`(32비트 로드)만 보인다면, 정렬(alignment) 문제가 있거나 컴파일러가
벡터화에 실패한 것이므로 반드시 원인을 조사해야 한다.

### 3.3 정렬(Alignment) 요구사항

`float4` 로드가 `LDG.E.128`으로 생성되려면 **16-byte 정렬**이 필요하다. 즉,
로드 대상 주소가 16의 배수여야 한다.

```
정렬된 주소 (16-byte aligned):
  주소 = 0x0000, 0x0010, 0x0020, ...  (하위 4비트가 0)
  -> LDG.E.128 하나로 로드 가능

비정렬 주소 (unaligned):
  주소 = 0x0004, 0x0014, ...  (하위 4비트가 0이 아님)
  -> 두 개의 캐시 라인에 걸침 -> 2개 트랜잭션으로 분할
```

이 프로젝트에서는 `memory_manager.cpp`에서 `ALIGNMENT_BYTES = 256`으로 설정하여
메모리 할당 시 256-byte 정렬을 보장한다.
(`src/driver/memory_manager.cpp:21`)

```cpp
constexpr size_t ALIGNMENT_BYTES = 256;  // For optimal coalescing
```

256-byte 정렬은 16-byte 정렬의 상위 호환이므로, `float4` 로드의 정렬 조건을
자동으로 만족한다. 또한 행렬의 리딩 디멘션(leading dimension)도 정렬에 맞추어
조정된다 (`memory_manager.cpp:47-51`).

---

## 4. 코드 분석 (Worked Example)

대상 파일: `src/kernels/sgemm_coalesced.cu`

### 4.1 sgemm_coalesced 커널 (라인별 워크스루)

**라인 23: FETCH_FLOAT4 매크로 정의**

```cuda
#define FETCH_FLOAT4(pointer) (reinterpret_cast<const float4*>(&(pointer))[0])
```

이 매크로는 커널 전체에서 128-bit 벡터화 로드를 생성하는 핵심 도구이다.
3.2절에서 분석한 것처럼, 주어진 주소에서 연속된 4개 float를 한 번에 로드한다.

**라인 25-36: 커널 시그니처**

```cuda
extern "C" __global__ void sgemm_coalesced(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    const int M, const int N, const int K,
    const int lda, const int ldb, const int ldc,
    const float alpha, const float beta)
```

나이브 커널과 동일한 시그니처이다. `__restrict__`는 포인터 간 별칭이 없음을
컴파일러에 알려 최적화를 가능하게 한다.

**라인 39-40: 스레드-원소 매핑**

```cuda
const int row = blockIdx.y * blockDim.y + threadIdx.y;
const int col = blockIdx.x * blockDim.x + threadIdx.x;
```

나이브 커널과 동일하게, 각 스레드가 출력 행렬 C의 하나의 원소를 담당한다.

**라인 47-48: 벡터화 루프 준비**

```cuda
const int K_vec = K / 4;
const int K_rem = K % 4;
```

K 차원을 4개씩 묶어 처리하기 위한 준비이다. `K_vec`는 4개씩 처리하는 반복 횟수,
`K_rem`는 나머지 원소 수이다. 예를 들어 K=1024이면 `K_vec=256`, `K_rem=0`이다.

**라인 52-54: A 행렬에서 float4 벡터화 로드**

```cuda
for (int k = 0; k < K_vec; ++k) {
    float4 a_vec = FETCH_FLOAT4(A[row * lda + k * 4]);
```

이것이 이 커널의 **핵심 최적화**이다. `A[row * lda + k * 4]`는 A 행렬에서
`row`번째 행의 `k * 4`번째 열 원소를 가리킨다. FETCH_FLOAT4 매크로는 이 위치에서
4개의 연속 원소(`k*4`, `k*4+1`, `k*4+2`, `k*4+3`)를 **한 번의 128-bit 로드**로
가져온다.

```
A 행렬 (Row-major):
                  k*4   k*4+1  k*4+2  k*4+3
row ->  [ ... |  a0  |  a1  |  a2  |  a3  | ... ]
               |<---- float4 한 번에 로드 ---->|
                      LDG.E.128
```

이 로드는 A 행렬이 Row-major로 저장되어 있기 때문에 가능하다. 같은 행의 연속
열은 메모리에서 연속 배치되므로, 16-byte 정렬만 만족하면 하나의 `LDG.E.128`
명령으로 처리된다.

**라인 59-62: B 행렬에서 스칼라 로드 (4개)**

```cuda
float b0 = B[(k * 4 + 0) * ldb + col];
float b1 = B[(k * 4 + 1) * ldb + col];
float b2 = B[(k * 4 + 2) * ldb + col];
float b3 = B[(k * 4 + 3) * ldb + col];
```

B 행렬에서는 여전히 **개별 스칼라 로드**(각각 32-bit)를 4번 수행한다. 왜
float4를 사용하지 못하는가?

B[k][col]에서 연속 4개 k값에 대한 접근은 `B[k*4][col]`, `B[k*4+1][col]`,
`B[k*4+2][col]`, `B[k*4+3][col]`이다. Row-major에서 이들의 메모리 주소를
계산하면:

```
B[(k*4+0) * ldb + col] = base + (k*4+0) * ldb + col
B[(k*4+1) * ldb + col] = base + (k*4+1) * ldb + col   (ldb만큼 점프!)
B[(k*4+2) * ldb + col] = base + (k*4+2) * ldb + col   (ldb만큼 또 점프!)
B[(k*4+3) * ldb + col] = base + (k*4+3) * ldb + col
```

ldb가 1024라면, 연속된 접근 사이의 간격이 1024 * 4 = 4096바이트이다. 이들은
메모리에서 연속되지 않으므로 **float4로 묶을 수 없다**.

단, 같은 k 값에 대해 워프 내 스레드들(서로 다른 col)의 접근 패턴을 보면:

```
Thread 0: B[k * ldb + col0]
Thread 1: B[k * ldb + col1]    (col1 = col0 + 1)
Thread 2: B[k * ldb + col2]    (col2 = col0 + 2)
...
```

이 경우 같은 행(k)에서 연속된 열에 접근하므로, **워프 단위로는 코얼레싱이
발생한다**. 개별 스레드의 시간적 연속성은 없지만, 워프 내 공간적 연속성은
있다는 점을 구분해야 한다.

**라인 64-67: 4개 곱셈-누적**

```cuda
sum += a_vec.x * b0;
sum += a_vec.y * b1;
sum += a_vec.z * b2;
sum += a_vec.w * b3;
```

float4의 4개 성분(x, y, z, w)을 각각 B의 4개 원소와 곱하여 누적한다.
컴파일러는 이를 4개의 FMA(Fused Multiply-Add) 명령으로 최적화할 수 있다.

**라인 71-73: 나머지 처리**

```cuda
for (int k = K_vec * 4; k < K; ++k) {
    sum += A[row * lda + k] * B[k * ldb + col];
}
```

K가 4의 배수가 아닌 경우, 나머지 원소들은 일반적인 스칼라 연산으로 처리한다.
K=1024처럼 4의 배수이면 이 루프는 실행되지 않는다.

**라인 76: 결과 저장**

```cuda
C[row * ldc + col] = alpha * sum + beta * C[row * ldc + col];
```

SGEMM의 표준 공식 `C = alpha * A * B + beta * C`를 적용하여 결과를 저장한다.

### 4.2 sgemm_coalesced_bt 커널 (전치 B 버전, 라인 86-127)

B가 전치된 형태(BT)로 제공될 때 사용하는 버전이다.

```cuda
extern "C" __global__ void sgemm_coalesced_bt(
    const float* __restrict__ A,
    const float* __restrict__ BT,  // B transposed: BT[N][K]
    float* __restrict__ C,
    ...
    const int ldbt,  // leading dimension of BT (= K)
    ...)
```

핵심 차이점은 **라인 107-111**에 있다:

```cuda
for (int k = 0; k < K_vec; ++k) {
    float4 a_vec = FETCH_FLOAT4(A[row * lda + k * 4]);
    float4 b_vec = FETCH_FLOAT4(BT[col * ldbt + k * 4]);
```

B가 전치되면 BT[col][k]의 형태가 되므로, k 차원이 **행 방향으로 연속 배치**
된다. 따라서 `BT[col * ldbt + k * 4]`에서 연속 4개 원소도 메모리에서 연속이며,
**A와 B 모두 float4 벡터화 로드가 가능**해진다.

```
원본 B (Row-major):
  B[k][col] -> 행이 k, 열이 col
  -> k를 바꾸면 ldb만큼 점프 (비연속!)

전치 BT (Row-major):
  BT[col][k] -> 행이 col, 열이 k
  -> k를 바꾸면 1만큼 이동 (연속!)
  -> float4 벡터화 가능!
```

**라인 113-116**: 두 float4 값의 성분별 곱셈-누적

```cuda
sum += a_vec.x * b_vec.x;
sum += a_vec.y * b_vec.y;
sum += a_vec.z * b_vec.z;
sum += a_vec.w * b_vec.w;
```

A와 B 모두 LDG.E.128로 로드되므로, 메모리 접근 효율이 크게 향상된다.

---

## 5. 왜 이것이 작동하는가?

### 질문 1: float4 로드가 LDG.E.128을 생성하는 이유는?

하드웨어 관점에서, `float4`는 128비트(16바이트) 크기의 데이터 타입이다.
CUDA 컴파일러(nvcc)는 `float4*` 포인터를 통한 로드를 인식하면, 32비트 로드 4번
대신 128비트 로드 1번을 생성한다. 이는 PTX 수준에서 `ld.global.v4.f32`로
표현되고, 최종 SASS에서 `LDG.E.128`로 변환된다.

이 하나의 128비트 로드는 메모리 컨트롤러에서 하나의 요청으로 처리되므로, 4개의
독립적인 32비트 로드보다 **명령어 오버헤드가 1/4**로 줄어든다. 같은 양의
데이터를 전송하되, 요청 횟수가 줄어드는 것이다.

### 질문 2: B 행렬 접근이 완전히 코얼레싱되지 않는 이유는?

B[k][col]에서 하나의 스레드가 k를 0, 1, 2, 3으로 바꾸면, 주소가 `ldb * 4`
바이트(예: 4096바이트)씩 점프한다. 이렇게 큰 간격의 접근은 서로 다른 캐시
라인에 흩어지며, 하나의 float4로 묶는 것이 불가능하다.

다만, 같은 k 값에 대해 **워프 내 서로 다른 스레드들**은 각각 다른 col에
접근한다. 이 경우 `B[k * ldb + col0]`, `B[k * ldb + col1]`, ... 로 연속
주소가 되므로 **워프 단위 코얼레싱은 발생한다**. 문제는 하나의 스레드 내에서
시간적으로 연속된 4개의 B 접근을 벡터화할 수 없다는 점이다.

### 질문 3: 이 최적화만으로 cuBLAS의 5-15%밖에 안 되는 이유는?

핵심 문제는 **데이터 재사용이 여전히 없다**는 것이다. 이 커널에서 A 행렬의
한 원소는 해당 행의 모든 출력 원소에 필요하고, B 행렬의 한 원소도 해당 열의
모든 출력 원소에 필요하다. 그러나 각 스레드가 독립적으로 글로벌 메모리에서
데이터를 로드하므로, **같은 데이터가 여러 스레드에 의해 중복 로드**된다.

산술 강도를 계산하면:

```
한 출력 원소 C[i][j] 계산:
  - 연산: 2K FLOP (K번의 곱셈 + K번의 덧셈)
  - 메모리: (K + K) * 4 bytes = 8K bytes (A의 한 행 + B의 한 열)
  - 산술 강도 = 2K / 8K = 0.25 FLOP/byte

A100 기준:
  - 메모리 대역폭: ~2TB/s
  - 연산 처리량: ~19.5 TFLOP/s
  - 밸런스 포인트: 19500 / 2000 = 9.75 FLOP/byte

0.25 vs 9.75 -> 약 39배 부족!
```

산술 강도가 밸런스 포인트에 비해 극도로 낮으므로, 메모리 대역폭에 의해 성능이
제한된다. 벡터화 로드로 대역폭 **활용률**은 높였지만, 대역폭 **요구량** 자체가
과도하게 높은 근본적 문제가 남아 있다.

### 질문 4: 전치된 B를 사용하면 더 나은 이유는?

`sgemm_coalesced_bt` 커널에서는 A와 B 모두 float4 벡터화 로드가 가능하다.
이는 두 가지 이점을 제공한다:

1. **명령어 수 감소**: B의 4개 스칼라 로드가 1개의 128-bit 로드로 대체된다.
   전체 글로벌 메모리 로드 명령어 수가 약 40% 감소한다.
2. **메모리 요청 효율 증가**: 128-bit 로드는 32-bit 로드 대비 요청당 전송량이
   4배이므로, 메모리 컨트롤러의 부담이 줄어든다.

그러나 근본적인 데이터 재사용 문제는 여전히 해결되지 않으므로, 성능 개선 폭은
제한적이다. 진정한 도약은 **공유 메모리를 활용한 타일링**(Level 2)에서 달성된다.

---

## 6. 시뮬레이션 5: 코얼레싱 패턴 검사기

이 시뮬레이션에서는 `sgemm_coalesced` 커널의 메모리 접근 패턴을 직접 추적하여,
A와 B의 코얼레싱 상태를 비교한다.

### 설정

- 행렬 크기: M=N=K=1024
- 리딩 디멘션: lda = ldb = ldc = 1024
- 블록 크기: blockDim = (16, 16) (blockDim.x = 16, blockDim.y = 16)
- 워프 크기: 32 스레드
- 워프 구성: ty = 0..1, tx = 0..15 (2행 x 16열 = 32 스레드가 하나의 워프)
- 블록 (0,0)의 첫 번째 워프를 분석한다

```
워프 내 스레드 배치 (blockDim.x=16, blockDim.y=16 기준):

  워프 0: threadIdx.y = 0..1, threadIdx.x = 0..15

      tx:  0    1    2    3    4    5   ...  15
  ty=0:  T00  T01  T02  T03  T04  T05  ...  T15
  ty=1:  T16  T17  T18  T19  T20  T21  ...  T31

  -> 각 스레드의 (row, col):
     T00: (0, 0),  T01: (0, 1),  ..., T15: (0, 15)
     T16: (1, 0),  T17: (1, 1),  ..., T31: (1, 15)
```

### Case A: A 행렬 접근 패턴 분석

k=0일 때 FETCH_FLOAT4를 통한 접근:

```
A[row * lda + k * 4]  (k=0일 때)

T00 (row=0): A[0 * 1024 + 0] = A[0]      -> 주소 0x0000
T01 (row=0): A[0 * 1024 + 0] = A[0]      -> 주소 0x0000  (같은 행!)
T02 (row=0): A[0 * 1024 + 0] = A[0]      -> 주소 0x0000  (같은 행!)
...
T15 (row=0): A[0 * 1024 + 0] = A[0]      -> 주소 0x0000  (같은 행!)
T16 (row=1): A[1 * 1024 + 0] = A[1024]   -> 주소 0x1000
T17 (row=1): A[1 * 1024 + 0] = A[1024]   -> 주소 0x1000  (같은 행!)
...
T31 (row=1): A[1 * 1024 + 0] = A[1024]   -> 주소 0x1000  (같은 행!)
```

**분석**: 같은 ty를 가진 스레드들(같은 row)은 **동일한 주소**에서 float4를
로드한다. 이는 **브로드캐스트(broadcast)** 패턴으로, 캐시에서 한 번 로드하면
동일한 데이터를 여러 스레드에 전달할 수 있다. 충돌이 아니라 효율적인 패턴이다.

그러나 ty=0과 ty=1의 접근 주소(0x0000과 0x1000)는 4096바이트 떨어져 있어 서로
다른 캐시 라인에 위치한다.

```
A 행렬 접근 트랜잭션 수:
  - ty=0 그룹 (16 스레드): 같은 주소 -> 1 트랜잭션 (float4 = 16B, 1 섹터)
  - ty=1 그룹 (16 스레드): 같은 주소 -> 1 트랜잭션
  - 총: 2 트랜잭션 / 워프
```

### Case B: B 행렬 접근 패턴 분석

k=0일 때 B[(k*4+0) * ldb + col]의 접근 (b0만 분석):

```
B[(0) * 1024 + col]

T00 (col=0):  B[0]     -> 주소 0x0000
T01 (col=1):  B[1]     -> 주소 0x0004
T02 (col=2):  B[2]     -> 주소 0x0008
T03 (col=3):  B[3]     -> 주소 0x000C
...
T15 (col=15): B[15]    -> 주소 0x003C
T16 (col=0):  B[0]     -> 주소 0x0000  (ty=1, col=0 -> 같은 주소!)
T17 (col=1):  B[1]     -> 주소 0x0004
...
T31 (col=15): B[15]    -> 주소 0x003C
```

**분석**: 서로 다른 col을 가진 스레드들은 **연속된 주소**에 접근한다!
T00(주소 0x0000), T01(주소 0x0004), T02(주소 0x0008), ... 이는 완벽한
코얼레싱 패턴이다. 단, ty=0과 ty=1이 같은 주소를 접근하므로 브로드캐스트가
추가로 발생한다.

```
B 행렬 접근 트랜잭션 수 (b0 한 번):
  - 16개 고유 주소 (0x0000 ~ 0x003C) = 64 bytes = 2 섹터
  - ty=0과 ty=1은 같은 주소 -> 브로드캐스트
  - 총: 2 섹터 = 1 트랜잭션 / 워프 (b0 한 번에 대해)

B의 b0, b1, b2, b3를 모두 합산하면:
  b0: B[0 * 1024 + col]   -> 2 섹터
  b1: B[1 * 1024 + col]   -> 2 섹터 (다른 캐시 라인!)
  b2: B[2 * 1024 + col]   -> 2 섹터 (또 다른 캐시 라인!)
  b3: B[3 * 1024 + col]   -> 2 섹터 (또 다른 캐시 라인!)
  총: 8 섹터 = 4 트랜잭션
```

### 비교 요약

```
k=0에서의 워프당 메모리 트랜잭션:

| 행렬 | 접근 방식    | 고유 주소 수 | 캐시 라인 수 | 트랜잭션 수 |
|------|-------------|-------------|-------------|------------|
| A    | float4 x 2  | 2 (브로드캐스트) | 2         | 2          |
| B    | 스칼라 x 4   | 16 x 4 행   | 4          | 4          |
| 합계 |              |             |             | 6          |
```

**관찰**: B의 각 b0, b1, b2, b3 접근은 워프 내에서는 코얼레싱되지만(col이
연속), 서로 다른 행(k값)에서 로드하므로 4개의 서로 다른 캐시 라인에 접근한다.
A는 브로드캐스트 패턴으로 효율적이지만, 벡터화 로드 하나당 16바이트만
활용되므로 대역폭 활용 측면에서는 B보다 오히려 비효율적일 수 있다.

---

## 7. 핵심 정리

이 모듈에서 배운 내용을 정리한다.

- **메모리 코얼레싱**: 워프 내 연속 스레드가 연속 메모리 주소에 접근하면 하나의
  넓은 트랜잭션으로 병합된다. 이는 GPU 메모리 성능의 기본 전제 조건이다.

- **float4 벡터화 로드**: 4개의 float(128비트)를 하나의 명령어로 로드하여,
  명령어 오버헤드를 1/4로 줄이고 대역폭 활용률을 약 4배 향상시킨다.

- **SASS 검증**: `LDG.E.128`이 생성되었는지 확인하는 것이 벡터화 성공의 핵심
  판단 기준이다. `LDG.E`만 보이면 정렬 문제를 의심해야 한다.

- **B 행렬의 한계**: Row-major B 행렬에서 k 차원 접근은 ldb 간격의 점프를
  유발하므로, 하나의 스레드 내에서 float4 벡터화가 불가능하다. 전치(BT)를
  사용하면 이 문제를 해결할 수 있다.

- **근본적 한계**: 데이터 재사용이 없으므로 산술 강도가 0.25 FLOP/byte에
  머물러, cuBLAS의 **약 5-15%** 성능에 그친다.

- **다음 단계**: 공유 메모리를 사용한 타일링(Level 2)이 데이터 재사용을
  도입하여 산술 강도를 근본적으로 개선한다.

---

## 8. 퀴즈

### 문제 1 (Remember)

128-bit 벡터화 로드에 대응하는 SASS 명령어는 무엇인가?

<details>
<summary>정답</summary>

`LDG.E.128`

32-bit 스칼라 로드는 `LDG.E`, 128-bit 벡터화 로드는 `LDG.E.128`이다.

</details>

### 문제 2 (Understand)

메모리 코얼레싱이 GPU 성능에 중요한 이유를 2-3문장으로 설명하라.

<details>
<summary>정답</summary>

GPU의 메모리 시스템은 넓은 대역폭을 제공하지만, 이를 최대로 활용하려면 한 번의
트랜잭션으로 가능한 많은 바이트를 전송해야 한다. 코얼레싱이 되지 않으면 워프의
32개 스레드가 각각 별도의 메모리 트랜잭션을 발생시켜, 최대 32배의 대역폭 낭비가
생긴다. 코얼레싱을 통해 다수의 개별 접근을 하나의 넓은 트랜잭션으로 병합하면,
메모리 컨트롤러의 부하가 줄고 실효 대역폭이 극대화된다.

</details>

### 문제 3 (Apply)

K=1024일 때, 벡터화 루프(`for (int k = 0; k < K_vec; ++k)`)의 반복 횟수는
몇 회인가?

<details>
<summary>정답</summary>

**256회**

`K_vec = K / 4 = 1024 / 4 = 256`

각 반복에서 4개의 원소를 처리하므로, 256 * 4 = 1024개 원소를 모두 처리한다.
나머지(`K_rem = 1024 % 4 = 0`)가 없으므로 나머지 루프는 실행되지 않는다.

</details>

### 문제 4 (Analyze)

B 행렬 접근(`B[(k*4+i) * ldb + col]`)이 하나의 스레드 내에서 float4 벡터화가
불가능한 이유를 메모리 주소 패턴으로 설명하라.

<details>
<summary>정답</summary>

하나의 스레드가 k를 0, 1, 2, 3으로 바꾸면:
- `B[(k*4+0) * ldb + col]` -> 주소 = base + (k*4+0) * ldb + col
- `B[(k*4+1) * ldb + col]` -> 주소 = base + (k*4+1) * ldb + col

두 주소의 차이는 `ldb * sizeof(float)` = ldb * 4 바이트이다. ldb=1024라면
4096바이트 간격이다. float4 벡터화 로드는 연속된 16바이트를 요구하는데,
4096바이트 간격의 데이터는 연속이 아니므로 하나의 float4로 묶는 것이 불가능하다.

반면, 같은 k에 대해 워프 내 서로 다른 스레드들(col = 0, 1, 2, ...)의 접근은
연속 주소가 되므로 워프 단위 코얼레싱은 발생한다. 개별 스레드의 시간적
벡터화와 워프의 공간적 코얼레싱을 구분하는 것이 핵심이다.

</details>

### 문제 5 (Apply)

`FETCH_FLOAT4(A[row * lda + k * 4])` 매크로를 전개(expand)하면 어떤 C++ 코드가
되는가?

<details>
<summary>정답</summary>

```cpp
(reinterpret_cast<const float4*>(&(A[row * lda + k * 4]))[0])
```

단계별로:
1. `A[row * lda + k * 4]` -- A 배열에서 해당 원소를 참조
2. `&(A[row * lda + k * 4])` -- 그 원소의 주소(`const float*`)를 구함
3. `reinterpret_cast<const float4*>(...)` -- `const float4*`로 캐스팅
4. `[0]` -- 첫 번째(유일한) float4 값을 역참조하여 반환

결과적으로, `A[row * lda + k * 4]`부터 `A[row * lda + k * 4 + 3]`까지의
4개 float가 하나의 float4로 로드된다.

</details>

### 문제 6 (Understand)

전치된 B를 사용하는 `sgemm_coalesced_bt` 커널이 원본 `sgemm_coalesced`보다
나은 이유는 무엇인가?

<details>
<summary>정답</summary>

원본 커널에서 B[k][col] 접근 시, k를 연속으로 바꾸면 주소가 ldb만큼 점프하여
float4 벡터화가 불가능하다. 4개의 스칼라 로드(LDG.E)를 사용해야 한다.

전치된 BT[col][k]에서는 k가 행 내 열 방향이 되어 메모리에서 연속 배치된다.
따라서 `BT[col * ldbt + k * 4]`에서 연속 4개 원소를 float4(LDG.E.128)로 로드할
수 있다.

결과적으로, A와 B 모두 128-bit 벡터화 로드를 사용하게 되어 글로벌 메모리 로드
명령어 수가 감소하고, 메모리 요청 효율이 향상된다.

</details>

### 문제 7 (Evaluate)

Level 0(나이브) 대비 Level 1(코얼레싱)의 예상 성능 향상은 약 3-5배이다.
이 향상을 메모리 대역폭 활용 관점에서 설명하라.

<details>
<summary>정답</summary>

나이브 커널에서는:
- A와 B 모두 32-bit 스칼라 로드(LDG.E)를 사용
- 코얼레싱이 최적화되지 않아 실효 대역폭이 이론적 대역폭의 20-30% 수준
- 명령어 수가 많아 명령어 발행 대역폭도 병목

코얼레싱 커널에서는:
- A에서 128-bit 벡터화 로드(LDG.E.128)를 사용하여 명령어당 전송량 4배 증가
- B의 워프 내 접근 패턴이 코얼레싱되어 트랜잭션 수 감소
- 실효 대역폭이 이론적 대역폭의 60-80% 수준으로 향상

대역폭 활용률이 약 20-30%에서 60-80%로 향상되므로, 대략 3-5배의 성능 개선이
관찰된다. 그러나 산술 강도(0.25 FLOP/byte)가 변하지 않으므로, 여전히 메모리
바운드 상태이며 cuBLAS 대비 5-15% 수준에 머문다.

</details>

---

## 9. 다음 단계 미리보기

이 모듈에서 배운 메모리 코얼레싱과 벡터화 로드는 **대역폭 활용률**을 높이는
기법이다. 그러나 근본적으로 각 데이터 원소를 글로벌 메모리에서 한 번만 로드하고
버리는 구조이므로, 산술 강도가 극히 낮게 유지된다.

다음 모듈(모듈 6: Level 2)에서는 **공유 메모리(Shared Memory)** 를 사용하여
데이터를 스레드 블록 내에서 **재사용**하는 **타일링(Tiling)** 기법을 배운다.
타일링을 통해 글로벌 메모리 접근 횟수를 타일 크기만큼 줄일 수 있으며, 산술
강도가 크게 향상되어 cuBLAS 대비 20-40%의 성능에 도달하게 된다.

핵심 질문: "같은 데이터를 여러 스레드가 공유할 수 있다면, 글로벌 메모리에서
몇 번만 로드하면 될까?"
