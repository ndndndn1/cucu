# 모듈 6: Level 2 -- 공유 메모리 타일링 (Shared Memory Tiling)

---

> **마스터리 체크포인트 모듈**: 이 모듈의 끝에 마스터리 체크포인트 퀴즈가 있으며,
> 80% 이상(8문항 중 6문항 이상) 통과해야 다음 모듈로 진행할 수 있다.

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다.

1. 공유 메모리 타일링의 원리와 데이터 재사용 효과를 **설명**할 수 있다.
2. 협력적 로딩(cooperative loading) 패턴을 **구현**할 수 있다.
3. `__syncthreads()`가 타일 루프 안에서 2번 필요한 이유를 **설명**할 수 있다.
4. 뱅크 충돌(bank conflict)의 발생 조건과 +1 패딩 해결법을 **분석**할 수 있다.
5. 인라인 PTX FMA 명령어를 **해석**할 수 있다.

---

## 2. 사전 복습

이전 모듈(모듈 5, 모듈 1)의 핵심 내용을 능동적으로 떠올려 본다. 답을 먼저
생각한 뒤 확인한다.

1. **float4 벡터화 로드가 생성하는 SASS 명령어는 무엇인가?**
   (힌트: 모듈 5에서 다룬 128비트 로드)

2. **공유 메모리(shared memory)의 접근 지연시간은 대략 몇 사이클인가?**
   (힌트: 모듈 1의 메모리 계층 표)

3. **나이브 커널의 산술 강도(arithmetic intensity)가 낮은 근본 원인은 무엇인가?**
   (힌트: 같은 데이터를 반복해서 글로벌 메모리에서 읽는다)

<details>
<summary>사전 복습 정답</summary>

1. `LDG.E.128` -- 128비트(16바이트) 글로벌 메모리 로드 명령어.
2. 약 20-30 사이클. 글로벌 메모리의 400-600 사이클에 비해 10배 이상 빠르다.
3. 데이터 재사용이 없기 때문이다. A 행렬의 같은 원소를 N번, B 행렬의 같은 원소를
   M번 글로벌 메모리에서 반복 로드하므로, 전송 바이트 대비 연산량이 극히 낮다.

</details>

---

## 3. 개념 설명

### 3.1 타일링 개념 (CRA 프레임워크)

#### Concrete (일상 비유)

도서관에서 리포트를 작성하는 상황을 상상해 보자. 참고할 책이 100권 필요하다.

- **비효율적인 방법**: 매번 서고에 가서 1권씩 가져온다. 100번 왕복해야 한다.
- **효율적인 방법**: 한 번에 16권씩 자기 책상 위에 가져와서 작업한다. 가져온
  16권을 모두 활용한 뒤, 다시 서고에 가서 다음 16권을 가져온다. 총 7번 왕복이면
  충분하다.

여기서 **서고 = 글로벌 메모리**, **책상 = 공유 메모리**, **16권 = 타일 크기**에
해당한다. 핵심은 한 번 가져온 데이터를 최대한 많이 재사용하는 것이다.

#### Representational (다이어그램)

행렬 C의 하나의 16x16 블록을 계산하는 과정을 시각화한다.

```
행렬 A (M x K)                    행렬 B (K x N)
+--+--+--+--+--+--+              +--+--+--+--+--+--+
|  |  |  |  |  |  |              |  |t0|  |  |  |  |
+--+--+--+--+--+--+              +--+--+--+--+--+--+
|  |  |  |  |  |  |              |  |t1|  |  |  |  |
+--+--+--+--+--+--+              +--+--+--+--+--+--+
|t0|t1|  |  |  |  | <- A 타일    |  |t2|  |  |  |  |
+--+--+--+--+--+--+              +--+--+--+--+--+--+
|t0|t1|  |  |  |  |              |  |  |  |  |  |  |
+--+--+--+--+--+--+              +--+--+--+--+--+--+
|  |  |  |  |  |  |              ^  ^  ^
|  |  |  |  |  |  |              B 타일
+--+--+--+--+--+--+

        |                               |
        v                               v
   +----------+                    +----------+
   | A_tile   |  공유 메모리       | B_tile   |  공유 메모리
   | [16][16] |  (~20 사이클)      | [16][16] |  (~20 사이클)
   +----------+                    +----------+
        \                               /
         \                             /
          +---> 내적 계산 (16회 FMA) <--+
                      |
                      v
               C[row][col] += sum
```

타일 루프의 흐름:

```
t=0: A의 열 0..15 타일 로드, B의 행 0..15 타일 로드 -> 부분합 계산
t=1: A의 열 16..31 타일 로드, B의 행 16..31 타일 로드 -> 부분합 누적
...
t=T-1: 마지막 타일 로드 -> 최종 합 완성 -> C에 기록
```

#### Abstract (수학적 정의)

데이터 재사용 팩터(data reuse factor)는 정확히 `BLOCK_SIZE`이다. 공유 메모리에
로드된 각 원소는 글로벌 메모리에서 **1번** 로드되고, 내적 계산에서
**BLOCK_SIZE번** 사용된다.

- A_tile의 원소 `A_tile[ty][k]`: 행이 같은 16개 스레드(tx=0..15)가 모두 사용
- B_tile의 원소 `B_tile[k][tx]`: 열이 같은 16개 스레드(ty=0..15)가 모두 사용

따라서 글로벌 메모리 트래픽이 나이브 커널 대비 **BLOCK_SIZE배 감소**한다.

나이브 커널의 산술 강도 vs 타일링 커널의 산술 강도:

```
나이브: FLOP / Bytes = 2K / (2K * 4) = 0.25 FLOP/byte
타일링: FLOP / Bytes = 2K / (2K/BLOCK_SIZE * 4) = 0.25 * BLOCK_SIZE FLOP/byte

BLOCK_SIZE = 16일 때:
타일링 산술 강도 = 0.25 * 16 = 4.0 FLOP/byte (나이브 대비 16배)
```

### 3.2 협력적 로딩 (Cooperative Loading)

타일링이 동작하려면 블록 내 모든 스레드가 **협력하여** 타일을 공유 메모리에
로드해야 한다. 이것이 협력적 로딩이다.

- 블록 크기: 16 x 16 = **256개 스레드**
- 각 스레드는 A_tile에서 **1개 원소**, B_tile에서 **1개 원소**를 로드한다.
- 타일당 총 로드: **256개(A) + 256개(B) = 512개** float 원소

스레드 `(ty, tx)`가 로드하는 원소:

```
A_tile[ty][tx] = A[row * lda + (t * BLOCK_SIZE_K + tx)]
B_tile[ty][tx] = B[(t * BLOCK_SIZE_K + ty) * ldb + col]
```

256개 스레드가 분담하므로, 타일 전체를 한 번에 로드할 수 있다. 한 스레드가 여러
원소를 로드할 필요가 없다.

### 3.3 동기화 (__syncthreads())

타일 루프 내부에는 `__syncthreads()`가 **반드시 2번** 필요하다. 하나라도
빠지면 결과가 틀려진다.

#### 첫 번째 동기화 (라인 93): 로드 완료 보장

```
A_tile[ty][tx] = ...;   // 각 스레드가 자기 원소 로드
B_tile[ty][tx] = ...;
__syncthreads();         // [1] 모든 스레드의 로드가 끝날 때까지 대기

for (k = 0; k < BLOCK_SIZE_K; ++k) {
    sum += A_tile[ty][k] * B_tile[k][tx];   // 다른 스레드가 로드한 데이터 사용
}
```

만약 첫 번째 동기화가 없다면: 스레드 A가 아직 `A_tile[3][7]`을 로드하지
않았는데, 스레드 B가 `A_tile[3][7]`을 읽으면 **초기화되지 않은 값** 또는
**이전 타일의 값**을 사용하게 된다. 결과가 틀려진다.

#### 두 번째 동기화 (라인 105): 연산 완료 보장

```
for (k = 0; k < BLOCK_SIZE_K; ++k) {
    sum += A_tile[ty][k] * B_tile[k][tx];   // 현재 타일 데이터로 연산
}
__syncthreads();         // [2] 모든 스레드의 연산이 끝날 때까지 대기

// 다음 반복: 새로운 타일 로드
A_tile[ty][tx] = ...;   // 다음 타일 데이터로 덮어씀
```

만약 두 번째 동기화가 없다면: 빠른 스레드 A가 다음 타일 데이터를
`A_tile[3][7]`에 이미 써버렸는데, 느린 스레드 B가 아직 현재 타일의
`A_tile[3][7]`로 연산 중이면 **덮어씌워진 값**을 사용하게 된다. 결과가
틀려진다.

요약:

| 동기화 | 위치 | 목적 | 생략 시 문제 |
|--------|------|------|-------------|
| 첫 번째 | 로드 후, 연산 전 | 타일 로드 완료 보장 | 미로드 데이터 사용 |
| 두 번째 | 연산 후, 다음 로드 전 | 연산 완료 보장 | 덮어쓰기로 데이터 오염 |

### 3.4 뱅크 충돌 심층 분석 (핵심)

뱅크 충돌(bank conflict)은 공유 메모리 타일링의 성능을 좌우하는 핵심 요소이다.
이 절의 내용을 정확히 이해해야 한다.

#### 공유 메모리 뱅크 구조

GPU의 공유 메모리는 **32개 뱅크**로 구성되어 있다. 연속된 4바이트(float 1개)
워드가 연속된 뱅크에 매핑된다.

```
주소(바이트):  0    4    8    12   ...  124   128  132  ...
뱅크 번호:     0    1    2    3    ...   31     0    1   ...
```

뱅크 번호 계산 공식:

```
뱅크 번호 = (바이트 주소 / 4) % 32
```

**뱅크 충돌 발생 조건**: 같은 워프(warp)의 **서로 다른 스레드**가 **같은
뱅크**의 **서로 다른 주소**에 동시에 접근하면, 접근이 직렬화(serialize)되어
성능이 저하된다.

참고: 같은 뱅크의 **같은 주소**에 접근하면 브로드캐스트(broadcast)가 발생하여
충돌이 아니다.

#### 패딩 없는 경우 -- 충돌 발생

`float A_tile[16][16]` (패딩 없음)로 선언한 경우를 분석한다.

행렬이 메모리에 행 단위로 연속 저장되므로:

```
A_tile[0][0..15] -> 바이트 주소 0..60   -> 뱅크 0, 1, 2, ..., 15
A_tile[1][0..15] -> 바이트 주소 64..124 -> 뱅크 16,17,18,..., 31
A_tile[2][0..15] -> 바이트 주소 128..188 -> 뱅크 0, 1, 2, ..., 15
A_tile[3][0..15] -> 바이트 주소 192..252 -> 뱅크 16,17,18,..., 31
...
```

여기서 핵심적인 문제가 발생한다. 내적 루프에서 스레드 `ty=0..15`가 **동일한
k** 값에 대해 `A_tile[ty][k]`를 읽을 때:

```
k=0일 때:
  Thread ty=0: A_tile[0][0] -> 바이트 (0*16+0)*4 = 0   -> 뱅크 0
  Thread ty=1: A_tile[1][0] -> 바이트 (1*16+0)*4 = 64  -> 뱅크 16
  Thread ty=2: A_tile[2][0] -> 바이트 (2*16+0)*4 = 128 -> 뱅크 0   *** 뱅크 0 충돌! ***
  Thread ty=3: A_tile[3][0] -> 바이트 (3*16+0)*4 = 192 -> 뱅크 16  *** 뱅크 16 충돌! ***
  ...
```

**행 폭이 16이면** 각 행이 16 * 4 = 64바이트를 차지한다. 64 / 4 = 16이므로
16개 뱅크 간격으로 다음 행이 시작된다. 즉 행 0과 행 2가 같은 뱅크에 매핑되고,
행 1과 행 3이 같은 뱅크에 매핑된다. 결과적으로 **매 2행마다 충돌**이
발생한다.

#### 패딩 있는 경우 (+1) -- 충돌 제거

`float A_tile[16][16 + 1]` (+1 패딩)로 선언한 경우:

실제 메모리 레이아웃에서 각 행은 17개 float = 68바이트를 차지한다. 68 / 4 = 17
이므로, 매 행이 시작하는 뱅크가 **17만큼 이동**한다. 17 % 32 = 17이므로 매 행의
시작 뱅크가 서로 다르다.

```
A_tile[0][0..15, pad] -> 뱅크 0, 1, 2, ..., 15, 16
A_tile[1][0..15, pad] -> 뱅크 17,18,19,..., 31, 0, 1
A_tile[2][0..15, pad] -> 뱅크 2, 3, 4, ..., 17,18
...
```

스레드 `ty=0..15`가 `A_tile[ty][k]`를 읽을 때:

```
뱅크 번호 = (ty * 17 + k) % 32
```

ty 값이 다르면 `ty * 17` 값이 17씩 차이나므로, 32로 나눈 나머지가 모두 다르다
(17과 32는 서로소이기 때문이다). 따라서 **모든 스레드가 서로 다른 뱅크에
접근**하게 되어 충돌이 완전히 제거된다.

구체적인 예시 (k=0):

```
Thread ty=0: (0 * 17 + 0) % 32 = 0
Thread ty=1: (1 * 17 + 0) % 32 = 17
Thread ty=2: (2 * 17 + 0) % 32 = 2
Thread ty=3: (3 * 17 + 0) % 32 = 19
Thread ty=4: (4 * 17 + 0) % 32 = 4
...
모두 다른 뱅크 -> 충돌 없음!
```

#### Nsight Compute로 확인

Nsight Compute의 "Memory Workload Analysis" -> "L1/TEX Cache" 섹션에서
"Wavefronts Shared" 메트릭을 확인한다.

- 이상적 값: **1.0** (충돌 없음, 모든 요청이 1회 웨이브프론트에 처리됨)
- 1.0 초과: 뱅크 충돌로 인해 추가 웨이브프론트(재실행)가 발생

+1 패딩을 적용하면 이 값이 정확히 1.0이 된다.

### 3.5 인라인 PTX FMA

`src/kernels/sgemm_tiled.cu`의 PTX 버전(라인 167-177)에서는 내적 계산에
인라인 PTX를 사용한다.

```cuda
asm volatile(
    "fma.rn.f32 %0, %1, %2, %0;"
    : "+f"(sum)
    : "f"(a), "f"(b)
);
```

각 부분의 의미:

| 구성 요소 | 의미 |
|-----------|------|
| `fma` | Fused Multiply-Add 명령어 |
| `.rn` | Round-to-Nearest (가장 가까운 값으로 반올림) |
| `.f32` | 32비트 단정밀도 부동소수점 |
| `%0` | 출력 피연산자 = `sum` ("+f"이므로 입출력 겸용) |
| `%1` | 첫 번째 입력 = `a` (A_tile에서 읽은 값) |
| `%2` | 두 번째 입력 = `b` (B_tile에서 읽은 값) |

수학적으로: `sum = a * b + sum` (d = a * b + d)

이 인라인 PTX가 필요한 이유:

- 컴파일러가 `sum += a * b`를 별도의 `MUL` + `ADD` 두 명령어로 분리할
  가능성을 차단한다.
- **FMA 단일 명령어**를 보장함으로써 2 FLOP을 1 사이클에 처리한다.
- SASS에서 `FFMA` 명령어로 변환되는 것을 확인할 수 있다.
- FMA는 중간 라운딩이 없어 MUL+ADD보다 **수치 정확도**도 높다.

---

## 4. 코드 분석

커널 전체 코드(`src/kernels/sgemm_tiled.cu`, 라인 39-112)를 라인별로 분석한다.

### 4.1 함수 시그니처 (라인 39-51)

```cuda
extern "C" __global__ void sgemm_tiled(
    const float* __restrict__ A,
    const float* __restrict__ B,
    float* __restrict__ C,
    const int M, const int N, const int K,
    const int lda, const int ldb, const int ldc,
    const float alpha, const float beta)
```

- `extern "C"`: Driver API에서 이름 맹글링 없이 커널을 찾기 위해 필요하다.
- `__restrict__`: 포인터 간 별칭(alias)이 없음을 보장하여 컴파일러 최적화를
  가능하게 한다.
- `alpha`, `beta`: BLAS 표준의 스케일링 파라미터. `C = alpha * A * B + beta * C`.

### 4.2 공유 메모리 선언 (라인 55-56)

```cuda
__shared__ float A_tile[BLOCK_SIZE][BLOCK_SIZE_K + SMEM_PADDING];     // [16][17]
__shared__ float B_tile[BLOCK_SIZE_K][BLOCK_SIZE + SMEM_PADDING];     // [16][17]
```

- `__shared__`: 이 배열을 공유 메모리에 할당한다. 블록 내 256개 스레드가 공유한다.
- `+ SMEM_PADDING` (+1): 뱅크 충돌 방지를 위한 패딩. 3.4절에서 상세히 분석하였다.
- 총 공유 메모리 사용량: `2 * 16 * 17 * 4 = 2176 바이트` (약 2.1 KB).

### 4.3 스레드 및 글로벌 인덱스 (라인 59-64)

```cuda
const int tx = threadIdx.x;                          // 블록 내 열 인덱스
const int ty = threadIdx.y;                          // 블록 내 행 인덱스
const int row = blockIdx.y * BLOCK_SIZE + ty;        // 글로벌 행
const int col = blockIdx.x * BLOCK_SIZE + tx;        // 글로벌 열
```

- `(ty, tx)`는 블록 내 2차원 위치. 범위: 0..15.
- `(row, col)`은 출력 행렬 C에서의 글로벌 위치. 이 스레드가 `C[row][col]`을 담당한다.

### 4.4 타일 수 계산 (라인 70)

```cuda
const int num_tiles = (K + BLOCK_SIZE_K - 1) / BLOCK_SIZE_K;
```

K 차원을 `BLOCK_SIZE_K`(16) 단위로 나눈다. 올림 나눗셈(ceiling division)으로
K가 16의 배수가 아닌 경우도 처리한다.

### 4.5 메인 타일 루프 (라인 72-106)

```cuda
for (int t = 0; t < num_tiles; ++t) {
```

이 루프가 전체 알고리즘의 핵심이다. 각 반복에서:

1. 타일을 공유 메모리에 로드
2. 동기화
3. 공유 메모리에서 부분 내적 계산
4. 동기화
5. 다음 타일로 이동

#### A 타일 로드 (라인 77-82)

```cuda
const int a_col = t * BLOCK_SIZE_K + tx;
if (row < M && a_col < K) {
    A_tile[ty][tx] = A[row * lda + a_col];
} else {
    A_tile[ty][tx] = 0.0f;
}
```

- `a_col`: 현재 타일에서의 열 위치. 타일 번호 `t`에 따라 이동한다.
- 경계 검사: `row < M && a_col < K`로 행렬 범위를 벗어나는 접근을 방지한다.
- 범위 밖이면 `0.0f`을 채운다. 0은 곱셈 시 영향을 주지 않으므로 결과에 오류가
  없다.

#### B 타일 로드 (라인 85-90)

```cuda
const int b_row = t * BLOCK_SIZE_K + ty;
if (b_row < K && col < N) {
    B_tile[ty][tx] = B[b_row * ldb + col];
} else {
    B_tile[ty][tx] = 0.0f;
}
```

- `b_row`: B 행렬에서의 행 위치. 마찬가지로 타일 번호에 따라 이동한다.
- A와 동일한 패턴: 경계 밖은 0으로 채운다.

#### 첫 번째 동기화 (라인 93)

```cuda
__syncthreads();
```

모든 256개 스레드가 A_tile과 B_tile 로드를 완료할 때까지 대기한다.

#### 내적 계산 (라인 98-101)

```cuda
#pragma unroll
for (int k = 0; k < BLOCK_SIZE_K; ++k) {
    sum += A_tile[ty][k] * B_tile[k][tx];
}
```

- `#pragma unroll`: 컴파일러에게 이 루프(16회 반복)를 완전히 펼치도록 지시한다.
  분기 오버헤드가 제거되고 명령어 수준 병렬성(ILP)이 향상된다.
- 각 반복: `A_tile[ty][k]`와 `B_tile[k][tx]`를 곱하여 `sum`에 누적한다.
- 이것이 **데이터 재사용**이 일어나는 지점이다. `A_tile[ty][k]`는 tx가 다른
  16개 스레드가 모두 사용하고, `B_tile[k][tx]`는 ty가 다른 16개 스레드가
  모두 사용한다.

#### 두 번째 동기화 (라인 105)

```cuda
__syncthreads();
```

모든 스레드가 현재 타일의 연산을 완료할 때까지 대기한다. 이후 다음 반복에서
타일을 덮어쓴다.

### 4.6 결과 기록 (라인 109-111)

```cuda
if (row < M && col < N) {
    C[row * ldc + col] = alpha * sum + beta * C[row * ldc + col];
}
```

- 모든 타일의 부분합이 `sum`에 누적된 상태이다.
- BLAS 표준에 따라 `C = alpha * (A * B) + beta * C`를 적용한다.
- 경계 검사로 범위 밖 기록을 방지한다.

---

## 5. 왜 이것이 작동하는가?

다음 질문에 스스로 답해 보라. 단순히 "무엇(what)"이 아니라 "왜(why)"를 설명할
수 있어야 진정한 이해이다.

### 질문 1: 데이터 재사용이 정확히 BLOCK_SIZE배인 이유는?

A_tile의 한 원소 `A_tile[ty][k]`를 생각해 보자. 이 원소는 `ty`가 같고 `tx`가
0부터 15까지인 **16개 스레드**가 모두 내적 계산에 사용한다. 즉 글로벌 메모리에서
1번 로드된 후 16번 재사용된다. B_tile도 마찬가지로, `B_tile[k][tx]`는 `ty`가
0부터 15까지인 16개 스레드가 사용한다. 따라서 재사용 팩터 = BLOCK_SIZE = 16.

### 질문 2: __syncthreads()를 하나만 사용하면 구체적으로 어떤 오류가 발생하는가?

**첫 번째만 생략하는 경우**: 스레드 0이 아직 `A_tile[0][3]`을 로드하지
않았는데, 스레드 5가 `k=3` 반복에서 `A_tile[0][3]`을 읽으면 0이나 쓰레기 값을
사용한다. 결과 행렬에 무작위 오류가 발생한다.

**두 번째만 생략하는 경우**: 스레드 0이 다음 타일(t+1)의 데이터를
`A_tile[0][3]`에 이미 기록했는데, 스레드 5가 아직 타일 t의 `A_tile[0][3]`으로
`k=3` 연산 중이면 타일 (t+1)의 값을 타일 t의 계산에 잘못 사용한다. 타일 경계
근처에서 오류가 발생한다.

### 질문 3: 패딩이 정확히 +1이면 충분한 이유는? +2가 필요한 경우는?

BLOCK_SIZE_K = 16일 때, 패딩 없이 행 폭은 16이다. 16 + 1 = 17이고, 17과 32는
서로소(GCD = 1)이므로 `ty * 17 mod 32`가 ty=0..31에 대해 모두 다른 값을
갖는다. 따라서 +1이면 충분하다.

+2가 필요한 경우: 만약 BLOCK_SIZE_K = 31이면 31 + 1 = 32인데, 32 % 32 = 0
이므로 모든 행이 같은 뱅크에서 시작하여 다시 충돌이 발생한다. 이때는 +2로
33(32와 서로소)을 만들어야 한다. 일반적으로, `(BLOCK_SIZE_K + padding)`과
32가 서로소가 되도록 패딩을 선택해야 한다.

### 질문 4: 타일링만으로 cuBLAS의 20-40%밖에 안 되는 이유는?

타일링은 글로벌 메모리 트래픽을 16배 줄이지만, 여전히 두 가지 한계가 있다.

1. **낮은 산술 강도**: 각 스레드가 C 행렬의 1개 원소만 계산한다. 공유 메모리에서
   레지스터로의 로드 횟수 대비 FMA 연산 수가 부족하다. (산술 강도 4.0은
   GPU의 루프라인 균형점에 훨씬 미달)
2. **레이턴시 은닉 부족**: 타일 로드와 연산이 순차적으로 실행된다. 로드 중에는
   연산 유닛이 놀고, 연산 중에는 메모리 유닛이 논다.

이 두 문제를 해결하는 것이 이후 모듈의 주제이다 (Level 3: 레지스터 블로킹,
Level 4: 더블 버퍼링).

---

## 6. 시뮬레이션 6: 뱅크 충돌 검출기

이 시뮬레이션에서는 공유 메모리 뱅크 번호를 직접 계산하여 충돌을 검출한다.
종이나 스프레드시트로 수행할 수 있다.

### 문제: 패딩 없는 경우

`float smem[16][16]` (패딩 없음)을 선언했다고 하자. 16개 스레드
(ty=0, 1, 2, ..., 15)가 **동시에** `smem[ty][5]`를 읽는다 (k=5인 경우).

각 스레드가 접근하는 바이트 주소와 뱅크 번호를 계산한다.

```
뱅크 번호 = ((ty * 16 + col) * 4 / 4) % 32 = (ty * 16 + col) % 32
```

col = 5일 때:

```
Thread ty=0:  주소 = (0  * 16 + 5) * 4 = 20   -> 뱅크 = (0  * 16 + 5) % 32 = 5
Thread ty=1:  주소 = (1  * 16 + 5) * 4 = 84   -> 뱅크 = (1  * 16 + 5) % 32 = 21
Thread ty=2:  주소 = (2  * 16 + 5) * 4 = 148  -> 뱅크 = (2  * 16 + 5) % 32 = 5   *** 충돌! (ty=0과 동일) ***
Thread ty=3:  주소 = (3  * 16 + 5) * 4 = 212  -> 뱅크 = (3  * 16 + 5) % 32 = 21  *** 충돌! (ty=1과 동일) ***
Thread ty=4:  주소 = (4  * 16 + 5) * 4 = 276  -> 뱅크 = (4  * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=5:  주소 = (5  * 16 + 5) * 4 = 340  -> 뱅크 = (5  * 16 + 5) % 32 = 21  *** 충돌! ***
Thread ty=6:  주소 = (6  * 16 + 5) * 4 = 404  -> 뱅크 = (6  * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=7:  주소 = (7  * 16 + 5) * 4 = 468  -> 뱅크 = (7  * 16 + 5) % 32 = 21  *** 충돌! ***
Thread ty=8:  주소 = (8  * 16 + 5) * 4 = 532  -> 뱅크 = (8  * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=9:  주소 = (9  * 16 + 5) * 4 = 596  -> 뱅크 = (9  * 16 + 5) % 32 = 21  *** 충돌! ***
Thread ty=10: 주소 = (10 * 16 + 5) * 4 = 660  -> 뱅크 = (10 * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=11: 주소 = (11 * 16 + 5) * 4 = 724  -> 뱅크 = (11 * 16 + 5) % 32 = 21  *** 충돌! ***
Thread ty=12: 주소 = (12 * 16 + 5) * 4 = 788  -> 뱅크 = (12 * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=13: 주소 = (13 * 16 + 5) * 4 = 852  -> 뱅크 = (13 * 16 + 5) % 32 = 21  *** 충돌! ***
Thread ty=14: 주소 = (14 * 16 + 5) * 4 = 916  -> 뱅크 = (14 * 16 + 5) % 32 = 5   *** 충돌! ***
Thread ty=15: 주소 = (15 * 16 + 5) * 4 = 980  -> 뱅크 = (15 * 16 + 5) % 32 = 21  *** 충돌! ***
```

패턴: 짝수 ty는 모두 뱅크 5, 홀수 ty는 모두 뱅크 21. **8-way 뱅크 충돌**이
두 뱅크에서 동시에 발생한다. 이 접근은 직렬화되어 8배 느려진다.

### 해결: +1 패딩 적용

`float smem[16][17]` (+1 패딩)로 변경한다. 행 폭이 17이 된다.

```
뱅크 번호 = (ty * 17 + col) % 32
```

col = 5일 때:

```
Thread ty=0:  (0  * 17 + 5) % 32 = 5
Thread ty=1:  (1  * 17 + 5) % 32 = 22
Thread ty=2:  (2  * 17 + 5) % 32 = 7
Thread ty=3:  (3  * 17 + 5) % 32 = 24
Thread ty=4:  (4  * 17 + 5) % 32 = 9
Thread ty=5:  (5  * 17 + 5) % 32 = 26
Thread ty=6:  (6  * 17 + 5) % 32 = 11
Thread ty=7:  (7  * 17 + 5) % 32 = 28
Thread ty=8:  (8  * 17 + 5) % 32 = 13
Thread ty=9:  (9  * 17 + 5) % 32 = 30
Thread ty=10: (10 * 17 + 5) % 32 = 15
Thread ty=11: (11 * 17 + 5) % 32 = 0
Thread ty=12: (12 * 17 + 5) % 32 = 17
Thread ty=13: (13 * 17 + 5) % 32 = 2
Thread ty=14: (14 * 17 + 5) % 32 = 19
Thread ty=15: (15 * 17 + 5) % 32 = 4
```

**16개 스레드가 모두 서로 다른 뱅크에 접근한다.** 충돌이 완전히 제거되었다.

핵심 원리: 17과 32가 서로소이므로, `ty * 17 mod 32`는 ty=0..31에 대해
32개의 서로 다른 나머지를 생성한다 (모듈러 산술의 기본 정리).

---

## 7. 핵심 정리

이 모듈에서 배운 내용을 요약한다.

- **공유 메모리 타일링** = 큰 행렬을 작은 타일로 나누어 빠른 공유 메모리에
  로드하고 재사용. 글로벌 메모리 트래픽이 **BLOCK_SIZE배 감소**한다.

- **협력적 로딩**: 블록 내 256개 스레드가 각각 1개 원소씩 분담하여 타일을
  로드한다. **2회의 `__syncthreads()` 동기화가 필수**이다 (로드 완료 보장 +
  연산 완료 보장).

- **+1 패딩**으로 뱅크 충돌을 완전히 제거한다. 핵심 원리: `(BLOCK_SIZE_K + 1)`과
  32가 서로소이면 모든 행의 시작 뱅크가 다르다.

- **인라인 PTX FMA** (`fma.rn.f32`)로 연산 효율을 보장한다. 컴파일러가 MUL+ADD로
  분리하는 것을 방지하고, SASS에서 `FFMA` 단일 명령어가 생성됨을 확인한다.

- **예상 성능**: cuBLAS의 약 **20-40%**. 아직 산술 강도와 레이턴시 은닉이
  부족하여 상한이 존재한다.

---

## 8. 마스터리 체크포인트 퀴즈

> **통과 기준: 8문항 중 6문항 이상 정답 (80%)**
>
> 이 퀴즈를 통과해야 다음 모듈(Level 3: 레지스터 블로킹)로 진행할 수 있다.
> 타일링, 동기화, 뱅크 충돌은 이후 모든 최적화의 기반이 되므로, 확실히
> 이해하고 넘어가야 한다.

---

**문제 1 (Remember)**: GPU 공유 메모리의 뱅크 수는 몇 개인가?

<details>
<summary>정답</summary>

**32개**. 연속된 4바이트(float) 워드가 연속된 뱅크에 매핑된다.

</details>

---

**문제 2 (Understand)**: 공유 메모리 타일링의 핵심 원리를 한 문장으로 설명하라.

<details>
<summary>정답</summary>

**데이터 재사용**. 글로벌 메모리에서 타일을 1번 로드한 후, 공유 메모리에서
BLOCK_SIZE번 재사용하여 글로벌 메모리 트래픽을 BLOCK_SIZE배 줄이는 것이다.

</details>

---

**문제 3 (Apply)**: BLOCK_SIZE = 32일 때 A_tile과 B_tile의 공유 메모리 총
사용량은 몇 바이트인가? (패딩 무시)

<details>
<summary>정답</summary>

A_tile: 32 x 32 x 4바이트 = 4096 바이트
B_tile: 32 x 32 x 4바이트 = 4096 바이트
합계: **8192 바이트 (8 KB)**

계산: `2 * 32 * 32 * 4 = 8192 bytes`

</details>

---

**문제 4 (Apply)**: 다음 코드의 빈칸을 채워 협력적 로딩을 완성하라.

```cuda
if (row < M && a_col < K) {
    A_tile[___][___] = A[___ * lda + ___];
}
```

<details>
<summary>정답</summary>

```cuda
if (row < M && a_col < K) {
    A_tile[ty][tx] = A[row * lda + a_col];
}
```

- `A_tile[ty][tx]`: 스레드의 블록 내 위치 (ty, tx)에 해당하는 타일 원소.
- `A[row * lda + a_col]`: 글로벌 행렬 A에서 (row, a_col) 위치의 원소.

</details>

---

**문제 5 (Analyze)**: 타일 루프에서 `__syncthreads()`를 첫 번째 것만 남기고
두 번째를 제거하면, 구체적으로 어떤 문제가 발생하는가? 어떤 상황에서 오류가
나타나는지 스레드 시나리오를 들어 설명하라.

<details>
<summary>정답</summary>

두 번째 `__syncthreads()`가 없으면, 빠른 스레드가 다음 타일(t+1)의 데이터를
공유 메모리에 덮어쓸 수 있는데, 느린 스레드가 아직 현재 타일(t)의 데이터로
연산 중일 수 있다.

예시 시나리오:
- 스레드 0이 타일 t의 내적 계산을 끝내고, 루프 t+1에서 `A_tile[0][3]`에
  새 값을 기록한다.
- 스레드 7은 아직 타일 t에서 `k=3` 반복 중이며, `A_tile[0][3]`을 읽으려 한다.
- 스레드 7은 타일 t의 값이 아닌 타일 (t+1)의 값을 읽게 되어 결과가 틀려진다.

</details>

---

**문제 6 (Analyze)**: 패딩 없이 `float A_tile[16][16]`을 선언했을 때, 스레드
ty=0과 ty=2가 `A_tile[ty][0]`을 동시에 읽으면 뱅크 충돌이 발생하는 이유를
뱅크 번호 계산으로 증명하라.

<details>
<summary>정답</summary>

행 폭 = 16 (float), 패딩 없음.

```
Thread ty=0: A_tile[0][0]
  바이트 주소 = (0 * 16 + 0) * 4 = 0
  뱅크 = 0 / 4 % 32 = 0

Thread ty=2: A_tile[2][0]
  바이트 주소 = (2 * 16 + 0) * 4 = 128
  뱅크 = 128 / 4 % 32 = 32 % 32 = 0
```

두 스레드 모두 **뱅크 0**에 접근하지만, 주소가 다르다 (0 vs 128).
따라서 뱅크 충돌이 발생하고 접근이 직렬화된다.

근본 원인: 행 폭 16 * 4바이트 = 64바이트 = 뱅크 16개분. 2행 간격이면
32개 뱅크를 정확히 한 바퀴 돌아 같은 뱅크로 돌아온다 (64 * 2 = 128바이트,
128 / 4 = 32, 32 % 32 = 0).

</details>

---

**문제 7 (Apply)**: 인라인 PTX 명령어 `fma.rn.f32 %0, %1, %2, %0`에서 각
피연산자의 역할을 설명하라.

<details>
<summary>정답</summary>

```
fma.rn.f32 %0, %1, %2, %0
           |   |   |   |
           d   a   b   c
```

- `%0` (출력, d): 결과가 저장되는 레지스터 = `sum`. "+f" 제약 조건에 의해
  입출력 겸용.
- `%1` (입력, a): 곱셈의 첫 번째 피연산자 = `A_tile[ty][k]`에서 읽은 값.
- `%2` (입력, b): 곱셈의 두 번째 피연산자 = `B_tile[k][tx]`에서 읽은 값.
- `%0` (입력, c): 덧셈 피연산자 = 현재까지의 누적합 `sum`.

연산: `sum = A_tile[ty][k] * B_tile[k][tx] + sum` (d = a * b + c)

`.rn`은 Round-to-Nearest, `.f32`는 32비트 단정밀도를 의미한다.

</details>

---

**문제 8 (Evaluate)**: Level 1(나이브/코얼레싱) 대비 타일링 커널의 글로벌
메모리 트래픽 감소 배율을 BLOCK_SIZE = 16 기준으로 계산하라. 이 개선이
cuBLAS 수준에 도달하기에 충분하지 않은 이유도 서술하라.

<details>
<summary>정답</summary>

**글로벌 메모리 트래픽 감소 배율: 16배 (BLOCK_SIZE)**

계산:
- 나이브: C의 각 원소를 계산하기 위해 A의 한 행(K개)과 B의 한 열(K개)을
  글로벌 메모리에서 읽는다. M*N개 원소에 대해 총 M*N*2K번 읽기.
- 타일링: 각 타일에서 A 타일(16*16)과 B 타일(16*16)을 한 번만 로드하고
  16번 재사용한다. 총 읽기 횟수가 1/16로 줄어든다.

**cuBLAS에 도달하기 충분하지 않은 이유:**

1. 산술 강도가 여전히 부족하다. 타일링의 산술 강도는 약 4.0 FLOP/byte이지만,
   A100의 루프라인 균형점은 약 100 FLOP/byte이다. 커널이 여전히 **메모리
   바운드** 영역에 있다.
2. 메모리 로드와 연산이 순차적으로 일어나 **레이턴시 은닉**이 되지 않는다.
   타일을 로드하는 동안 연산 유닛이 유휴 상태이다.
3. 각 스레드가 1개 원소만 담당하므로 스레드당 레지스터 활용이 비효율적이다.

</details>

---

## 9. 다음 단계 미리보기

현재 타일링 커널은 각 스레드가 C 행렬의 **1개 원소**만 계산한다. 이 때문에
산술 강도가 4.0 FLOP/byte에 불과하여 GPU의 연산 능력을 충분히 활용하지
못한다.

다음 모듈(모듈 7: Level 3 -- 레지스터 블로킹)에서는 각 스레드가 **64개
원소**(8x8 타일)를 레지스터에서 계산하는 기법을 배운다. 이 단일 변경만으로
산술 강도가 **8배** 향상되어 cuBLAS의 50-70%에 도달한다. 핵심 아이디어는
외적(outer product) 기반 마이크로커널로, 공유 메모리에서 레지스터로의 로드
횟수 대비 FMA 연산 횟수를 극대화하는 것이다.

---

*이 모듈의 커널 코드: `src/kernels/sgemm_tiled.cu` (라인 39-112: 기본 버전,
라인 120-197: PTX 버전)*

*뱅크 충돌 분석 주석: `src/kernels/sgemm_tiled.cu` (라인 200-227)*
