# 모듈 7: Level 3 - 레지스터 블로킹 (Register Blocking)

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다:

- 산술 강도가 0.25 FLOP/byte에서 2.0 FLOP/byte로 증가하는 원리를 수식으로 증명할 수 있다
- 3계층 타일 구조(Block-Thread-K)의 각 계층의 역할과 크기를 설명할 수 있다
- 외적(Outer Product) 마이크로커널의 데이터 재사용 패턴을 분석할 수 있다
- 레지스터 압박(Register Pressure)과 점유율(Occupancy) 트레이드오프를 정량적으로 평가할 수 있다

---

## 2. 사전 복습

다음은 이전 모듈(모듈 6: Level 2 타일링, 모듈 3: 행렬곱셈 수학적 배경)에서 다룬 핵심 개념에 대한 인출 연습이다. 답을 먼저 스스로 떠올린 후 확인하라.

1. **공유 메모리 타일링에서 데이터 재사용 팩터는 얼마인가?** 타일 크기 TILE_SIZE일 때, 각 데이터 원소가 공유 메모리에서 몇 번 재사용되는지 계산하라.

2. **뱅크 충돌을 방지하는 패딩의 원리는 무엇인가?** `__shared__ float tile[16][16+1]`에서 `+1`이 뱅크 매핑을 어떻게 변화시키는지 설명하라.

3. **산술 강도(Arithmetic Intensity)의 정의와 단위는 무엇인가?** 루프라인 모델에서 산술 강도가 커널의 성능 상한을 어떻게 결정하는지 서술하라.

---

## 3. 개념 설명

### 3.1 산술 강도 문제 진단: Level 2의 한계

Level 2(타일링 커널)는 글로벌 메모리 접근을 공유 메모리로 대체하여 대역폭을 절약했다. 그러나 근본적인 문제가 남아 있다. **각 스레드가 단 1개의 출력 원소만 계산한다는 점**이다.

Level 2에서 한 스레드의 K 반복당 동작을 분석해 보자.

```
[Level 2: 1스레드 = 1원소]

K 반복 1회당:
  로드: A_tile[row][k]  -->  1개 float = 4 bytes
        B_tile[k][col]  -->  1개 float = 4 bytes
  합계: 2개 로드 = 8 bytes

  연산: C += A * B      -->  1 FMA = 2 FLOPs

  산술 강도 = 2 FLOPs / 8 bytes = 0.25 FLOP/byte
```

이 산술 강도는 **매우 낮다**. 루프라인 모델에서 이것이 의미하는 바를 보자.

```
성능 상한 = min(피크 연산, 피크 대역폭 * 산술 강도)

A100 기준:
  피크 FP32 연산: ~19,500 GFLOP/s
  L1/공유메모리 대역폭: ~19,000 GB/s (이론값)

  성능 상한 = min(19500, 19000 * 0.25)
            = min(19500, 4750)
            = 4750 GFLOP/s

  --> 연산 유닛의 24%만 활용 가능!
```

문제의 본질은 간단하다. **로드한 데이터를 한 번밖에 사용하지 않는다.** A_tile에서 읽은 값은 한 번의 곱셈에만 사용되고 버려진다. B_tile에서 읽은 값도 마찬가지이다.

#### 해결 방향: 스레드 조잡화(Thread Coarsening)

해결책은 각 스레드가 **여러 개의 출력 원소**를 담당하도록 하는 것이다. 구체적으로, TM=8, TN=8으로 설정하여 한 스레드가 8x8 = 64개의 출력 원소를 계산하게 한다.

```
[Level 3: 1스레드 = 64원소 (TM=8, TN=8)]

K 반복 1회당:
  로드: A에서 TM개 = 8개 float = 32 bytes
        B에서 TN개 = 8개 float = 32 bytes
  합계: 16개 로드 = 64 bytes

  연산: TM * TN개 FMA = 8 * 8 = 64 FMA = 128 FLOPs

  산술 강도 = 128 FLOPs / 64 bytes = 2.0 FLOP/byte
```

산술 강도가 **0.25에서 2.0으로, 8배 향상**되었다. 이것이 레지스터 블로킹의 핵심 원리이다.

#### 수학적 일반화

TM x TN 스레드 타일의 산술 강도를 일반 공식으로 유도한다.

```
로드량 = (TM + TN) * sizeof(float) = (TM + TN) * 4 bytes
FLOPs  = TM * TN * 2  (각 FMA = 2 FLOPs)

산술 강도 = (TM * TN * 2) / ((TM + TN) * 4)
          = TM * TN / (2 * (TM + TN))

TM = TN = T로 놓으면:
  산술 강도 = T^2 / (2 * 2T) = T / 4

  T = 1  --> AI = 0.25 (Level 2)
  T = 4  --> AI = 1.0
  T = 8  --> AI = 2.0  (Level 3)
  T = 16 --> AI = 4.0
```

스레드 타일 크기를 키울수록 산술 강도가 **선형으로** 증가한다. 그러나 무한정 키울 수는 없다. TM * TN개의 레지스터가 필요하고, GPU 스레드당 레지스터 한계는 255개이기 때문이다.

---

### 3.2 3계층 타일 구조 (Block - Thread - K)

Level 3에서는 행렬곱의 타일링이 **3개의 계층**으로 확장된다.

```
+===================================================================+
|  Grid 수준: 결과 행렬 C (M x N)                                    |
|                                                                    |
|  +---+---+---+---+                                                |
|  |   |   |   |   |   <-- 각 칸 = 1 Block의 출력 (BM x BN)         |
|  +---+---+---+---+       BM = 128, BN = 128                       |
|  |   |   |   |   |                                                |
|  +---+---+---+---+   Grid 크기: ceil(M/128) x ceil(N/128)         |
|  |   |   |   |   |                                                |
|  +---+---+---+---+                                                |
+===================================================================+

+===================================================================+
|  Block 수준: 하나의 블록 타일 (128 x 128)                           |
|                                                                    |
|  +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+               |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  <-- 16열     |
|  +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+               |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |               |
|  +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+  16행         |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |               |
|  +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+               |
|  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :  :               |
|  +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+               |
|                                                                    |
|  각 칸 = 1 Thread의 출력 (TM x TN = 8 x 8)                        |
|  16 x 16 = 256 스레드/블록                                         |
+===================================================================+

+===================================================================+
|  Thread 수준: 하나의 스레드 타일 (8 x 8)                            |
|                                                                    |
|  reg_C[0][0]  reg_C[0][1]  ...  reg_C[0][7]                       |
|  reg_C[1][0]  reg_C[1][1]  ...  reg_C[1][7]                       |
|  ...                                                               |
|  reg_C[7][0]  reg_C[7][1]  ...  reg_C[7][7]                       |
|                                                                    |
|  64개 float = 64개 레지스터 (모두 레지스터에 상주!)                  |
+===================================================================+
```

세 계층의 정합성을 확인하자.

```
Block 타일:   BM x BN = 128 x 128 = 16,384 원소/블록
Thread 타일:  TM x TN = 8 x 8     = 64 원소/스레드
스레드 수:    (BM/TM) x (BN/TN)   = 16 x 16 = 256 스레드/블록

검증: 256 * 64 = 16,384 = 128 * 128  (정합!)
```

K 차원은 BK=8 크기의 타일로 분할되어 반복된다. 각 K 반복에서:

```
K 반복:
  (1) 글로벌 --> 공유: A[BM x BK] = 128 x 8 = 1024 floats 로드
                       B[BK x BN] = 8 x 128 = 1024 floats 로드

  (2) 공유 --> 레지스터: 각 스레드가 reg_A[8], reg_B[8] 로드

  (3) 레지스터에서 연산: 8x8 = 64 FMA (외적 마이크로커널)
```

이 구조를 데이터 흐름으로 정리하면:

```
글로벌 메모리        공유 메모리           레지스터
(~400 cycle)    -->  (~20 cycle)      -->  (~1 cycle)
                                           |
A[M x K]  ------>   As[128 x 8]  ------>  reg_A[8]  -+
                                                      |--> reg_C[8][8] (64 FMA)
B[K x N]  ------>   Bs[8 x 128]  ------>  reg_B[8]  -+
```

---

### 3.3 외적(Outer Product) vs 내적(Dot Product) 마이크로커널

레지스터 블로킹의 핵심은 **외적(Outer Product)** 연산이다. 이를 내적(Dot Product)과 비교하여 왜 외적이 데이터 재사용에 유리한지 이해하자.

#### 내적(Dot Product) 방식

```
내적: C[i][j] = sum_k( A[i][k] * B[k][j] )

1개 결과를 계산하려면:
  A에서 K개 원소 로드
  B에서 K개 원소 로드
  총 2K 로드 --> 1개 결과 (K FMA)

데이터 재사용: 없음
각 로드한 값은 정확히 1번 사용됨
```

#### 외적(Outer Product) 방식

```
외적: reg_C[m][n] += reg_A[m] * reg_B[n]  (모든 m, n에 대해)

K 루프 1회에서:
  A에서 TM개 원소 로드 --> reg_A[0..7]
  B에서 TN개 원소 로드 --> reg_B[0..7]
  총 TM + TN = 16 로드 --> TM * TN = 64개 결과 갱신 (64 FMA)

데이터 재사용:
  reg_A[m]은 TN=8번 재사용 (reg_B[0..7]과 각각 곱해짐)
  reg_B[n]은 TM=8번 재사용 (reg_A[0..7]과 각각 곱해짐)
```

시각적으로 비교하면:

```
=== 내적 (Level 2) ===           === 외적 (Level 3) ===

A[k] * B[k] --> C[i][j]         reg_A[0..7]    reg_B[0..7]
                                      |              |
K번 반복, 1개 결과                     +--- 외적 ---+
                                      |
로드: 2K                         reg_C[8][8] = 64개 결과
결과: 1
재사용: 0                        로드: 16
                                 결과: 64
                                 재사용: 각 값 8회
```

이것이 Level 3의 "마이크로커널"이라 불리는 이유이다. 가장 안쪽의 핵심 연산 단위로서, **16번의 로드로 64번의 FMA를 수행**한다. 이 높은 연산 대 로드 비율이 산술 강도를 극적으로 높이는 원동력이다.

---

### 3.4 레지스터 사용량 분석

레지스터 블로킹의 대가는 **높은 레지스터 사용량**이다. 각 스레드가 사용하는 레지스터를 정밀하게 추적해 보자.

```
[레지스터 사용량 분석]

데이터 레지스터:
  reg_C[TM][TN] = reg_C[8][8] = 64 float = 64 레지스터
  reg_A[TM]     = reg_A[8]    = 8 float  = 8 레지스터
  reg_B[TN]     = reg_B[8]    = 8 float  = 8 레지스터
  소계: 80 레지스터

인덱스 및 루프 변수 (추정):
  tx, ty, tid                               = 3 레지스터
  bx, by                                    = 2 레지스터
  block_row_start, block_col_start          = 2 레지스터
  thread_row_start, thread_col_start        = 2 레지스터
  k_tile, k, m, n (루프 변수)               = 4 레지스터
  A_TILE_ROW, A_TILE_COL 등 (로드 인덱스)   = ~6 레지스터
  임시 변수 (주소 계산 등)                   = ~5 레지스터
  소계: ~24 레지스터

------------------------------------------
총 추정: ~90-104 레지스터/스레드
------------------------------------------
```

Ampere(SM 8.0) 아키텍처의 레지스터 제한:

```
[Ampere 레지스터 제한]

스레드당 최대 레지스터: 255
SM당 총 레지스터:       65,536

경고 임계값:
  <= 128 레지스터:  안전 (일반적인 최적 범위)
  129-200 레지스터: 점유율 감소, ILP로 보상 가능
  201-255 레지스터: 점유율 크게 감소, 주의 필요
  > 255 레지스터:   레지스터 스필링 발생! (치명적 성능 저하)
```

#### 레지스터 스필링(Register Spilling)

스레드의 레지스터 수요가 255를 초과하면, 컴파일러는 일부 값을 **로컬 메모리(Local Memory)**에 저장한다. 로컬 메모리는 실제로는 글로벌 메모리(DRAM)에 위치하므로, 접근 지연시간이 ~400 사이클로 급증한다.

```
[레지스터 스필링 확인 방법]

SASS 코드에서 다음 명령어를 검색한다:
  LDL (Load Local)   -- 로컬 메모리에서 레지스터로 읽기
  STL (Store Local)  -- 레지스터에서 로컬 메모리로 쓰기

이 명령어가 존재하면 스필링이 발생한 것이다!

확인 명령어:
  cuobjdump -sass sgemm_register_blocking.cubin | grep -E "LDL|STL"

Nsight Compute에서:
  Launch Statistics > Registers Per Thread
```

-- 참조: `docs/ANALYSIS_GUIDE.md:127-139`

---

### 3.5 점유율 트레이드오프

레지스터 사용량이 점유율에 미치는 영향을 정량적으로 계산한다.

```
[점유율 계산]

가정: 스레드당 90 레지스터, Ampere GPU

SM당 총 레지스터: 65,536
SM당 최대 스레드: 2,048 (= 64 워프)

레지스터에 의한 스레드 제한:
  65,536 / 90 = 728 스레드 (소수점 이하 버림)
  728 / 32 = 22 워프 (워프 단위로 정렬)
  실제: 22 * 32 = 704 스레드

점유율 = 704 / 2048 = 34.4%

만약 컴파일러가 104 레지스터를 사용한다면:
  65,536 / 104 = 630 --> 19 워프 --> 608 스레드
  점유율 = 608 / 2048 = 29.7%
```

이 점유율은 Level 2 타일링(~50-75%)보다 **상당히 낮다**. 그런데도 성능이 더 좋은 이유는 무엇인가?

```
[ILP vs TLP 트레이드오프]

TLP (Thread-Level Parallelism):
  더 많은 워프 --> 메모리 지연 시간을 워프 전환으로 은닉
  점유율 ↑ --> TLP ↑

ILP (Instruction-Level Parallelism):
  한 스레드 내에서 독립 명령어를 파이프라인으로 동시 실행
  레지스터에 64개 독립 FMA --> ILP ↑↑↑

Level 2: 높은 TLP (50-75% 점유율) + 낮은 ILP (1 FMA/반복)
Level 3: 낮은 TLP (25-35% 점유율) + 높은 ILP (64 FMA/반복)

핵심 통찰:
  64개의 독립적 FMA가 파이프라인을 가득 채우므로,
  워프 전환 없이도 연산 유닛이 유휴 상태가 되지 않는다.
  ILP가 TLP의 부족을 보상하고도 남는다!
```

-- 참조: `docs/ANALYSIS_GUIDE.md:157-163`

```
일반적인 오해: 높은 점유율 = 높은 성능

실제:
- 레지스터 blocking은 낮은 점유율로도 높은 성능 달성 가능
- ILP(Instruction-Level Parallelism)가 높으면 25-50% 점유율로 충분
- 메모리 바운드 커널은 높은 점유율이 유리
```

---

## 4. 코드 분석

이 절에서는 `src/kernels/sgemm_register_blocking.cu`의 기본 버전(라인 1-187)과 최적화 버전(라인 197-311)을 단계별로 분석한다. Level 3부터는 핵심 부분에 집중하고 나머지는 스스로 분석하는 부분 워크스루(Faded Worked Example) 방식을 적용한다.

### 4.1 타일 파라미터 정의 (라인 35-43)

```cuda
// src/kernels/sgemm_register_blocking.cu:35-43
#define BM 128          // Block tile M
#define BN 128          // Block tile N
#define BK 8            // Block tile K
#define TM 8            // Thread tile M
#define TN 8            // Thread tile N

#define THREADS_PER_BLOCK_X (BN / TN)  // 128 / 8 = 16
#define THREADS_PER_BLOCK_Y (BM / TM)  // 128 / 8 = 16
```

5개의 타일 파라미터가 전체 커널의 동작을 결정한다.

| 파라미터 | 값 | 역할 |
|---------|-----|------|
| BM | 128 | 블록이 담당하는 출력 M 방향 크기 |
| BN | 128 | 블록이 담당하는 출력 N 방향 크기 |
| BK | 8 | K 방향 타일 크기 (한 반복에서 처리하는 K 폭) |
| TM | 8 | 스레드가 담당하는 출력 M 방향 크기 |
| TN | 8 | 스레드가 담당하는 출력 N 방향 크기 |

파생되는 값:

```
스레드 블록 크기: (BN/TN) x (BM/TM) = 16 x 16 = 256 스레드
블록당 출력 원소: BM * BN = 128 * 128 = 16,384
스레드당 출력 원소: TM * TN = 8 * 8 = 64
공유 메모리 A 타일: BM * BK = 128 * 8 = 1,024 floats
공유 메모리 B 타일: BK * BN = 8 * 128 = 1,024 floats
K 방향 반복 횟수: ceil(K / BK)
```

---

### 4.2 스레드-출력 매핑 (라인 64-78)

```cuda
// src/kernels/sgemm_register_blocking.cu:64-78
const int tx = threadIdx.x;  // 0..15 (N 방향)
const int ty = threadIdx.y;  // 0..15 (M 방향)
const int tid = ty * blockDim.x + tx;  // 0..255 (선형 ID)

const int block_row_start = by * BM;       // 블록의 M 시작 위치
const int block_col_start = bx * BN;       // 블록의 N 시작 위치

const int thread_row_start = ty * TM;      // 0, 8, 16, ..., 120
const int thread_col_start = tx * TN;      // 0, 8, 16, ..., 120
```

스레드 (tx=3, ty=5)의 예시를 추적한다.

```
thread_row_start = 5 * 8 = 40
thread_col_start = 3 * 8 = 24

이 스레드가 담당하는 출력 영역:
  C[block_row_start + 40 .. 47][block_col_start + 24 .. 31]
  = 블록 내 8x8 = 64개 원소
```

전체 256개 스레드가 128x128 블록 타일을 빈틈없이 분할한다.

```
+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
|0,0 |0,1 |0,2 |0,3 |0,4 |0,5 |0,6 |0,7 |0,8 |0,9 |0,10|0,11|0,12|0,13|0,14|0,15|
+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
|1,0 |1,1 |1,2 |    |    |    |    |    |    |    |    |    |    |    |    |1,15|
+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+
|    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |
:    :    :    :    :    :    :    :    :    :    :    :    :    :    :    :    :
|15,0|15,1|    |    |    |    |    |    |    |    |    |    |    |    |    |15,15|
+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+

각 칸 (ty, tx) = TM x TN = 8 x 8 원소
16 x 16 칸 = 256 스레드
총 원소: 16*8 x 16*8 = 128 x 128 = 16,384 (블록 타일과 일치)
```

---

### 4.3 레지스터 선언 (라인 80-85)

```cuda
// src/kernels/sgemm_register_blocking.cu:80-85
float reg_C[TM][TN] = {{0.0f}};   // 64 floats: 축적기 (누적기)
float reg_A[TM];                    // 8 floats: A 프래그먼트
float reg_B[TN];                    // 8 floats: B 프래그먼트
```

`reg_C`는 0으로 초기화되어 전체 K 루프에 걸쳐 결과를 누적한다. `reg_A`와 `reg_B`는 매 K 반복마다 새로 로드되는 임시 값이다.

이 선언에서 주목할 점은 `reg_C`의 **이중 초기화 구문** `{{0.0f}}`이다. 이는 C++에서 2차원 배열의 모든 원소를 0으로 초기화하는 구문이다. 컴파일러는 이를 64개 레지스터의 초기화 코드로 변환한다.

---

### 4.4 협력적 타일 로딩 (라인 90-136)

이 부분은 256개 스레드가 **협력하여** 공유 메모리에 타일을 로드하는 구간이다. Level 2와 달리, 타일 크기가 스레드 수보다 크기 때문에 각 스레드가 **여러 원소**를 로드해야 한다.

#### A 타일 로딩 (BM x BK = 128 x 8 = 1024 원소)

```cuda
// src/kernels/sgemm_register_blocking.cu:90-121
const int A_TILE_THREAD_PER_ROW = BK;           // 8
const int A_TILE_ROW_STRIDE = 256 / 8;          // 32
const int A_TILE_ROW = tid / A_TILE_THREAD_PER_ROW;  // 0..31
const int A_TILE_COL = tid % A_TILE_THREAD_PER_ROW;  // 0..7

#pragma unroll
for (int load_offset = 0; load_offset < BM; load_offset += A_TILE_ROW_STRIDE) {
    int row_idx = A_TILE_ROW + load_offset;
    // ...
    As[row_idx][A_TILE_COL] = A[global_row * lda + global_col];
}
```

이 로딩 패턴을 구체적으로 분석한다.

```
A 타일: 128행 x 8열 = 1024 원소
스레드: 256개

스레드당 로드 수: 1024 / 256 = 4회

로딩 패턴 (A_TILE_ROW_STRIDE = 32):
  반복 0: load_offset = 0    --> 스레드 0..255가 행 0..31 로드
  반복 1: load_offset = 32   --> 스레드 0..255가 행 32..63 로드
  반복 2: load_offset = 64   --> 스레드 0..255가 행 64..95 로드
  반복 3: load_offset = 96   --> 스레드 0..255가 행 96..127 로드

예시: tid = 13
  A_TILE_ROW = 13 / 8 = 1
  A_TILE_COL = 13 % 8 = 5

  반복 0: As[ 1][5]  = A[...] (행 1, 열 5)
  반복 1: As[33][5]  = A[...]
  반복 2: As[65][5]  = A[...]
  반복 3: As[97][5]  = A[...]
```

#### B 타일 로딩 (BK x BN = 8 x 128 = 1024 원소)

```cuda
// src/kernels/sgemm_register_blocking.cu:98-136
const int B_TILE_THREAD_PER_ROW = BN;           // 128
const int B_TILE_ROW_STRIDE = 256 / 128;        // 2

#pragma unroll
for (int load_offset = 0; load_offset < BK; load_offset += B_TILE_ROW_STRIDE) {
    int row_idx = B_TILE_ROW + load_offset;
    // ...
    Bs[row_idx][B_TILE_COL] = B[global_row * ldb + global_col];
}
```

```
B 타일: 8행 x 128열 = 1024 원소
B_TILE_ROW_STRIDE = 256 / 128 = 2

반복 0: load_offset = 0  --> 스레드 0..255가 행 0..1 로드
반복 1: load_offset = 2  --> 스레드 0..255가 행 2..3 로드
반복 2: load_offset = 4  --> 스레드 0..255가 행 4..5 로드
반복 3: load_offset = 6  --> 스레드 0..255가 행 6..7 로드

B_TILE_THREAD_PER_ROW = 128이므로:
  tid 0..127  --> 행 0의 열 0..127
  tid 128..255 --> 행 1의 열 0..127
```

B 타일 로딩에서는 같은 워프 내 32개 스레드가 연속된 열을 접근하므로, 공유 메모리 쓰기 시 뱅크 충돌이 발생하지 않는다.

---

### 4.5 외적 마이크로커널: 핵심 혁신 (라인 140-166)

**이 구간이 Level 3의 핵심이다.**

```cuda
// src/kernels/sgemm_register_blocking.cu:140-166
__syncthreads();

#pragma unroll
for (int k = 0; k < BK; ++k) {
    // (1) A 프래그먼트 로드: 공유 메모리 --> 레지스터
    #pragma unroll
    for (int m = 0; m < TM; ++m) {
        reg_A[m] = As[thread_row_start + m][k];
    }

    // (2) B 프래그먼트 로드: 공유 메모리 --> 레지스터
    #pragma unroll
    for (int n = 0; n < TN; ++n) {
        reg_B[n] = Bs[k][thread_col_start + n];
    }

    // (3) 외적: 레지스터 --> 레지스터 (64 FMA!)
    #pragma unroll
    for (int m = 0; m < TM; ++m) {
        #pragma unroll
        for (int n = 0; n < TN; ++n) {
            reg_C[m][n] += reg_A[m] * reg_B[n];
        }
    }
}
```

각 단계를 K=0에서의 구체적 스레드(ty=5, tx=3)에 대해 추적한다.

```
thread_row_start = 40, thread_col_start = 24, k = 0

(1) A 프래그먼트 로드:
  reg_A[0] = As[40][0]
  reg_A[1] = As[41][0]
  reg_A[2] = As[42][0]
  reg_A[3] = As[43][0]
  reg_A[4] = As[44][0]
  reg_A[5] = As[45][0]
  reg_A[6] = As[46][0]
  reg_A[7] = As[47][0]
  --> A 타일의 열 0에서 8개 연속 행을 읽음 (수직 방향)

(2) B 프래그먼트 로드:
  reg_B[0] = Bs[0][24]
  reg_B[1] = Bs[0][25]
  reg_B[2] = Bs[0][26]
  reg_B[3] = Bs[0][27]
  reg_B[4] = Bs[0][28]
  reg_B[5] = Bs[0][29]
  reg_B[6] = Bs[0][30]
  reg_B[7] = Bs[0][31]
  --> B 타일의 행 0에서 8개 연속 열을 읽음 (수평 방향)

(3) 외적 (64 FMA):
  reg_C[0][0] += reg_A[0] * reg_B[0]  = As[40][0] * Bs[0][24]
  reg_C[0][1] += reg_A[0] * reg_B[1]  = As[40][0] * Bs[0][25]
  reg_C[0][2] += reg_A[0] * reg_B[2]  = As[40][0] * Bs[0][26]
  ...
  reg_C[0][7] += reg_A[0] * reg_B[7]  = As[40][0] * Bs[0][31]
  reg_C[1][0] += reg_A[1] * reg_B[0]  = As[41][0] * Bs[0][24]
  ...
  reg_C[7][7] += reg_A[7] * reg_B[7]  = As[47][0] * Bs[0][31]

  총: 8 * 8 = 64 FMA = 128 FLOPs!
```

외적의 **데이터 재사용** 패턴을 시각화하면:

```
        reg_B[0]  reg_B[1]  reg_B[2]  ...  reg_B[7]
           |         |         |              |
reg_A[0]--[*]-------[*]-------[*]----...----[*]  --> reg_A[0]은 8회 사용
reg_A[1]--[*]-------[*]-------[*]----...----[*]  --> reg_A[1]은 8회 사용
reg_A[2]--[*]-------[*]-------[*]----...----[*]  --> reg_A[2]은 8회 사용
...        |         |         |              |
reg_A[7]--[*]-------[*]-------[*]----...----[*]  --> reg_A[7]은 8회 사용
           |         |         |              |
       8회 사용  8회 사용  8회 사용       8회 사용

16번 로드 --> 64번 연산 (4:1 비율)
```

#### `#pragma unroll`의 역할

이 코드에서 5개의 `#pragma unroll`이 사용되었다. 컴파일러에게 루프를 완전히 펼치도록 지시하는 것이다.

`#pragma unroll`이 없을 경우:

```
분기 명령어 (LOOP):  k < BK 비교, 조건부 점프
인덱스 계산:         k++, m++, n++ 증가
의존성:              루프 카운터에 대한 데이터 의존성

--> 파이프라인 버블 발생, ILP 저하
```

`#pragma unroll`이 있을 경우:

```
BK=8, TM=8, TN=8 루프가 완전히 펼쳐짐
분기 명령어: 없음
인덱스 계산: 상수로 치환 (컴파일 타임 계산)
의존성: 독립 FMA만 남음

--> 64개 FFMA 명령어가 연속 배치, 최대 ILP
```

---

### 4.6 결과 저장 (라인 171-186)

```cuda
// src/kernels/sgemm_register_blocking.cu:171-186
#pragma unroll
for (int m = 0; m < TM; ++m) {
    const int global_row = block_row_start + thread_row_start + m;
    if (global_row < M) {
        #pragma unroll
        for (int n = 0; n < TN; ++n) {
            const int global_col = block_col_start + thread_col_start + n;
            if (global_col < N) {
                float c_val = C[global_row * ldc + global_col];
                C[global_row * ldc + global_col] =
                    alpha * reg_C[m][n] + beta * c_val;
            }
        }
    }
}
```

이 구간은 64개의 결과를 레지스터에서 글로벌 메모리로 기록한다. `alpha`와 `beta`를 적용하는 BLAS 표준 인터페이스(`C = alpha * A * B + beta * C`)를 따른다.

경계 검사(`if (global_row < M)`, `if (global_col < N)`)는 행렬 크기가 BM 또는 BN의 배수가 아닌 경우를 처리한다.

---

### 4.7 최적화 버전: 패딩과 인라인 PTX FMA (라인 197-311)

최적화 버전은 두 가지를 개선한다.

#### (1) +1 패딩으로 뱅크 충돌 방지

```cuda
// src/kernels/sgemm_register_blocking.cu:210-212
__shared__ float As[BM][BK + 1];   // 128 x 9 (+1 패딩)
__shared__ float Bs[BK][BN + 1];   // 8 x 129 (+1 패딩)
```

기본 버전의 `As[128][8]`에서 열 수 8은 공유 메모리의 32개 뱅크에 대해:

```
뱅크 매핑 (패딩 없이, BK=8):
  As[row][0] --> 뱅크 (row * 8 + 0) % 32
  As[row][1] --> 뱅크 (row * 8 + 1) % 32
  ...

같은 열(k)에 대해:
  As[0][k], As[1][k], As[2][k], As[3][k] 접근 시:
  뱅크 = (0*8+k)%32, (1*8+k)%32, (2*8+k)%32, (3*8+k)%32
       = k, 8+k, 16+k, 24+k
  --> 4개 모두 다른 뱅크 (충돌 없음)

  그러나 As[0][k]와 As[4][k]:
  뱅크 = k%32, (32+k)%32 = k, k
  --> 같은 뱅크! (충돌 발생!)
```

`As[128][8+1]`로 패딩하면 행 간격이 9로 바뀌어:

```
뱅크 매핑 (패딩 있음, BK+1=9):
  As[0][k] --> 뱅크 (0*9 + k) % 32
  As[4][k] --> 뱅크 (4*9 + k) % 32 = (36 + k) % 32 = (4 + k) % 32

  k=0일 때: 뱅크 0 vs 뱅크 4  --> 충돌 없음!
```

#### (2) 인라인 PTX FMA 명령어

```cuda
// src/kernels/sgemm_register_blocking.cu:283-288
asm volatile(
    "fma.rn.f32 %0, %1, %2, %0;"
    : "+f"(rC[m][n])          // 입출력: 결과 누적기
    : "f"(rA[m]), "f"(rB[n])  // 입력: A, B 프래그먼트
);
```

이 인라인 어셈블리는 컴파일러에게 **반드시 FMA 명령어를 사용하도록** 강제한다.

| 요소 | 의미 |
|------|------|
| `fma.rn.f32` | 단정밀도 FMA, 최근접 짝수 반올림 |
| `%0` | 첫 번째 오퍼랜드: `rC[m][n]` (읽기+쓰기) |
| `%1` | 두 번째 오퍼랜드: `rA[m]` (읽기 전용) |
| `%2` | 세 번째 오퍼랜드: `rB[n]` (읽기 전용) |
| `"+f"` | float 레지스터, 입출력 겸용 |
| `"f"` | float 레지스터, 입력 전용 |

컴파일러가 `a * b + c`를 별도의 곱셈과 덧셈으로 분리하는 것을 방지한다. FMA는 곱셈과 덧셈을 **단일 사이클**에 수행하므로 2배의 처리량을 얻고, 중간 반올림 오류도 방지한다.

---

## 5. 왜 이것이 작동하는가? (Elaborative Interrogation)

아래 5개의 심층 질문에 대해 먼저 스스로 답해 본 후, 하단의 힌트와 설명을 확인하라.

### 질문 1: 산술 강도가 정확히 2.0이 되는 수학적 증명

BK 루프 1회에서 한 스레드의 동작을 정리하라.

<details>
<summary>풀이 보기</summary>

```
BK 루프의 k-th 반복에서:

로드:
  reg_A[0..7] = As[thread_row_start+0..7][k]  --> 8 floats = 32 bytes
  reg_B[0..7] = Bs[k][thread_col_start+0..7]  --> 8 floats = 32 bytes
  합계: 64 bytes

연산:
  reg_C[m][n] += reg_A[m] * reg_B[n]  (m=0..7, n=0..7)
  --> 8 * 8 = 64 FMA = 128 FLOPs

산술 강도 = 128 FLOPs / 64 bytes = 2.0 FLOP/byte

BK=8 반복 전체:
  로드: 8 * 64 = 512 bytes
  연산: 8 * 128 = 1024 FLOPs
  산술 강도 = 1024 / 512 = 2.0 FLOP/byte (변하지 않음)
```

산술 강도는 BK에 무관하게 일정하다. 이는 매 반복에서 로드와 연산의 비율이 동일하기 때문이다. 산술 강도를 높이려면 TM과 TN을 키워야 한다.

</details>

---

### 질문 2: 외적이 내적보다 데이터 재사용에 유리한 이유

reg_A와 reg_B 관점에서, 한 번 로드한 값이 몇 번 사용되는지 비교하라.

<details>
<summary>풀이 보기</summary>

**내적 방식** (Level 2):

```
C[i][j]의 계산: sum_k( A[i][k] * B[k][j] )
  A[i][k]를 로드 --> B[k][j]와 1번 곱한 후 버림
  B[k][j]를 로드 --> A[i][k]와 1번 곱한 후 버림
  재사용 횟수: 1 (재사용 없음)
```

**외적 방식** (Level 3):

```
reg_A[m]을 로드 (m=0..7) --> reg_B[0..7] 각각과 곱함
  reg_A[m] 재사용 횟수: TN = 8

reg_B[n]을 로드 (n=0..7) --> reg_A[0..7] 각각과 곱함
  reg_B[n] 재사용 횟수: TM = 8
```

핵심은 레지스터에 값을 **유지한 채** 여러 연산에 반복 사용한다는 점이다. 레지스터 접근은 ~1 사이클이므로 재사용 비용이 사실상 0이다. 공유 메모리에서 다시 읽는 것(~20 사이클)과 비교하면 극적인 차이가 있다.

</details>

---

### 질문 3: A_TILE_ROW_STRIDE = 32의 의미

256개 스레드가 128x8 = 1024개 원소를 로드할 때, 스트라이드가 32인 이유를 설명하라.

<details>
<summary>풀이 보기</summary>

```
A 타일 크기: 128행 x 8열
A_TILE_THREAD_PER_ROW = BK = 8 (한 행에 8개 열 = 8개 스레드가 1행을 담당)

256개 스레드를 8열로 배치하면:
  256 / 8 = 32행을 한 번에 로드할 수 있음

따라서 A_TILE_ROW_STRIDE = 32

128행을 로드하려면: 128 / 32 = 4번 반복이 필요

반복 0: 행 0..31   (256 스레드가 32행 x 8열 = 256원소 로드)
반복 1: 행 32..63  (256 스레드가 32행 x 8열 = 256원소 로드)
반복 2: 행 64..95  (256 스레드가 32행 x 8열 = 256원소 로드)
반복 3: 행 96..127 (256 스레드가 32행 x 8열 = 256원소 로드)

총: 4 * 256 = 1024 원소 (128 x 8과 일치)
```

스트라이드 32는 "256개 스레드가 한 번에 32개 행을 처리할 수 있으므로, 128행 전체를 채우려면 32행씩 건너뛰며 4번 반복해야 한다"는 의미이다.

</details>

---

### 질문 4: 점유율이 25%로 떨어져도 성능이 좋은 구체적 이유

ILP 관점에서 64개 독립 FMA가 어떻게 파이프라인을 채우는지 설명하라.

<details>
<summary>풀이 보기</summary>

GPU의 FMA 파이프라인은 여러 사이클의 깊이를 가진다. 명령어 A가 발행되고 결과가 준비되기까지 수 사이클이 걸린다. 이 동안 파이프라인이 빈 상태가 되면 처리량이 낭비된다.

**Level 2 (1 FMA/반복)**:

```
사이클 0: FMA 발행 (reg_C += A * B)
사이클 1: FMA 결과 대기... (reg_C 사용 불가)
사이클 2: FMA 결과 대기...
사이클 3: FMA 완료, 다음 K 반복의 로드 시작
사이클 4: 공유 메모리 로드 대기...
...
--> 파이프라인 대부분이 비어있음 (버블 다수)
--> 다른 워프로 전환하여 은닉 (TLP 의존)
```

**Level 3 (64 FMA/반복)**:

```
사이클 0: FMA_00 발행 (reg_C[0][0] += reg_A[0] * reg_B[0])
사이클 1: FMA_01 발행 (reg_C[0][1] += reg_A[0] * reg_B[1])  <-- 독립!
사이클 2: FMA_02 발행 (reg_C[0][2] += reg_A[0] * reg_B[2])  <-- 독립!
...
사이클 63: FMA_77 발행 (reg_C[7][7] += reg_A[7] * reg_B[7])
사이클 64: 다음 K의 로드 시작, FMA_00 결과 이미 준비됨
--> 파이프라인이 꽉 차 있음 (버블 거의 없음)
--> 워프 전환 없이도 유닛 활용도 높음
```

64개의 FMA는 서로 다른 reg_C 원소를 갱신하므로 데이터 의존성이 **전혀 없다**. 따라서 파이프라인에 연속으로 발행할 수 있고, 파이프라인 깊이보다 발행할 명령어가 많으므로 하드웨어가 최대 속도로 동작한다.

점유율 25%에서는 활성 워프가 적어 TLP가 낮지만, 각 워프 내에서 ILP가 충분히 높으므로 연산 유닛이 유휴 상태가 되지 않는다.

</details>

---

### 질문 5: `#pragma unroll`이 없으면 성능이 떨어지는 이유

컴파일러가 루프를 펼치지 않을 때 발생하는 오버헤드를 구체적으로 나열하라.

<details>
<summary>풀이 보기</summary>

`#pragma unroll` 없이 루프가 유지될 경우의 오버헤드:

1. **분기 오버헤드**: 매 반복마다 `k < BK`, `m < TM`, `n < TN` 비교 및 조건부 점프 명령어가 실행된다. 3중 루프이므로 반복당 3번의 비교+점프가 필요하다.

2. **인덱스 증가 오버헤드**: `k++`, `m++`, `n++` 증가 명령어. 이들은 FMA와 무관한 "쓸모없는" 연산이다.

3. **ILP 저해**: 루프 카운터에 대한 데이터 의존성이 생긴다. `n++`의 결과를 다음 반복의 `n < TN` 비교에서 사용해야 하므로, 다음 FMA 발행이 지연된다.

4. **주소 계산 오버헤드**: `As[thread_row_start + m][k]`에서 `m`이 상수가 아니면 매번 덧셈이 필요하다. 언롤링 시 `m`이 0, 1, 2, ..., 7로 컴파일 타임 상수가 되어 주소가 미리 계산된다.

5. **레지스터 할당 비효율**: 컴파일러가 루프 변수를 위한 레지스터를 별도로 확보해야 하며, 레지스터 할당 최적화 범위가 루프 내부로 제한된다.

실험적으로, `#pragma unroll`을 제거하면 성능이 20-40% 저하되는 것이 관측된다. 이는 연산 파이프라인의 활용도가 분기와 인덱스 연산으로 인해 크게 떨어지기 때문이다.

</details>

---

## 6. 시뮬레이션 7: 외적 연산 추적기

이 시뮬레이션에서는 축소된 크기(TM=4, TN=4)로 외적 마이크로커널의 동작을 한 단계씩 추적하고, 크기에 따른 산술 강도 변화를 분석한다.

### 6.1 축소 버전: TM=4, TN=4

다음 값이 공유 메모리에서 레지스터로 로드되었다고 가정한다.

```
reg_A = [a0, a1, a2, a3]
reg_B = [b0, b1, b2, b3]
```

외적 연산 후 reg_C의 상태를 완성하라.

```
reg_C (4x4 행렬):

          reg_B[0]    reg_B[1]    reg_B[2]    reg_B[3]
          (b0)        (b1)        (b2)        (b3)
        +----------+-----------+-----------+-----------+
reg_A[0]| a0 * b0  |  a0 * b1  |  a0 * b2  |  a0 * b3 |
  (a0)  +----------+-----------+-----------+-----------+
reg_A[1]| a1 * b0  |  a1 * b1  |  a1 * b2  |  a1 * b3 |
  (a1)  +----------+-----------+-----------+-----------+
reg_A[2]| a2 * b0  |  a2 * b1  |  a2 * b2  |  a2 * b3 |
  (a2)  +----------+-----------+-----------+-----------+
reg_A[3]| a3 * b0  |  a3 * b1  |  a3 * b2  |  a3 * b3 |
  (a3)  +----------+-----------+-----------+-----------+
```

산술 강도를 계산한다.

```
로드: 4 + 4 = 8 floats = 32 bytes
FMA:  4 * 4 = 16 FMA = 32 FLOPs
산술 강도 = 32 / 32 = 1.0 FLOP/byte
```

### 6.2 실제 버전: TM=8, TN=8

동일한 계산을 TM=8, TN=8에 대해 수행한다.

```
로드: 8 + 8 = 16 floats = 64 bytes
FMA:  8 * 8 = 64 FMA = 128 FLOPs
산술 강도 = 128 / 64 = 2.0 FLOP/byte
```

### 6.3 크기별 비교 표

| TM | TN | 로드 (floats) | 로드 (bytes) | FMA 수 | FLOPs | 산술 강도 (FLOP/byte) | 레지스터 (C만) |
|----|----|----|----|----|----|----|-----|
| 1  | 1  | 2  | 8  | 1  | 2  | 0.25 | 1   |
| 2  | 2  | 4  | 16 | 4  | 8  | 0.50 | 4   |
| 4  | 4  | 8  | 32 | 16 | 32 | 1.00 | 16  |
| 8  | 8  | 16 | 64 | 64 | 128| 2.00 | 64  |
| 16 | 16 | 32 | 128| 256| 512| 4.00 | 256 |

마지막 행(TM=TN=16)에 주목하라. 산술 강도는 4.0으로 우수하지만, C 축적기만 256개 레지스터가 필요하다. 여기에 reg_A(16), reg_B(16), 인덱스 변수(~20)를 더하면 ~308개로 **255를 초과**한다. 이 경우 레지스터 스필링이 발생하여 오히려 성능이 저하된다.

### 6.4 산술 강도 변화 그래프 (ASCII)

```
산술 강도
(FLOP/byte)
    |
4.0 |                                          * (TM=TN=16, 스필링 위험!)
    |
3.0 |
    |
2.0 |                        * (TM=TN=8, 최적점)
    |
1.0 |          * (TM=TN=4)
    |
0.5 |    * (TM=TN=2)
0.25| * (TM=TN=1, Level 2)
    +----+----+----+----+----+----+----+--->  T (= TM = TN)
         1    2    4    6    8   12   16

산술 강도 = T / 4  (TM = TN = T일 때)

최적 구간: TM=TN=8
  - 산술 강도 2.0 (충분히 높음)
  - 레지스터 ~90개 (255 이내)
  - 점유율 ~25-35% (ILP로 보상 가능)
```

### 6.5 비대칭 타일 연습

TM과 TN이 다른 경우를 직접 계산해 보라.

**문제**: TM=4, TN=16일 때의 산술 강도, 레지스터 사용량, Level 3 대비 변화를 계산하라.

<details>
<summary>풀이 보기</summary>

```
로드: TM + TN = 4 + 16 = 20 floats = 80 bytes
FMA:  TM * TN = 4 * 16 = 64 FMA = 128 FLOPs
산술 강도 = 128 / 80 = 1.6 FLOP/byte

레지스터:
  reg_C[4][16] = 64 레지스터 (TM=8, TN=8과 동일!)
  reg_A[4]     = 4 레지스터
  reg_B[16]    = 16 레지스터
  인덱스 등    = ~24 레지스터
  합계: ~108 레지스터

비교 (TM=8, TN=8 vs TM=4, TN=16):
  - FMA 수: 64 vs 64 (동일)
  - 산술 강도: 2.0 vs 1.6 (TM=4,TN=16이 낮음)
  - 레지스터(C만): 64 vs 64 (동일)
  - 총 레지스터: ~90 vs ~108 (TM=4,TN=16이 더 많음)
```

결론: TM=TN일 때 산술 강도가 최대가 된다. 이는 일반 공식 `TM*TN / (2*(TM+TN))`에서 TM+TN이 최소가 되는 조건(TM*TN 고정 시 TM=TN)과 일치한다. AM-GM 부등식에 의해 `TM + TN >= 2*sqrt(TM*TN)`이며, 등호 조건은 TM=TN이다.

</details>

---

## 7. 핵심 정리

1. **핵심 혁신**: 1스레드가 **64개 원소**를 계산한다 (Level 2의 1원소에서 64배 증가). 이를 **스레드 조잡화(Thread Coarsening)** 또는 **레지스터 블로킹(Register Blocking)**이라 한다.

2. **산술 강도**: 0.25에서 **2.0 FLOP/byte**로 **8배 향상**되었다. 일반 공식은 `AI = TM * TN / (2 * (TM + TN))`이며, TM=TN=T일 때 `AI = T/4`이다.

3. **3계층 타일 구조**: Block(128x128) -> Thread(8x8) -> K(BK=8). 256 스레드가 16,384개 출력 원소를 분담한다.

4. **외적(Outer Product) 마이크로커널**: 16번의 공유 메모리 로드(reg_A[8] + reg_B[8])로 **64번의 FMA**를 수행한다. 각 로드 값은 레지스터에서 **8회 재사용**된다.

5. **레지스터 사용량**: 약 **90개/스레드** (reg_C 64개 + reg_A 8개 + reg_B 8개 + 인덱스 등 ~10개). Ampere 한계인 255 이내이므로 스필링이 발생하지 않는다.

6. **점유율 트레이드오프**: 점유율 약 **25-35%**로 낮아지지만, 64개 독립 FMA에 의한 높은 **ILP(명령어 수준 병렬성)**가 이를 보상한다.

7. **최적화 버전**: +1 패딩으로 뱅크 충돌을 방지하고, 인라인 PTX `fma.rn.f32`로 FMA 사용을 보장한다.

8. **예상 성능**: cuBLAS 대비 **50-70%**. Level 2(20-40%)에서 크게 도약한다.

---

## 8. 퀴즈

### 문제 1 (Remember)

**질문**: TM=8, TN=8일 때 한 스레드가 계산하는 C 원소의 수는 몇 개인가?

<details>
<summary>정답 보기</summary>

**64개**. TM * TN = 8 * 8 = 64이다. 이 64개의 값은 모두 `reg_C[8][8]` 배열에 레지스터로 유지되며, 전체 K 루프에 걸쳐 누적된 후 한 번에 글로벌 메모리에 기록된다.

참조: `src/kernels/sgemm_register_blocking.cu:81` (`float reg_C[TM][TN]`)

</details>

---

### 문제 2 (Understand)

**질문**: 레지스터 블로킹이 산술 강도를 높이는 원리를 수식으로 설명하라. 산술 강도의 일반 공식을 유도하고, TM=TN=1 (Level 2)과 TM=TN=8 (Level 3)을 대입하여 비교하라.

<details>
<summary>정답 보기</summary>

K 루프 1회에서:

```
로드량: (TM + TN) * 4 bytes  (reg_A와 reg_B에 로드)
FLOPs:  TM * TN * 2          (FMA 하나당 2 FLOPs)

산술 강도 = (TM * TN * 2) / ((TM + TN) * 4)
          = TM * TN / (2 * (TM + TN))
```

대입:

```
TM=TN=1: AI = 1*1 / (2*(1+1)) = 1/4 = 0.25 FLOP/byte
TM=TN=8: AI = 8*8 / (2*(8+8)) = 64/32 = 2.0 FLOP/byte
```

비율: 2.0 / 0.25 = 8배 향상.

핵심: 분모(로드량)는 TM+TN으로 **선형** 증가하지만, 분자(FLOPs)는 TM*TN으로 **이차** 증가한다. 따라서 타일 크기를 키울수록 산술 강도가 향상된다.

</details>

---

### 문제 3 (Apply)

**질문**: BM=128, BN=128, BK=8일 때 블록당 공유 메모리 사용량을 계산하라. 기본 버전과 패딩 버전 각각에 대해 계산하라.

<details>
<summary>정답 보기</summary>

**기본 버전**:

```
As[BM][BK] = As[128][8]   = 128 * 8   = 1024 floats = 4096 bytes
Bs[BK][BN] = Bs[8][128]   = 8 * 128   = 1024 floats = 4096 bytes
합계: 2048 floats = 8192 bytes = 8 KB
```

**패딩 버전(+1)**:

```
As[BM][BK+1] = As[128][9]  = 128 * 9  = 1152 floats = 4608 bytes
Bs[BK][BN+1] = Bs[8][129]  = 8 * 129  = 1032 floats = 4128 bytes
합계: 2184 floats = 8736 bytes = 약 8.5 KB
```

Ampere SM당 최대 공유 메모리: 164 KB. 8.5 KB는 이의 5.2%에 불과하므로 공유 메모리는 점유율의 병목이 아니다. 레지스터가 주된 제한 요인이다.

</details>

---

### 문제 4 (Apply)

**질문**: 256개 스레드가 A 타일의 1024 원소를 로드할 때, 스레드당 로드 횟수는 몇 회인가? B 타일에 대해서도 동일하게 답하라.

<details>
<summary>정답 보기</summary>

**A 타일**:

```
A 타일 크기: BM * BK = 128 * 8 = 1024 원소
스레드 수: 256
스레드당 로드 횟수: 1024 / 256 = 4회

루프 반복 수: BM / A_TILE_ROW_STRIDE = 128 / 32 = 4
(각 반복에서 스레드가 1개씩 로드하므로 총 4회)
```

**B 타일**:

```
B 타일 크기: BK * BN = 8 * 128 = 1024 원소
스레드 수: 256
스레드당 로드 횟수: 1024 / 256 = 4회

루프 반복 수: BK / B_TILE_ROW_STRIDE = 8 / 2 = 4
```

A와 B 모두 스레드당 정확히 **4회** 로드한다. 총 글로벌 메모리 로드량은 2048 floats = 8192 bytes/블록/K반복이다.

</details>

---

### 문제 5 (Analyze)

**질문**: `A_TILE_ROW_STRIDE = 256/8 = 32`의 의미를 256개 스레드의 로드 패턴 관점에서 설명하라. 왜 `256/BK`인가?

<details>
<summary>정답 보기</summary>

A 타일의 형태는 128행 x 8열이다.

```
스레드 배치:
  tid를 8열(=BK)로 나눈다:
    A_TILE_ROW = tid / 8   (행 인덱스: 0..31)
    A_TILE_COL = tid % 8   (열 인덱스: 0..7)

256개 스레드가 8개 열에 배치되면:
  256 / 8 = 32개 행을 한 번에 로드할 수 있다.

이것이 A_TILE_ROW_STRIDE = 32인 이유이다.
```

"스트라이드 32"는 한 번의 로드 반복에서 스레드들이 커버하는 행 범위(32행)를 의미한다. 128행 전체를 로드하려면 stride=32씩 4번 반복해야 한다.

만약 BK=16이었다면 A_TILE_ROW_STRIDE = 256/16 = 16이 되고, 128/16 = 8번 반복이 필요하다. BK가 작을수록 한 번에 더 많은 행을 로드할 수 있어 로드 반복 횟수가 줄어든다.

</details>

---

### 문제 6 (Analyze)

**질문**: 레지스터 사용량이 255개를 초과하면 어떤 현상이 발생하며, SASS 코드에서 이를 어떻게 확인할 수 있는가?

<details>
<summary>정답 보기</summary>

**발생 현상**: **레지스터 스필링(Register Spilling)**

스레드의 레지스터 수요가 하드웨어 한계(255)를 초과하면, 컴파일러는 일부 레지스터 값을 **로컬 메모리(Local Memory)**에 저장한다. 로컬 메모리는 물리적으로 글로벌 메모리(DRAM)에 위치하며, 접근 지연시간이 ~400-600 사이클이다. 이는 레지스터의 ~1 사이클과 비교하면 수백 배 느리다.

**SASS 확인 방법**:

```bash
cuobjdump -sass <cubin파일> | grep -E "LDL|STL"
```

- `LDL` (Load Local): 로컬 메모리에서 레지스터로 읽기
- `STL` (Store Local): 레지스터에서 로컬 메모리로 쓰기

이 명령어가 존재하면 스필링이 발생한 것이다.

**Nsight Compute에서**: `Launch Statistics` > `Registers Per Thread` 항목에서 스레드당 레지스터 수를 확인한다. 또한 `Local Memory Per Thread` 값이 0보다 크면 스필링을 의심해야 한다.

**해결책** (참조: `docs/ANALYSIS_GUIDE.md:135-139`):
1. `__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)` 지시자 사용
2. TM, TN 크기 줄이기 (예: 8x8 -> 8x4)
3. 불필요한 변수의 범위(scope) 최소화

</details>

---

### 문제 7 (Evaluate)

**질문**: 점유율이 25%인 Level 3 커널이 점유율 50%인 Level 2 커널보다 빠른 이유를 ILP와 산술 강도 관점에서 분석하라.

<details>
<summary>정답 보기</summary>

두 관점에서 비교한다.

**1. 산술 강도 관점**:

```
Level 2 (50% 점유율):
  AI = 0.25 FLOP/byte
  성능 상한 = 피크대역폭 * 0.25
  --> 메모리 바운드: 연산 유닛의 대부분이 유휴

Level 3 (25% 점유율):
  AI = 2.0 FLOP/byte
  성능 상한 = 피크대역폭 * 2.0 (또는 피크연산, 더 작은 쪽)
  --> 연산 바운드로 전환: 메모리 대역폭에 여유 발생
```

Level 2는 메모리가 병목이므로 점유율을 높여도 대역폭이 증가하지 않는 한 성능이 개선되지 않는다. Level 3는 산술 강도가 8배 높아 동일한 대역폭으로 8배 더 많은 FLOPs를 수행할 수 있다.

**2. ILP 관점**:

```
Level 2:
  K 반복당 1 FMA --> 다음 FMA는 다음 K 반복의 로드에 의존
  파이프라인 활용: 낮음
  워프 전환으로 은닉 시도 (TLP 의존)

Level 3:
  K 반복당 64 FMA --> 모두 독립적 (reg_C의 서로 다른 원소)
  파이프라인 활용: 높음 (64개 명령어가 파이프라인을 가득 채움)
  워프 전환 없이도 유닛 활용도 극대화 (ILP 주도)
```

결론: 높은 점유율은 TLP를 통해 **메모리 지연시간을 은닉**하는 메커니즘이다. 그러나 Level 3에서는 (1) 산술 강도가 높아 메모리 요청 자체가 적고, (2) ILP가 높아 워프 전환 없이도 파이프라인이 채워지므로, 점유율이 낮아도 성능이 우수하다.

</details>

---

### 문제 8 (Create)

**질문**: TM=4, TN=16으로 변경 시 레지스터 사용량과 산술 강도의 변화를 계산하고, TM=8, TN=8 대비 어떤 장단점이 있는지 분석하라.

<details>
<summary>정답 보기</summary>

**레지스터 사용량**:

```
reg_C[4][16]  = 64 레지스터
reg_A[4]      = 4 레지스터
reg_B[16]     = 16 레지스터
인덱스 등     = ~24 레지스터
합계: ~108 레지스터
```

**산술 강도**:

```
AI = TM * TN / (2 * (TM + TN))
   = 4 * 16 / (2 * (4 + 16))
   = 64 / 40
   = 1.6 FLOP/byte
```

**비교 (TM=8, TN=8 vs TM=4, TN=16)**:

| 항목 | TM=8, TN=8 | TM=4, TN=16 |
|------|------------|-------------|
| FMA 수/K반복 | 64 | 64 |
| 로드 수/K반복 | 16 | 20 |
| 산술 강도 | 2.0 | 1.6 |
| reg_C | 64 | 64 |
| reg_A + reg_B | 16 | 20 |
| 총 레지스터(추정) | ~90 | ~108 |
| 스레드 배치 (BM/TM x BN/TN) | 16x16 | 32x8 |

**장점 (TM=4, TN=16)**:
- M 방향으로 더 많은 스레드(32)가 배치되어, M이 작은 행렬에서 작업 분배가 균일할 수 있다.

**단점 (TM=4, TN=16)**:
- 산술 강도가 2.0에서 1.6으로 **20% 감소**한다.
- 레지스터 사용량이 ~90에서 ~108로 증가하여 점유율이 더 낮아진다.
- B 프래그먼트(reg_B[16])의 공유 메모리 로드 시 같은 행에서 16개 연속 원소를 읽으므로 뱅크 충돌 패턴이 달라질 수 있다.

결론: TM=TN인 정사각형 타일이 산술 강도 관점에서 최적이다. 비대칭 타일은 특수한 행렬 형태(예: M<<N)에서만 고려할 가치가 있다.

</details>

---

## 9. 다음 단계 미리보기

이 모듈에서 레지스터 블로킹으로 산술 강도를 8배 향상시켜 cuBLAS 대비 50-70%의 성능에 도달했다. 그러나 아직 개선의 여지가 있다. 현재 커널의 남은 병목은 무엇인가?

```
현재 (Level 3) 타임라인:

  K반복 0:  [===== 로드 =====][=== 동기화 ===][===== 연산 =====]
  K반복 1:                                     [===== 로드 =====][=== 동기화 ===][===== 연산 =====]
  K반복 2:                                                                        [===== 로드 =====]...

  문제: 로드와 연산이 시간적으로 겹치지 않는다!
  --> 연산 중에 메모리 유닛이 유휴
  --> 로드 중에 연산 유닛이 유휴
```

**다음 모듈(모듈 8: Level 4 - 더블 버퍼링)**에서는 두 개의 공유 메모리 버퍼를 교대로 사용하여, **현재 타일에서 연산하는 동안 다음 타일을 미리 로드**하는 소프트웨어 파이프라이닝 기법을 배운다.

```
이상적 타임라인 (Level 4):

  버퍼A: [== 로드 0 ==][== 연산 0 ==][== 로드 2 ==][== 연산 2 ==]...
  버퍼B:               [== 로드 1 ==][== 연산 1 ==][== 로드 3 ==]...

  --> 로드와 연산이 시간적으로 중첩!
  --> 글로벌 메모리 레이턴시를 "숨길" 수 있다
```

이를 통해 cuBLAS 대비 70-85%의 성능에 도달하게 된다. 핵심 질문은 "공유 메모리를 2배 사용하는 대가와, 레이턴시 은닉의 이득 중 어느 쪽이 큰가?"이다.

---

*이 모듈은 BareMetal-SGEMM 프로젝트의 `src/kernels/sgemm_register_blocking.cu`를 기반으로 하며, 레지스터 블로킹의 수학적 원리와 하드웨어 트레이드오프를 체계적으로 분석하였다.*
