# 모듈 8: Level 4 - 더블 버퍼링 (소프트웨어 파이프라이닝)

---

## 1. 학습 목표

이 모듈을 완료하면 다음을 할 수 있다:

- 글로벌 메모리 레이턴시가 성능에 미치는 영향을 정량적으로 분석할 수 있다
- 더블 버퍼링의 파이프라인 구조를 설명할 수 있다
- 프롤로그-메인루프 패턴을 구현할 수 있다
- Nsight Compute의 스톨 분석에서 레이턴시 은닉 효과를 평가할 수 있다

---

## 2. 사전 복습

이전 모듈(7, 6)에서 배운 내용을 능동적으로 떠올려 본다. 답을 먼저 생각한 뒤, 부록의 퀴즈 정답에서 확인하라.

1. 레지스터 블로킹에서 외적(outer product) 마이크로커널의 데이터 재사용 원리는 무엇인가?
2. 공유 메모리에서 뱅크 충돌을 방지하기 위해 추가하는 패딩의 크기는 얼마인가?
3. 레지스터 블로킹 커널의 산술 강도(Arithmetic Intensity)는 어떻게 계산되는가?

---

## 3. 개념 설명

이 절에서는 CRA(Concrete-Representational-Abstract) 프레임워크와 이중 부호화(Dual Coding) 기법을 적용하여 더블 버퍼링의 핵심 개념을 단계적으로 설명한다.

### 3.1 레이턴시 문제

#### Concrete (구체적 비유)

레스토랑에서 요리사(연산 유닛)가 재료(데이터)를 기다리는 상황을 상상해 보자. 재료 배달에 100분이 걸리고, 요리에는 1분밖에 걸리지 않는다면, 요리사는 전체 시간의 99%를 아무것도 하지 못하고 대기하게 된다.

해결책은 무엇인가? **현재 요리를 하면서 다음 재료를 미리 주문하는 것이다.** 첫 번째 재료가 도착하면 요리를 시작하고, 동시에 두 번째 재료 주문을 넣는다. 두 번째 재료가 도착할 때쯤 첫 번째 요리는 이미 완료되어 있으므로, 바로 두 번째 요리에 착수할 수 있다. 요리사가 대기하는 시간이 거의 사라진다.

GPU에서도 정확히 같은 원리가 적용된다. 글로벌 메모리에서 데이터를 가져오는 데 수백 사이클이 걸리는 동안, 연산 유닛은 놀고 있다. 더블 버퍼링은 현재 데이터로 연산을 수행하면서 다음 데이터를 미리 로드하여, 이 대기 시간을 연산 뒤에 숨기는 기법이다.

#### Representational (구조적 표현)

싱글 버퍼(Level 3까지의 방식)와 더블 버퍼의 타임라인을 비교한다. 로드에 8 단위, 연산에 2 단위의 시간이 걸린다고 가정하자.

**싱글 버퍼 (로드와 연산이 순차적)**:

```
시간  0       8  10      18  20      28  30
Load  [===0===]          [===1===]          [===2===]
Comp           [=0=]              [=1=]              [=2=]
       로드     연산  대기  로드     연산  대기  로드     연산
```

로드가 끝나야 연산이 시작되고, 연산이 끝나야 다음 로드가 시작된다. 매 반복마다 로드+연산 = 10 단위가 소요된다. 3개의 타일을 처리하면 총 30 단위.

**더블 버퍼 (로드와 연산이 중첩)**:

```
시간  0       8  10      18  20      28
Load  [===0===][===1===][===2===][===3===]
Comp           [=0=]   [=1=]   [=2=]   [=3=]
```

첫 번째 로드가 끝나면 그 이후부터는 로드와 연산이 동시에 진행된다. 첫 번째 타일 이후 각 반복은 max(로드, 연산) = 8 단위만 소요된다. 로드 시간 뒤에 연산이 완전히 숨겨진다.

#### Abstract (수학적/기술적 표현)

글로벌 메모리 레이턴시는 약 **400 사이클**이다. FMA(Fused Multiply-Add) 명령어 하나는 약 **4 사이클**이다.

레지스터 블로킹 커널에서 한 타일당 수행하는 FMA 횟수는:

```
BK * TM * TN = 8 * 8 * 8 = 512 FMA
512 FMA * 4 사이클/FMA = 2048 사이클 (연산 시간)
```

연산 시간(2048 사이클)이 글로벌 메모리 레이턴시(400 사이클)보다 충분히 크므로, 연산을 수행하는 동안 다음 타일의 로드를 완전히 숨길 수 있다. 이것이 더블 버퍼링이 효과적인 근본적 이유이다.

만약 연산 시간이 레이턴시보다 짧다면(예: BK가 매우 작은 경우), 더블 버퍼링의 효과는 제한적이 된다. 핵심 조건은 다음과 같다:

```
연산 시간 >= 메모리 레이턴시  -->  레이턴시가 완전히 은닉됨
연산 시간 <  메모리 레이턴시  -->  레이턴시가 부분적으로만 은닉됨
```

### 3.2 더블 버퍼 구조

더블 버퍼링의 핵심은 공유 메모리를 **두 세트** 유지하는 것이다.

```
공유 메모리 레이아웃:

As[0][128][9]   As[1][128][9]     <-- A 행렬용 버퍼 2개
Bs[0][8][129]   Bs[1][8][129]     <-- B 행렬용 버퍼 2개

kt=0:  연산 -> As[0], Bs[0]    로드 -> As[1], Bs[1]
kt=1:  연산 -> As[1], Bs[1]    로드 -> As[0], Bs[0]
kt=2:  연산 -> As[0], Bs[0]    로드 -> As[1], Bs[1]
kt=3:  연산 -> As[1], Bs[1]    로드 -> As[0], Bs[0]
```

코드에서 버퍼 인덱스를 결정하는 방식은 간단하다:

```c
int curr_buf = kt % 2;       // 현재 연산에 사용할 버퍼
int next_buf = (kt + 1) % 2; // 다음 데이터를 로드할 버퍼
```

`kt % 2`로 **0과 1을 교대로 사용**한다. 현재 연산 중인 버퍼에는 이전 반복에서 로드한 유효한 데이터가 들어 있고, 다른 버퍼에는 다음 반복을 위한 데이터를 로드한다. 두 작업이 서로 다른 버퍼를 사용하므로 데이터 충돌이 없다.

배열의 `+1` 패딩(`BK + 1`, `BN + 1`)은 Level 2에서 배운 뱅크 충돌 방지 기법이 그대로 적용된 것이다.

### 3.3 프롤로그-메인루프 구조

더블 버퍼링 커널의 전체 흐름은 세 단계로 나뉜다:

```
[프롤로그]     [메인 루프]                            [에필로그]
첫 타일 로드   로드(다음) + 연산(현재) 반복            결과 저장
     |              |                                    |
     v              v                                    v
load tile[0]   for kt = 0 .. num_k_tiles-1:           store rC
syncthreads        prefetch tile[kt+1]                to C
                   compute  tile[kt]
                   syncthreads
```

**프롤로그가 왜 필요한가?**

메인 루프는 "현재 버퍼의 데이터로 연산하면서, 다음 버퍼에 데이터를 로드"하는 구조이다. 그런데 루프의 첫 번째 반복(kt=0)에서 연산하려면, buffer[0]에 이미 데이터가 들어 있어야 한다. 프롤로그가 없으면 **빈 버퍼에 대해 연산을 수행하는 오류**가 발생한다.

프롤로그는 이 "닭이 먼저냐 달걀이 먼저냐" 문제를 해결한다. 첫 번째 타일을 미리 로드해 두면, 메인 루프는 항상 유효한 데이터에 대해 연산할 수 있다.

### 3.4 PTX 캐시 힌트

PTX 버전(`sgemm_double_buffer_ptx`)에서는 글로벌 메모리 로드 시 캐시 정책을 명시적으로 지정한다.

```
ld.global.ca.f32 %0, [%1];
```

| 캐시 힌트 | 의미 | L1 캐시 | L2 캐시 | 용도 |
|-----------|------|---------|---------|------|
| `.ca` | Cache at All levels | 캐시함 | 캐시함 | 재사용이 예상되는 데이터 |
| `.cg` | Cache at Global level | 캐시 안 함 | 캐시함 | 한 번만 읽는 스트리밍 데이터 |

더블 버퍼링에서는 로드한 데이터를 공유 메모리에 저장한 후 여러 스레드가 반복 접근하므로, `.ca` 힌트로 L1과 L2 모두에 캐시하여 L2 활용도를 높인다.

PTX 버전의 또 다른 특징은 명시적 FMA(Fused Multiply-Add) 사용이다:

```
fma.rn.f32 %0, %1, %2, %0;   // %0 = %1 * %2 + %0 (반올림 모드: nearest)
```

컴파일러가 자동으로 FMA를 생성하는 경우가 대부분이지만, PTX로 명시하면 컴파일러의 최적화 판단에 의존하지 않고 정확한 명령어 스케줄링을 보장할 수 있다.

### 3.5 공유 메모리 압박

더블 버퍼링은 공유 메모리를 **2배** 사용한다. 이것이 점유율(Occupancy)에 미치는 영향을 분석해 보자.

**공유 메모리 사용량 계산**:

```
As[2][128][9]:  2 * 128 * 9 * 4바이트  =  9,216 바이트
Bs[2][8][129]:  2 * 8 * 129 * 4바이트  =  8,256 바이트
합계:                                   = 17,472 바이트 (약 17 KB)
```

A100 GPU의 SM당 공유 메모리는 최대 164 KB(설정에 따라 다름)이다. 17 KB라면 이론적으로 SM당 최대 9개 블록까지 상주 가능하지만, 레지스터 사용량 등 다른 제약도 함께 고려해야 한다.

**싱글 버퍼(Level 3) 대비 비교**:

| 항목 | 싱글 버퍼 | 더블 버퍼 | 비고 |
|------|-----------|-----------|------|
| As 크기 | 128 * 9 * 4 = 4,608 B | 2 * 128 * 9 * 4 = 9,216 B | 2배 |
| Bs 크기 | 8 * 129 * 4 = 4,128 B | 2 * 8 * 129 * 4 = 8,256 B | 2배 |
| 합계 | 약 8.5 KB | 약 17 KB | 2배 |

이것은 전형적인 **트레이드오프**이다. 공유 메모리를 2배 사용하여 SM에 상주할 수 있는 블록 수가 줄어들 수 있지만(낮은 점유율), 대신 레이턴시 은닉으로 각 블록의 효율이 크게 향상된다. 실측 결과, 이 트레이드오프는 더블 버퍼링 쪽이 압도적으로 유리하다.

---

## 4. 코드 분석

이 절에서는 `src/kernels/sgemm_double_buffer.cu`의 CUDA 커널을 단계별로 분석한다.

### 4.1 공유 메모리 선언 (라인 55-56)

```c
__shared__ float As[2][BM][BK + 1];  // +1 for bank conflict avoidance
__shared__ float Bs[2][BK][BN + 1];
```

첫 번째 차원의 `[2]`가 더블 버퍼의 핵심이다. 이전 레벨에서는 `As[BM][BK+1]` 하나만 있었지만, 이제 `As[0]`과 `As[1]` 두 세트가 존재한다. 패딩(`+1`)은 Level 2에서 배운 뱅크 충돌 방지 기법을 그대로 유지한다.

### 4.2 람다 헬퍼 함수 (라인 83-130)

코드를 구조화하기 위해 세 개의 람다 함수를 정의한다.

**load_A_tile** (라인 83-94):

```c
auto load_A_tile = [&](int buf_idx, int k_tile) {
    int k_base = k_tile * BK;
    for (int i = 0; i < BM; i += 32) {
        int row = A_load_row + i;
        if (row < BM) {
            int grow = block_row_start + row;
            int gcol = k_base + A_load_col;
            As[buf_idx][row][A_load_col] =
                (grow < M && gcol < K) ? A[grow * lda + gcol] : 0.0f;
        }
    }
};
```

- `buf_idx`: 어느 버퍼에 저장할지 (0 또는 1)
- `k_tile`: K 차원에서 몇 번째 타일인지
- 경계 검사(`grow < M && gcol < K`)로 행렬 범위 밖 접근을 방지하고, 범위 밖이면 0.0f를 저장
- 루프의 `+= 32`는 256개 스레드(= BM/32 = 128/32 = 4 반복)가 BM = 128개 행을 협력적으로 로드하기 위한 스트라이드

**load_B_tile** (라인 97-108): A와 동일한 패턴이지만, B 행렬의 레이아웃에 맞추어 `+= 2` 스트라이드를 사용한다.

**compute_tile** (라인 111-130):

```c
auto compute_tile = [&](int buf_idx) {
    #pragma unroll
    for (int k = 0; k < BK; ++k) {
        #pragma unroll
        for (int m = 0; m < TM; ++m) {
            rA[m] = As[buf_idx][thread_row + m][k];
        }
        #pragma unroll
        for (int n = 0; n < TN; ++n) {
            rB[n] = Bs[buf_idx][k][thread_col + n];
        }
        #pragma unroll
        for (int m = 0; m < TM; ++m) {
            #pragma unroll
            for (int n = 0; n < TN; ++n) {
                rC[m][n] += rA[m] * rB[n];
            }
        }
    }
};
```

Level 3의 외적 마이크로커널과 동일한 구조이다. 유일한 차이는 `buf_idx`를 인자로 받아 **어느 버퍼에서 읽을지** 결정한다는 점이다. `rA`, `rB`에 공유 메모리 데이터를 레지스터로 복사한 뒤, TM * TN = 8 * 8 = 64개의 FMA를 수행한다.

### 4.3 프롤로그 (라인 132-137)

```c
// Prologue: Load first tile
if (num_k_tiles > 0) {
    load_A_tile(0, 0);
    load_B_tile(0, 0);
}
__syncthreads();
```

첫 번째 타일(k_tile = 0)을 buffer[0]에 로드하고, `__syncthreads()`로 블록 내 모든 스레드의 로드가 완료될 때까지 동기화한다. 이 동기화가 끝난 후에야 메인 루프에서 buffer[0]의 데이터를 안전하게 읽을 수 있다.

### 4.4 메인 루프 (라인 140-156)

```c
for (int kt = 0; kt < num_k_tiles; ++kt) {
    int curr_buf = kt % 2;
    int next_buf = (kt + 1) % 2;

    // Prefetch next tile (if exists) while computing current
    if (kt + 1 < num_k_tiles) {
        load_A_tile(next_buf, kt + 1);
        load_B_tile(next_buf, kt + 1);
    }

    // Compute on current buffer
    compute_tile(curr_buf);

    __syncthreads();
}
```

이 루프가 더블 버퍼링의 핵심이다. 한 반복에서 일어나는 일을 단계별로 분해한다:

1. **버퍼 인덱스 계산**: `curr_buf`는 현재 연산할 데이터가 있는 버퍼, `next_buf`는 다음 데이터를 로드할 버퍼
2. **프리페치 (로드 시작)**: `load_A_tile`과 `load_B_tile`이 `next_buf`에 다음 타일을 로드한다. 이 로드 명령은 **비동기적으로** 발행된다. 즉, 글로벌 메모리에 요청을 보낸 후 완료를 기다리지 않고 다음 명령으로 넘어간다.
3. **연산 (현재 버퍼)**: `compute_tile(curr_buf)`가 이전에 로드 완료된 데이터로 연산을 수행한다. 이 연산은 프리페치의 메모리 트랜잭션이 진행되는 **동안** 실행된다.
4. **동기화**: `__syncthreads()`가 블록 내 모든 스레드의 로드와 연산이 완료될 때까지 대기한다.

중요한 점은, 로드와 연산이 **서로 다른 버퍼**를 사용한다는 것이다. 로드는 `next_buf`에 쓰고, 연산은 `curr_buf`에서 읽는다. 따라서 데이터 경합이 발생하지 않으며, `__syncthreads()`가 루프당 **1번**만 필요하다.

**kt별 실행 흐름 추적**:

| kt | curr_buf | next_buf | 연산 대상 | 로드 대상 |
|----|----------|----------|-----------|-----------|
| 0  | 0        | 1        | tile[0] from buf[0] | tile[1] into buf[1] |
| 1  | 1        | 0        | tile[1] from buf[1] | tile[2] into buf[0] |
| 2  | 0        | 1        | tile[2] from buf[0] | tile[3] into buf[1] |
| 3  | 1        | 0        | tile[3] from buf[1] | (마지막이면 로드 생략) |

### 4.5 결과 저장 (라인 159-172)

```c
for (int m = 0; m < TM; ++m) {
    int grow = block_row_start + thread_row + m;
    if (grow < M) {
        for (int n = 0; n < TN; ++n) {
            int gcol = block_col_start + thread_col + n;
            if (gcol < N) {
                float c_old = C[grow * ldc + gcol];
                C[grow * ldc + gcol] = alpha * rC[m][n] + beta * c_old;
            }
        }
    }
}
```

모든 K 타일에 대한 누적이 완료된 후, 레지스터의 결과를 글로벌 메모리의 C 행렬에 기록한다. `alpha * rC + beta * C_old` 형태는 BLAS 표준의 SGEMM 인터페이스(C = alpha * A * B + beta * C)를 따른다.

### 4.6 PTX 버전의 차이점

`sgemm_double_buffer_ptx` (라인 181-385)는 동일한 알고리즘을 인라인 PTX로 구현한 버전이다. 주요 차이점은 세 가지이다:

1. **캐시 힌트 명시**: `ld.global.ca.f32`로 L1+L2 캐시 정책을 지정
2. **명시적 FMA**: `fma.rn.f32`로 컴파일러 의존 없이 직접 FMA 명령 사용
3. **벡터화 저장**: 결과 저장 시 `float4` 단위로 4개 원소를 한 번에 기록하여 메모리 트랜잭션 수를 줄임

---

## 5. 왜 이것이 작동하는가?

아래의 질문들에 대해 스스로 답을 구성해 본 뒤, 이후의 설명과 비교하라.

### 질문 1: 로드와 연산이 진정으로 동시에 실행되는 하드웨어 메커니즘은 무엇인가?

GPU의 SM은 여러 기능 유닛을 독립적으로 보유한다. 메모리 로드를 담당하는 **로드/저장 유닛(LSU, Load/Store Unit)**과 산술 연산을 담당하는 **수학 유닛(FP32/FP64/INT)** 은 물리적으로 분리되어 있다. 워프 스케줄러가 로드 명령을 LSU에 발행하면, 로드 결과가 돌아오기를 기다리지 않고 즉시 다른 워프의 수학 명령을 수학 유닛에 발행할 수 있다. 이것이 **명령어 수준 병렬성(ILP)** 과 **스레드 수준 병렬성(TLP)** 의 조합으로, 서로 다른 종류의 명령이 동시에 파이프라인을 채우는 원리이다.

### 질문 2: BK * TM * TN FMA 사이클이 글로벌 메모리 레이턴시보다 커야 하는 이유는?

더블 버퍼링에서 로드를 숨기려면, 연산이 로드보다 오래 걸려야 한다. 한 타일의 연산 시간은 BK * TM * TN개의 FMA에 비례하고, 로드 시간은 글로벌 메모리 레이턴시에 비례한다. 만약 연산 시간이 로드 시간보다 짧으면, 연산이 먼저 끝나고 다음 타일의 데이터가 아직 도착하지 않아 결국 대기가 발생한다. 따라서 레이턴시를 완전히 숨기려면 다음 조건이 성립해야 한다:

```
BK * TM * TN * FMA_latency >= Global_memory_latency
8 * 8 * 8 * 4 = 2048 >= 400  (조건 충족)
```

### 질문 3: __syncthreads()가 메인 루프에서 1번만 필요한 이유는?

싱글 버퍼에서는 같은 버퍼에 로드하면서 동시에 읽기 때문에, 로드 전후로 두 번의 동기화가 필요했다(로드 완료 확인 + 연산 완료 후 다음 로드 전 확인). 더블 버퍼에서는 로드 대상(next_buf)과 연산 대상(curr_buf)이 **서로 다른 물리적 메모리 공간**이므로, 한쪽에 쓰면서 다른 쪽에서 읽어도 데이터 경합이 없다. 따라서 각 반복의 끝에서 한 번의 `__syncthreads()`만으로 충분하다. 이 동기화는 "모든 스레드가 현재 반복의 로드와 연산을 모두 완료했음"을 보장한다.

### 질문 4: Long Scoreboard가 0%가 되지 않는 이유는?

Long Scoreboard 스톨은 레지스터가 아직 글로벌 메모리 로드의 결과를 받지 못했을 때 발생한다. 더블 버퍼링으로 대부분의 로드 레이턴시를 숨기더라도, 다음과 같은 상황에서 여전히 스톨이 발생할 수 있다:

- **프롤로그 단계**: 첫 타일 로드는 숨길 수 없다 (파이프라인의 "충전" 단계)
- **마지막 타일**: 더 이상 로드할 타일이 없어 프리페치가 생략된다
- **로드 대역폭 포화**: 많은 블록이 동시에 글로벌 메모리에 접근하면 레이턴시가 예상보다 길어질 수 있다
- **연산이 로드보다 빨리 끝나는 경우**: 특정 조건에서 FMA 연산이 로드 시간을 완전히 커버하지 못할 수 있다

따라서 Long Scoreboard는 감소하지만 완전히 제거되지는 않으며, 이는 정상적인 동작이다.

---

## 6. 시뮬레이션 8: 파이프라인 타임라인 빌더

이 시뮬레이션에서는 싱글 버퍼와 더블 버퍼의 타임라인을 직접 그려보고, 이론적 속도 향상을 계산한다.

### 설정

- 글로벌 메모리 로드 시간: **100 사이클** (한 타일 기준)
- 연산 시간: **10 사이클** (한 타일 기준)
- 동기화 오버헤드: **무시** (간소화를 위해)
- 처리할 타일 수: **4개**

### 싱글 버퍼 타임라인 (Level 3)

```
시간    0        100 110      210 220      310 320      420 430
        |         |   |        |   |        |   |        |   |
Load    [===0====]    [===1====]   [===2====]   [===3====]
Comp              [0]          [1]          [2]          [3]
Sync          |         |           |           |           |
```

- 각 타일: 로드(100) + 연산(10) = **110 사이클**
- 총 시간: 4 * 110 = **440 사이클**
- 연산 유닛 활용률: 4 * 10 / 440 = **9.1%**

### 더블 버퍼 타임라인 (Level 4)

```
시간    0        100 110      210 220      310 320 330
        |         |   |        |   |        |   |   |
Load    [===0====][===1====][===2====][===3====]
Comp              [0]       [1]       [2]       [3]
Sync          |         |         |         |       |
```

- 프롤로그: 100 사이클 (첫 타일 로드)
- 이후 3 타일: 각 max(100, 10) = **100 사이클**
- 마지막 연산: 10 사이클
- 총 시간: 100 + 3 * 100 + 10 = **410 사이클**

실제로는 연산(10) < 로드(100)이므로 연산이 로드 시간 안에 완전히 숨겨진다.

보다 현실적인 비율(연산 시간 > 로드 시간)을 사용해 보자:

### 현실적 비율의 타임라인

- 글로벌 메모리 로드 시간: **80 사이클**
- 연산 시간: **100 사이클** (BK * TM * TN FMA가 충분히 많은 경우)

**싱글 버퍼**:

```
시간    0       80  180     260  360     440  540     620
        |        |    |      |    |       |    |       |
Load    [==0===]      [==1===]    [==2===]     [==3===]
Comp             [==0===]         [==1===]     [==2===]     [==3===]
```

- 각 타일: 80 + 100 = **180 사이클**
- 총 시간: 4 * 180 = **720 사이클**

**더블 버퍼**:

```
시간    0       80  180     280     380     480
        |        |    |      |       |       |
Load    [==0===][==1===][==2===][==3===]
Comp             [==0===][==1===][==2===][==3===]
```

- 프롤로그: 80 사이클
- 이후 각 타일: max(80, 100) = **100 사이클**
- 총 시간: 80 + 4 * 100 = **480 사이클**

### 속도 향상 계산

**일반 공식**:

```
싱글 버퍼 총 시간:  N * (T_load + T_comp)
더블 버퍼 총 시간:  T_load + N * max(T_load, T_comp)

속도 향상 = N * (T_load + T_comp) / (T_load + N * max(T_load, T_comp))
```

위의 현실적 비율(T_load = 80, T_comp = 100, N = 4)에 대해:

```
싱글: 4 * (80 + 100) = 720
더블: 80 + 4 * max(80, 100) = 80 + 400 = 480
속도 향상: 720 / 480 = 1.50 (50% 향상)
```

N이 충분히 크면(N >> 1):

```
속도 향상 ≈ (T_load + T_comp) / max(T_load, T_comp)
```

T_comp > T_load인 이상적 경우(연산이 로드를 완전히 숨기는 경우):

```
속도 향상 ≈ (T_load + T_comp) / T_comp = 1 + T_load / T_comp
```

### 실습 과제

종이에 다음 조건에 대해 타임라인을 직접 그리고 속도 향상을 계산하라:

1. T_load = 400, T_comp = 2048, N = 512 (실제 SGEMM에 가까운 값)
2. T_load = 400, T_comp = 200, N = 512 (연산이 로드보다 짧은 경우)
3. 두 경우의 차이를 비교하고, "연산 시간 >= 로드 시간"이 왜 중요한지 서술하라

---

## 7. 핵심 정리

- **더블 버퍼링** = 로드와 연산의 시간적 중첩(temporal overlap)으로 메모리 레이턴시를 숨기는 기법이다
- **As[2], Bs[2]** 로 공유 메모리를 2배 사용하여 두 세트의 버퍼를 교대로 활용한다
- **프롤로그**: 메인 루프 진입 전에 첫 번째 타일을 반드시 미리 로드해야 한다. 프롤로그가 없으면 첫 반복에서 빈 데이터에 대해 연산하는 오류가 발생한다
- **버퍼 교대**: `curr_buf = kt % 2`, `next_buf = (kt+1) % 2`로 매 반복 0과 1을 번갈아 사용한다
- **__syncthreads()가 1번**: 로드와 연산이 서로 다른 버퍼를 사용하므로 데이터 경합이 없다
- **스톨 변화**: Long Scoreboard 약 40-50%에서 약 20-30%로 감소, Math Pipe Throttle 약 10-20%에서 약 30-40%로 증가. 이는 연산 유닛이 더 많이 활용되고 있음을 의미한다
- **PTX 캐시 힌트**: `ld.global.ca.f32`로 L1+L2 캐시 정책을 명시하여 메모리 계층 활용도를 향상시킨다
- **공유 메모리 트레이드오프**: 2배의 공유 메모리 사용으로 점유율이 낮아질 수 있으나, 레이턴시 은닉 효과가 이를 상쇄한다
- **예상 성능**: cuBLAS 대비 약 70-85%

---

## 8. 마스터리 체크포인트 퀴즈 [MASTERY CHECKPOINT]

> **통과 기준: 7문항 중 6문항 이상 정답 (약 80%)**
>
> 이 체크포인트는 Level 4까지의 핵심 개념을 충분히 이해했는지 확인한다.
> 통과하지 못하면 3.1-3.5절과 4절을 다시 학습한 후 재도전하라.
> 정답과 해설은 `docs/education/12_부록_퀴즈_정답.md`에서 확인할 수 있다.

---

**문제 1** (Remember / 기억)

더블 버퍼링에서 사용하는 공유 메모리 버퍼의 세트 수는 몇 개인가?

(a) 1개
(b) 2개
(c) 3개
(d) 4개

---

**문제 2** (Understand / 이해)

소프트웨어 파이프라이닝의 핵심 원리를 가장 정확하게 설명한 것은?

(a) 글로벌 메모리의 대역폭을 2배로 증가시킨다
(b) 연산 명령의 실행 속도를 향상시킨다
(c) 메모리 로드와 연산을 시간적으로 중첩시켜 로드 레이턴시를 숨긴다
(d) 공유 메모리의 뱅크 충돌을 완전히 제거한다

---

**문제 3** (Apply / 적용)

메인 루프에서 kt = 0, 1, 2, 3에 대해 `curr_buf`와 `next_buf`의 값을 각각 나열하라.

| kt | curr_buf = kt % 2 | next_buf = (kt+1) % 2 |
|----|--------------------|-----------------------|
| 0  | ?                  | ?                     |
| 1  | ?                  | ?                     |
| 2  | ?                  | ?                     |
| 3  | ?                  | ?                     |

---

**문제 4** (Analyze / 분석)

프롤로그(첫 타일 선로드)를 생략하면 어떤 문제가 발생하는가? 다음 중 가장 정확한 설명을 고르라.

(a) 컴파일 에러가 발생한다
(b) 메인 루프의 첫 반복에서 초기화되지 않은 공유 메모리 데이터로 연산하여 잘못된 결과가 나온다
(c) 성능이 약간 저하되지만 결과는 정확하다
(d) __syncthreads()에서 교착 상태(deadlock)가 발생한다

---

**문제 5** (Analyze / 분석)

Nsight Compute의 스톨 분석에서, 더블 버퍼링 적용 후 Long Scoreboard가 약 40-50%에서 약 20-30%로 감소하고 Math Pipe Throttle이 약 10-20%에서 약 30-40%로 증가하였다. 이 변화가 의미하는 바를 서술하라.

---

**문제 6** (Evaluate / 평가)

더블 버퍼링으로 공유 메모리 사용량이 약 8.5 KB에서 약 17 KB로 2배 증가하였다. A100 GPU(SM당 최대 164 KB 공유 메모리)에서 이 증가가 점유율에 미치는 영향을 분석하고, 그럼에도 불구하고 더블 버퍼링이 유리한 이유를 설명하라.

---

**문제 7** (Create / 창조)

현재 커널은 `num_k_tiles = (K + BK - 1) / BK`로 타일 수를 계산하며, K가 BK로 나누어떨어지지 않을 때 경계 검사(`grow < M && gcol < K`)로 처리한다. 만약 경계 검사 없이 정확한 타일 수만큼만 처리한다면, 나머지 원소(K % BK개의 열)를 처리하기 위한 **에필로그(epilogue)** 를 어떻게 설계할 수 있는지 의사 코드(pseudo-code)로 작성하라.

힌트: 메인 루프는 `K / BK`개의 완전한 타일만 처리하고, 에필로그에서 나머지를 별도로 처리한다.

---

## 9. 다음 단계 미리보기

이 모듈에서 더블 버퍼링으로 메모리 레이턴시를 연산 뒤에 숨기는 기법을 배웠다. 그러나 현재의 구현에는 한 가지 비효율이 남아 있다. 글로벌 메모리의 데이터를 공유 메모리에 저장하려면, 먼저 **레지스터를 경유**해야 한다는 점이다.

```
현재 경로:  글로벌 메모리 --> 레지스터 --> 공유 메모리
```

이 과정에서 레지스터가 임시 저장소로 사용되며, 이는 레지스터 압박을 가중시킨다.

다음 모듈(09. Level 5 - 비동기 복사)에서는 NVIDIA Ampere 아키텍처(SM 8.0)에서 도입된 `cp.async` 명령어를 배운다. 이 명령어는 글로벌 메모리에서 공유 메모리로 **레지스터를 우회하여 직접 복사**하는 하드웨어 비동기 복사를 수행한다.

```
cp.async 경로:  글로벌 메모리 --> 공유 메모리 (레지스터 우회!)
```

레지스터 압박 해소와 더불어, 트리플 버퍼링으로 파이프라인 깊이를 더 늘려 cuBLAS의 85-95% 성능에 도달하는 것을 목표로 한다.

---

*이 문서는 BareMetal-SGEMM 프로젝트의 `src/kernels/sgemm_double_buffer.cu`를 기반으로 하며, 더블 버퍼링(소프트웨어 파이프라이닝)을 통한 레이턴시 은닉 기법을 다룬다.*
