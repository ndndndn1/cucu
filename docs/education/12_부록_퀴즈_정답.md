# 부록: 퀴즈 정답 및 해설

이 문서는 전체 교육 과정(모듈 01-10)의 모든 퀴즈 문항에 대한 정답과 상세 해설을 포함한다. 총 75문항이다.

> **사용법**: 각 모듈의 퀴즈를 먼저 풀어본 뒤, 이 문서에서 정답을 확인하라. 틀린 문항은 해당 모듈로 돌아가 개념을 복습한 후 다시 시도하라.

---

## 모듈 01: 사전지식 -- GPU 아키텍처 (8문항)

### 문제 1 (Remember)
**질문**: GPU에서 하나의 워프를 구성하는 스레드 수는 몇 개인가?

**정답**: **32개**. 이 값은 모든 NVIDIA GPU에서 동일하며, `include/sgemm_kernels.hpp:42`에서 `constexpr int WARP_SIZE = 32;`로 정의되어 있다.

---

### 문제 2 (Remember)
**질문**: 글로벌 메모리(HBM) 접근의 지연시간은 약 몇 사이클인가?

**정답**: **약 400-600 사이클**. 이는 레지스터 접근(~1 사이클) 대비 수백 배 느리며, 이 지연시간을 줄이는 것이 SGEMM 최적화의 핵심 동기이다.

---

### 문제 3 (Understand)
**질문**: 레지스터와 공유 메모리의 범위(scope) 차이를 설명하라.

**정답**: **레지스터**는 **스레드 전용(thread-private)**이다. 하나의 스레드만이 자신의 레지스터를 읽고 쓸 수 있으며, 다른 스레드는 접근할 수 없다. **공유 메모리**는 **블록 내 공유(block-shared)**이다. 같은 블록에 속한 모든 스레드가 동일한 공유 메모리 공간을 읽고 쓸 수 있다. 이를 통해 블록 내 스레드 간 데이터 교환이 가능하다.

---

### 문제 4 (Understand)
**질문**: GPU가 CPU보다 행렬곱 연산에 유리한 이유를 두 가지 이상 서술하라.

**정답**:
1. **높은 데이터 병렬성**: 행렬곱의 각 결과 원소는 독립적으로 계산할 수 있다. M x N 결과 행렬의 모든 원소를 동시에 계산하는 것이 가능하며, GPU의 수천 개 코어가 이를 병렬로 처리한다.
2. **높은 연산 대 메모리 비율**: 행렬곱의 연산 강도(Arithmetic Intensity)는 O(N^3) 연산 / O(N^2) 데이터로, 행렬이 클수록 연산 비중이 높아진다. GPU는 높은 부동소수점 처리량을 가지고 있어 이러한 연산 집약적 작업에 적합하다.
3. **메모리 접근 패턴**: 적절한 타일링을 적용하면 행렬곱의 메모리 접근이 규칙적이고 예측 가능하여, GPU의 메모리 합치기(coalescing) 메커니즘을 최대한 활용할 수 있다.

---

### 문제 5 (Apply)
**질문**: 4096 x 4096 행렬에 대해 16 x 16 스레드 블록을 사용할 경우, 필요한 그리드 크기(dim3)는?

**정답**:
```
grid.x = 4096 / 16 = 256
grid.y = 4096 / 16 = 256
그리드 크기: dim3(256, 256)
총 블록 수: 256 * 256 = 65,536 블록
총 스레드 수: 65,536 * 256 = 16,777,216 스레드
```

---

### 문제 6 (Apply)
**질문**: SM당 최대 레지스터 수가 65,536개이고, 커널이 스레드당 128개의 레지스터를 사용한다면, SM당 동시에 실행 가능한 최대 스레드 수는?

**정답**:
```
SM당 최대 스레드 수 (레지스터 기준) = 65,536 / 128 = 512 스레드
이는 512 / 32 = 16 워프에 해당한다.
```
만약 SM의 하드웨어 최대 스레드 수가 2048이라면, 레지스터 제한으로 인해 실제로는 512 스레드만 활성화 가능하다. 이 경우 점유율은 `512 / 2048 = 25%`이다.

---

### 문제 7 (Analyze)
**질문**: 어떤 SGEMM 커널의 점유율이 25%인데도 높은 성능을 보이는 경우가 있다. 이 현상의 원인을 분석하라.

**정답**: 점유율이 25%로 낮지만 높은 성능을 보이는 주된 이유는 **레지스터 블로킹에 의한 높은 ILP(Instruction-Level Parallelism)**이다.
- 스레드당 많은 레지스터를 사용하여 결과 행렬의 큰 타일(예: 8x8)을 레지스터에 유지한다.
- 이를 통해 매 반복마다 64개의 독립적인 FMA 연산을 발급할 수 있다.
- 이 FMA 연산들은 서로 의존성이 없으므로, 파이프라인에서 동시에 실행될 수 있다.
- 결과적으로 워프 수준 병렬성(TLP)이 낮아도 명령어 수준 병렬성(ILP)이 이를 보상한다.

---

### 문제 8 (Understand)
**질문**: `DeviceInfo` 구조체에서 `supports_async_copy`가 `compute_major >= 8`일 때 `true`가 되는 이유는 무엇인가?

**정답**: `compute_major >= 8`은 **Ampere 아키텍처 이상**을 의미한다. Ampere에서 `cp.async` 명령어가 도입되었으며, 이 명령어는 글로벌 메모리에서 공유 메모리로의 데이터 복사를 비동기적으로 수행하여, 데이터가 전송되는 동안 스레드가 다른 연산을 계속 수행할 수 있게 한다. 이전 아키텍처에서는 레지스터를 경유하는 2단계 과정이 필요했다.

---

## 모듈 02: 사전지식 -- CUDA 프로그래밍 기초 (6문항)

### 문제 1 (Remember)
**질문**: Driver API에서 컨텍스트를 생성하는 함수의 이름은 무엇인가?

**정답**: **`cuCtxCreate`**. 시그니처: `CUresult cuCtxCreate(CUcontext* pctx, unsigned int flags, CUdevice dev)`. 코드 참조: `src/driver/cuda_driver.cpp:78`.

---

### 문제 2 (Understand)
**질문**: PTX JIT 컴파일의 장점을 두 가지 이상 설명하시오.

**정답**:
1. **아키텍처 이식성**: PTX는 가상 ISA이므로 sm_80에서 빌드한 PTX를 sm_90 이상의 미래 GPU에서도 JIT 컴파일하여 실행할 수 있다.
2. **런타임 최적화**: JIT 컴파일러는 실행 시점의 정확한 GPU 모델, 드라이버 버전, 사용 가능한 리소스를 알고 있으므로, 해당 환경에 최적화된 기계어를 생성할 수 있다.
3. **배포 편의성**: 하나의 PTX 파일만 배포하면 여러 GPU 아키텍처를 지원할 수 있다.
4. **동적 로딩**: 커널을 실행 시점에 파일로부터 로드할 수 있다. 이 프로젝트처럼 6개의 최적화 레벨을 선택적으로 로드하는 데 적합하다.

---

### 문제 3 (Apply)
**질문**: 다음 커널에 대해 `cuLaunchKernel`에 전달할 `void**` 파라미터 배열을 작성하시오.
```cuda
__global__ void vecadd(float* A, float* B, float* C, int N)
```

**정답**:
```cpp
void* args[] = {
    &d_A,    // float* A
    &d_B,    // float* B
    &d_C,    // float* C
    &N       // int N
};
```
핵심: 각 원소는 **값의 주소**이다. `d_A` 자체(값)가 아니라 `&d_A`(주소)를 넣어야 한다.

---

### 문제 4 (Analyze)
**질문**: JIT 컴파일이 실패했을 때, 에러 원인을 담은 로그를 어떻게 얻을 수 있는가?

**정답**: `JitOptions::to_cu_options()`에서 `CU_JIT_ERROR_LOG_BUFFER`와 `CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES` 옵션을 설정한다. `cuModuleLoadDataEx`가 실패하면 에러 로그 버퍼에 컴파일 에러 메시지가 기록된다. 코드 참조: `src/driver/cuda_driver.cpp:171-189` (옵션 설정), `src/driver/cuda_driver.cpp:236-241` (에러 로그 추출).

---

### 문제 5 (Understand)
**질문**: CUDA 프로그래밍에서 RAII 패턴이 특히 중요한 이유를 `DeviceMemory` 클래스를 예로 들어 설명하시오.

**정답**: GPU 메모리는 CPU 메모리보다 훨씬 제한적(일반적으로 8~48GB)이며, 누수된 GPU 메모리는 프로그램이 종료될 때까지 회수되지 않는다. `DeviceMemory`는 생성자에서 `cuMemAlloc`으로 할당하고, 소멸자에서 `cuMemFree`로 해제한다. RAII 없이는: (1) 예외 발생 시 메모리 누수, (2) 복잡한 try-catch 블록 필요, (3) 이중 해제 위험이 있다. 복사 금지(`delete`)와 이동 허용을 통해 소유권이 항상 하나의 객체에만 존재하도록 보장한다.

---

### 문제 6 (Apply)
**질문**: 4096 x 4096 크기의 `float` 행렬을 GPU에 할당하려면 몇 바이트가 필요한가?

**정답**: `4096 * 4096 * sizeof(float) = 4096 * 4096 * 4 = 67,108,864 바이트 = 64 MB`. SGEMM에서 A, B, C 세 행렬 모두 4096 x 4096이면 총 192 MB가 필요하다.

---

## 모듈 03: 사전지식 -- 행렬곱셈 수학적 배경 (5문항)

### 문제 1 (Remember)
**질문**: SGEMM의 총 FLOP 수를 M, N, K로 나타내는 공식을 쓰시오.

**정답**: `총 FLOP = 2 * M * N * K`. 각 출력 원소에 K번의 FMA가 필요하고, FMA 하나가 2 FLOP이며, 출력 원소가 M*N개이므로 2*M*N*K이다.

---

### 문제 2 (Apply)
**질문**: M=2048, N=2048, K=2048일 때 총 FLOP 수를 GFLOP 단위로 계산하시오.

**정답**: `2 * 2048^3 = 2 * 8,589,934,592 = 17,179,869,184 ≈ 17.18 GFLOP`

---

### 문제 3 (Understand)
**질문**: "산술 강도가 높다"는 것은 무엇을 의미하는가?

**정답**: 산술 강도가 높다는 것은 메모리에서 전송한 바이트 대비 많은 연산을 수행한다는 의미이다. 데이터를 한 번 로드하면 여러 번 재사용하여 많은 연산에 활용한다. 루프라인 모델에서 릿지 포인트 오른쪽의 연산 바운드 영역에 해당한다.

---

### 문제 4 (Apply)
**질문**: M=N=K=4096, 실행 시간 2.0ms일 때 달성된 GFLOP/s는?

**정답**:
```
총 FLOP = 2 * 4096^3 = 137,438,953,472
GFLOP/s = 137,438,953,472 / (0.002 * 10^9) ≈ 68,720 GFLOP/s
```
참고: 이 값은 단일 GPU의 피크를 초과하므로 비현실적이다. 현실적인 실행 시간은 최소 약 7ms 이상이어야 한다.

---

### 문제 5 (Analyze)
**질문**: 루프라인 모델에서 릿지 포인트를 결정하는 두 가지 하드웨어 요소는?

**정답**: (1) **피크 연산 처리량** (GFLOP/s): SM 수, FP32 코어 수, 클럭 주파수로 결정. (2) **피크 메모리 대역폭** (GB/s): 메모리 버스 폭, 메모리 클럭으로 결정. `릿지 포인트 = 피크 연산 / 피크 대역폭`. 릿지 포인트가 높은 GPU에서는 더 높은 산술 강도가 필요하므로, 메모리 최적화(코얼레싱, 타일링, 레지스터 블로킹)의 중요성이 커진다.

---

## 모듈 04: Level 0 -- 나이브 구현 (8문항)

### Q1 (Remember)
**질문**: 나이브 커널에서 한 스레드가 계산하는 C 원소의 수는?

**정답**: 1개. 1스레드 = 1원소 전략을 사용한다.

---

### Q2 (Understand)
**질문**: `blockIdx.y`와 `threadIdx.y`의 역할을 각각 설명하시오.

**정답**: `blockIdx.y`는 그리드 내에서 현재 블록의 y축 위치(행 방향 블록 인덱스)를 나타낸다. `threadIdx.y`는 블록 내에서 현재 스레드의 y축 위치를 나타낸다. 둘을 결합하여 `row = blockIdx.y * blockDim.y + threadIdx.y`로 글로벌 행 인덱스를 계산한다.

---

### Q3 (Apply)
**질문**: M=1024, N=1024, 블록 크기 16x16일 때 총 스레드 수를 계산하시오.

**정답**:
```
grid = dim3(64, 64) = 4,096 블록
스레드/블록 = 256
총 스레드 수 = 4,096 * 256 = 1,048,576
```
이것은 출력 행렬 C의 원소 수(1024 * 1024 = 1,048,576)와 동일하다.

---

### Q4 (Analyze)
**질문**: 같은 워프에서 ty가 다른 Thread(tx=0, ty=0)과 Thread(tx=0, ty=1)이 A에서 접근하는 주소의 차이는?

**정답**: `lda * sizeof(float)` 바이트. lda=4096이면 `4096 * 4 = 16,384` 바이트. 이 간격은 128바이트 캐시 라인보다 훨씬 크므로 코얼레싱이 불가능하다.

---

### Q5 (Analyze)
**질문**: 데이터 재사용 관점에서 산술 강도가 낮은 이유를 설명하시오.

**정답**: 각 스레드가 K 루프를 독립적으로 실행하면서, 같은 데이터를 다른 스레드와 공유하지 않고 매번 글로벌 메모리에서 새로 로드한다. A[row][k]는 N번, B[k][col]는 M번 중복 로드된다. 공유 메모리에 캐싱하지 않으므로 산술 강도가 2 FLOP / 8 bytes = 0.25 FLOP/byte에 불과하다.

---

### Q6 (Understand)
**질문**: `__restrict__` 키워드의 목적은?

**정답**: 해당 포인터가 가리키는 메모리 영역이 다른 포인터와 겹치지 않음을 컴파일러에게 보장한다. 이를 통해 로드/스토어 순서 재배치, 불필요한 재로드 제거, 명령어 스케줄링 최적화가 가능하다.

---

### Q7 (Apply)
**질문**: K=4096일 때 한 스레드의 총 글로벌 메모리 로드 횟수는?

**정답**: K 루프에서 매 반복 A 1회 + B 1회 = 2회. K=4096이므로 8,192회. C 읽기/쓰기 포함 약 8,194회.

---

### Q8 (Evaluate)
**질문**: cuBLAS의 1-5%에 불과한 근본 이유 3가지를 요약하시오.

**정답**:
1. **메모리 코얼레싱 부재**: A 행렬의 비연속 접근으로 다수의 메모리 트랜잭션 발생.
2. **데이터 재사용 부재**: 같은 원소가 수천 번 중복 로드됨. 매번 400-800 사이클 지불.
3. **낮은 산술 강도**: 0.25 FLOP/byte로 완전히 메모리 바운드. Long Scoreboard 스톨이 80% 이상.

---

## 모듈 05: Level 1 -- 메모리 코얼레싱 (7문항)

### 문제 1 (Remember)
**질문**: 128-bit 벡터화 로드에 대응하는 SASS 명령어는?

**정답**: `LDG.E.128`. 32-bit 스칼라 로드는 `LDG.E`이다.

---

### 문제 2 (Understand)
**질문**: 메모리 코얼레싱이 GPU 성능에 중요한 이유를 설명하라.

**정답**: GPU의 메모리 시스템은 넓은 대역폭을 제공하지만, 이를 최대로 활용하려면 한 번의 트랜잭션으로 가능한 많은 바이트를 전송해야 한다. 코얼레싱이 되지 않으면 최대 32배의 대역폭 낭비가 생긴다. 코얼레싱으로 다수의 접근을 하나의 넓은 트랜잭션으로 병합하면 실효 대역폭이 극대화된다.

---

### 문제 3 (Apply)
**질문**: K=1024일 때 벡터화 루프의 반복 횟수는?

**정답**: **256회**. `K_vec = K / 4 = 1024 / 4 = 256`. 각 반복에서 4개 원소를 처리하므로 256 * 4 = 1024개 원소 전체를 처리한다.

---

### 문제 4 (Analyze)
**질문**: B 행렬 접근이 하나의 스레드 내에서 float4 벡터화가 불가능한 이유는?

**정답**: 하나의 스레드가 k를 바꾸면 주소 차이가 `ldb * sizeof(float)` = ldb * 4 바이트이다. ldb=1024라면 4096바이트 간격이다. float4는 연속 16바이트를 요구하므로 4096바이트 간격의 데이터는 불가능하다. 개별 스레드의 시간적 벡터화와 워프의 공간적 코얼레싱은 다른 개념이다.

---

### 문제 5 (Apply)
**질문**: `FETCH_FLOAT4(A[row * lda + k * 4])` 매크로를 전개하면?

**정답**: `(reinterpret_cast<const float4*>(&(A[row * lda + k * 4]))[0])`. A[row * lda + k * 4]부터 4개 float가 하나의 float4로 로드된다.

---

### 문제 6 (Understand)
**질문**: 전치된 B를 사용하는 커널이 원본보다 나은 이유는?

**정답**: 원본에서 B[k][col] 접근 시 k를 바꾸면 ldb만큼 점프하여 float4 불가능. 전치된 BT[col][k]에서는 k가 행 내 열 방향이 되어 메모리에서 연속 배치. 따라서 A와 B 모두 `LDG.E.128` 사용 가능.

---

### 문제 7 (Evaluate)
**질문**: Level 0 대비 Level 1의 약 3-5배 향상을 메모리 대역폭 관점에서 설명하라.

**정답**: 나이브에서는 32-bit 스칼라 로드에 코얼레싱이 최적화되지 않아 실효 대역폭이 이론적 대역폭의 20-30%. 코얼레싱 커널에서는 128-bit 벡터화 로드로 실효 대역폭이 60-80%로 향상. 약 3-5배 개선이지만 산술 강도가 여전히 0.25 FLOP/byte이므로 메모리 바운드 상태.

---

## 모듈 06: Level 2 -- 공유 메모리 타일링 (8문항) ★ 마스터리 체크포인트

> 통과 기준: 8문항 중 6문항 이상 정답 (80%)

### 문제 1 (Remember)
**질문**: GPU 공유 메모리의 뱅크 수는?

**정답**: **32개**. 연속된 4바이트(float) 워드가 연속된 뱅크에 매핑된다.

---

### 문제 2 (Understand)
**질문**: 공유 메모리 타일링의 핵심 원리를 한 문장으로 설명하라.

**정답**: **데이터 재사용**. 글로벌 메모리에서 타일을 1번 로드한 후, 공유 메모리에서 BLOCK_SIZE번 재사용하여 글로벌 메모리 트래픽을 BLOCK_SIZE배 줄이는 것이다.

---

### 문제 3 (Apply)
**질문**: BLOCK_SIZE = 32일 때 공유 메모리 총 사용량은? (패딩 무시)

**정답**: A_tile: 32 x 32 x 4 = 4096 bytes, B_tile: 32 x 32 x 4 = 4096 bytes. **합계: 8192 바이트 (8 KB)**.

---

### 문제 4 (Apply)
**질문**: 협력적 로딩 코드의 빈칸을 채우라.

**정답**:
```cuda
A_tile[ty][tx] = A[row * lda + a_col];
```
`A_tile[ty][tx]`는 블록 내 위치에 해당하는 타일 원소, `A[row * lda + a_col]`는 글로벌 행렬의 해당 원소이다.

---

### 문제 5 (Analyze)
**질문**: 두 번째 `__syncthreads()`를 제거하면 어떤 문제가 발생하는가?

**정답**: 빠른 스레드가 다음 타일(t+1)의 데이터를 덮어쓸 때, 느린 스레드가 아직 현재 타일(t)의 데이터로 연산 중일 수 있다. 예: 스레드 0이 타일 t+1의 `A_tile[0][3]`에 새 값을 기록하는데, 스레드 7이 타일 t에서 `A_tile[0][3]`을 읽으면 잘못된 값을 얻는다.

---

### 문제 6 (Analyze)
**질문**: 패딩 없이 `float A_tile[16][16]`에서 ty=0과 ty=2가 `A_tile[ty][0]`에 동시 접근하면 뱅크 충돌이 발생하는 이유를 증명하라.

**정답**:
```
Thread ty=0: A_tile[0][0] → 바이트 주소 = 0 → 뱅크 = 0 / 4 % 32 = 0
Thread ty=2: A_tile[2][0] → 바이트 주소 = 128 → 뱅크 = 128 / 4 % 32 = 0
```
두 스레드 모두 **뱅크 0**에 접근하지만 주소가 다르므로 뱅크 충돌 발생. 근본 원인: 행 폭 16 * 4 = 64바이트 = 뱅크 16개분이라 2행 간격이면 32개 뱅크를 한 바퀴 돌아 같은 뱅크로 돌아온다.

---

### 문제 7 (Apply)
**질문**: `fma.rn.f32 %0, %1, %2, %0`에서 각 피연산자의 역할은?

**정답**: `%0`(d): 결과 저장 = sum, `%1`(a): 곱셈 첫 번째 피연산자 = A_tile[ty][k], `%2`(b): 곱셈 두 번째 피연산자 = B_tile[k][tx], `%0`(c): 덧셈 피연산자 = 현재 누적합. 연산: `d = a * b + c`, `.rn` = Round-to-Nearest, `.f32` = 32비트 단정밀도.

---

### 문제 8 (Evaluate)
**질문**: 타일링 커널의 메모리 트래픽 감소 배율(BLOCK_SIZE=16)과 cuBLAS 수준에 도달하기 충분하지 않은 이유는?

**정답**: **16배 감소**. 각 타일을 1번 로드하고 16번 재사용. 그러나 cuBLAS에 미달인 이유: (1) 산술 강도가 약 4.0 FLOP/byte로 릿지 포인트보다 낮아 여전히 메모리 바운드, (2) 메모리 로드와 연산이 순차적으로 일어나 레이턴시 은닉 부재, (3) 각 스레드가 1개 원소만 담당하여 레지스터 활용 비효율적.

---

## 모듈 07: Level 3 -- 레지스터 블로킹 (8문항)

### 문제 1 (Remember)
**질문**: TM=8, TN=8일 때 한 스레드가 계산하는 C 원소의 수는?

**정답**: **64개**. TM * TN = 8 * 8 = 64. 이 64개는 `reg_C[8][8]`에 레지스터로 유지된다. 참조: `src/kernels/sgemm_register_blocking.cu:81`.

---

### 문제 2 (Understand)
**질문**: 레지스터 블로킹이 산술 강도를 높이는 원리를 수식으로 설명하라.

**정답**:
```
산술 강도 = TM * TN / (2 * (TM + TN))
TM=TN=1: AI = 1/4 = 0.25 FLOP/byte
TM=TN=8: AI = 64/32 = 2.0 FLOP/byte
```
분모(로드량)는 선형 증가하지만 분자(FLOPs)는 이차 증가하므로 타일이 클수록 산술 강도가 향상된다.

---

### 문제 3 (Apply)
**질문**: BM=128, BN=128, BK=8일 때 공유 메모리 사용량은? (기본/패딩 각각)

**정답**: 기본: As[128][8] + Bs[8][128] = 2 * 1024 * 4 = **8 KB**. 패딩(+1): As[128][9] + Bs[8][129] = (1152 + 1032) * 4 = **약 8.5 KB**.

---

### 문제 4 (Apply)
**질문**: 256개 스레드가 A 타일(1024 원소)을 로드할 때 스레드당 로드 횟수는?

**정답**: **4회**. 1024 / 256 = 4. A와 B 모두 스레드당 4회 로드. 총 8192 bytes/블록/K반복.

---

### 문제 5 (Analyze)
**질문**: `A_TILE_ROW_STRIDE = 256/8 = 32`의 의미는?

**정답**: A 타일(128행 x 8열)에서 256개 스레드를 8열로 나누면 256/8 = 32개 행을 한 번에 로드 가능. "스트라이드 32"는 한 번의 로드 반복에서 커버하는 행 범위이다. 128행 전체를 로드하려면 128/32 = 4번 반복.

---

### 문제 6 (Analyze)
**질문**: 레지스터 255개 초과 시 발생하는 현상과 SASS에서 확인 방법은?

**정답**: **레지스터 스필링** 발생. 일부 값이 로컬 메모리(물리적으로 글로벌 메모리)에 저장되어 ~400-600 사이클 지연. SASS에서 `LDL`(Load Local), `STL`(Store Local) 명령어가 보이면 스필링이 발생한 것이다. Nsight Compute의 `Local Memory Per Thread > 0`으로도 확인 가능.

---

### 문제 7 (Evaluate)
**질문**: 점유율 25%인 Level 3가 점유율 50%인 Level 2보다 빠른 이유를 분석하라.

**정답**: **산술 강도 관점**: Level 2는 AI=0.25으로 메모리 바운드, Level 3는 AI=2.0으로 연산 바운드에 가까워짐. **ILP 관점**: Level 2는 K반복당 1 FMA로 파이프라인 활용이 낮지만, Level 3는 64개 독립 FMA로 파이프라인을 가득 채운다. 높은 점유율은 TLP로 메모리 레이턴시를 숨기는 메커니즘이지만, Level 3에서는 메모리 요청 자체가 적고 ILP가 높아 점유율이 낮아도 성능이 우수하다.

---

### 문제 8 (Create)
**질문**: TM=4, TN=16으로 변경 시 레지스터 사용량과 산술 강도의 변화는?

**정답**: AI = 4*16 / (2*(4+16)) = 64/40 = 1.6 FLOP/byte (TM=8,TN=8의 2.0 대비 20% 감소). 레지스터: reg_C[4][16]=64, reg_A[4]=4, reg_B[16]=16, 총 ~108개 (8x8의 ~90 대비 증가). TM=TN인 정사각형 타일이 산술 강도 관점에서 최적이다 (AM-GM 부등식).

---

## 모듈 08: Level 4 -- 더블 버퍼링 (7문항) ★ 마스터리 체크포인트

> 통과 기준: 7문항 중 6문항 이상 정답 (약 80%)

### 문제 1 (Remember)
**질문**: 더블 버퍼링에서 사용하는 공유 메모리 버퍼의 세트 수는?

**정답**: **(b) 2개**. As[2][BM][BK+1], Bs[2][BK][BN+1]로 두 세트의 버퍼를 교대로 사용한다.

---

### 문제 2 (Understand)
**질문**: 소프트웨어 파이프라이닝의 핵심 원리를 가장 정확하게 설명한 것은?

**정답**: **(c) 메모리 로드와 연산을 시간적으로 중첩시켜 로드 레이턴시를 숨긴다**. 대역폭을 2배로 늘리거나 연산 속도를 향상시키는 것이 아니라, 시간적 중첩으로 유휴 시간을 줄이는 것이다.

---

### 문제 3 (Apply)
**질문**: kt = 0, 1, 2, 3에 대해 curr_buf와 next_buf의 값은?

**정답**:

| kt | curr_buf = kt % 2 | next_buf = (kt+1) % 2 |
|----|--------------------|-----------------------|
| 0  | 0                  | 1                     |
| 1  | 1                  | 0                     |
| 2  | 0                  | 1                     |
| 3  | 1                  | 0                     |

---

### 문제 4 (Analyze)
**질문**: 프롤로그를 생략하면 어떤 문제가 발생하는가?

**정답**: **(b) 메인 루프의 첫 반복에서 초기화되지 않은 공유 메모리 데이터로 연산하여 잘못된 결과가 나온다**. 프롤로그는 첫 번째 타일을 미리 로드하는 단계이다. 이것 없이 메인 루프에 진입하면, curr_buf에 유효한 데이터가 없는 상태에서 연산을 시작하게 된다.

---

### 문제 5 (Analyze)
**질문**: Long Scoreboard 40-50%→20-30%, Math Pipe Throttle 10-20%→30-40% 변화의 의미는?

**정답**: 더블 버퍼링으로 메모리 로드와 연산이 시간적으로 중첩되어, 글로벌 메모리 데이터 대기(Long Scoreboard)가 감소했다. 동시에 연산 유닛이 더 많이 활용되어 Math Pipe Throttle이 증가했다. 이는 GPU가 **"기다림"에서 "연산"으로** 시간을 재분배한 것이다. Math Pipe Throttle이 높아지는 것은 긍정적 신호이다.

---

### 문제 6 (Evaluate)
**질문**: 공유 메모리 2배 증가(~8.5KB→~17KB)가 점유율에 미치는 영향과 더블 버퍼링이 유리한 이유는?

**정답**: A100의 SM당 164 KB 기준으로 17 KB는 10.4%에 불과하여 공유 메모리 자체가 점유율 병목이 될 가능성은 낮다. 레지스터가 주된 점유율 제한 요인이다. 더블 버퍼링이 유리한 이유: 점유율이 다소 낮아져도, 메모리 레이턴시 은닉 효과가 점유율 감소에 의한 성능 저하를 크게 상쇄한다. Long Scoreboard 스톨이 절반으로 줄어 전체 처리량이 증가한다.

---

### 문제 7 (Create)
**질문**: K % BK != 0일 때 에필로그를 어떻게 설계하는가?

**정답**:
```
메인 루프: K / BK개의 완전한 타일 처리

에필로그 (나머지 처리):
  remainder = K % BK
  if (remainder > 0) {
      // 1. 나머지 타일을 공유 메모리에 로드 (유효 범위만)
      load partial tile: As[0..BM-1][0..remainder-1]
                         Bs[0..remainder-1][0..BN-1]
      // 유효 범위 밖은 0으로 패딩
      __syncthreads()

      // 2. 내부 K 루프를 remainder 횟수만 반복
      for (k = 0; k < remainder; ++k) {
          // 외적 마이크로커널 (동일)
      }
      __syncthreads()
  }

에필로그 장점: 메인 루프에서 경계 검사를 제거하여 분기 오버헤드 감소
에필로그 단점: 코드 복잡도 증가, 나머지 타일에서 점유율이 낮을 수 있음
```

---

## 모듈 09: Level 5 -- 비동기 복사 (10문항) ★ 최종 마스터리 체크포인트

### Q1 (Remember)
**질문**: cp.async 사용에 필요한 최소 Compute Capability는?

**정답**: **8.0** (Ampere 아키텍처). 코드에서 `__CUDA_ARCH__ >= 800`으로 가드. Volta(7.0)나 Turing(7.5)에서는 사용 불가. 참조: `src/kernels/sgemm_async_copy.cu:43`.

---

### Q2 (Remember)
**질문**: SASS에서 cp.async가 변환된 명령어 이름은?

**정답**: **LDGSTS** (Load Global Store Shared). 글로벌→공유 직접 전송을 단일 하드웨어 명령으로 수행.

---

### Q3 (Understand)
**질문**: cp.async의 레지스터 우회가 가져오는 구체적 이점 2가지는?

**정답**:
1. **레지스터 압력 감소 → 점유율 향상**: 전송 과정에서 레지스터를 사용하지 않아 블록당 레지스터 수가 줄고, SM당 더 많은 블록 동시 실행 가능.
2. **Long Scoreboard 스톨 제거**: LDG의 목적지 레지스터를 STS가 읽을 때 발생하는 의존성 사슬이 원천적으로 존재하지 않음.

---

### Q4 (Apply)
**질문**: `cp_async_wait_group(2)` 호출 시 하드웨어가 보장하는 것은?

**정답**: 미완료 그룹 수가 2개 이하가 될 때까지 대기. 미완료 3개(예: {G3, G4, G5})라면 가장 오래된 G3의 완료를 보장. 이미 2개 이하면 즉시 반환.

---

### Q5 (Apply)
**질문**: NUM_STAGES=3, kt=5일 때 curr_stage와 prefetch_stage는?

**정답**: `curr_stage = 5 % 3 = 2`. `prefetch_tile = 5 + 2 = 7`, `prefetch_stage = 7 % 3 = 1`. Buf2의 데이터로 연산하면서 타일 7을 Buf1에 비동기 로드.

---

### Q6 (Analyze)
**질문**: commit_group 없이 wait_group(2)를 호출하면?

**정답**: 그룹이 형성되지 않아 특정 타일의 완료를 보장할 수 없다. (1) 아직 도착하지 않은 데이터로 연산하여 정확성 문제 발생. (2) 안전을 위해 wait_group(0)을 사용하면 비동기성이 완전히 제거되어 성능 저하.

---

### Q7 (Analyze)
**질문**: 트리플 버퍼링이 더블 버퍼링보다 레이턴시 은닉에 유리한 이유는?

**정답**: 더블 버퍼링은 파이프라인 깊이 1(동시 로드 1개), 트리플 버퍼링은 깊이 2(동시 로드 2개). `T_load > T_compute`일 때 더블 버퍼링은 유휴 시간 발생하지만, 트리플 버퍼링은 `T_load > 2 * T_compute`가 아닌 한 유휴 없음.

---

### Q8 (Evaluate)
**질문**: NUM_STAGES=4의 장단점을 공유 메모리와 점유율 관점에서 평가하라.

**정답**: **장점**: 파이프라인 깊이 3으로 더 긴 레이턴시도 숨김 가능. **단점**: 공유 메모리 ~26.2KB→~34.9KB 증가. SM당 동시 블록 수 감소 가능. 대부분 NUM_STAGES=3이면 충분하므로 추가 깊이가 공유 메모리 증가를 정당화하지 못하는 경우가 많다.

---

### Q9 (Create)
**질문**: Ampere 미만 GPU를 위한 폴백 전략을 설계하라.

**정답**: `#if __CUDA_ARCH__ >= 800`으로 분기. Ampere 이상에서는 cp.async + 트리플 버퍼링, 그 미만에서는 NUM_STAGES=2의 더블 버퍼링 + LDG/STS 방식. 레지스터 프리페치로 소프트웨어적 비동기성 모방: LDG 발행 → 연산 먼저 수행 → STS 나중에. cp.async만큼 효율적이지는 않지만 파이프라인 구조는 유지.

---

### Q10 (Create)
**질문**: BK를 8에서 16으로 변경 시 연쇄적 영향은?

**정답**:
1. 공유 메모리 ~26.2KB→~50.9KB (약 2배 증가)
2. K 내부 루프 8회→16회 (타일당 연산량 2배, 산술 강도 향상)
3. K 타일 수 절반 (루프 오버헤드 감소)
4. 스레드당 로드 4개→8개 (로드 패턴 조정 필요)
5. 패딩 BK+1=17 유지
6. NUM_STAGES 재검토 필요 (점유율 영향)
7. 최적 BK는 프로파일링으로 결정

---

## 모듈 10: 종합 성능분석 및 프로파일링 (8문항)

### Q1 (Remember)
**질문**: SOL 분석의 두 가지 핵심 축은?

**정답**: SM Throughput (%)과 Memory Throughput (%). SM은 연산 유닛 활용도, Memory는 메모리 대역폭 활용도.

---

### Q2 (Understand)
**질문**: Latency Bound 상태에서 SM과 Memory 모두 낮은 이유는?

**정답**: 메모리 요청을 보낸 뒤 대기하는 동안 연산도, 추가 메모리 요청도 못하는 상태. 동시 비행 요청이 적어 메모리 대역폭을 포화시키지 못하고(Memory SOL 낮음), 대기 중 연산 유닛도 유휴(SM SOL 낮음). 요청-대기-연산이 직렬로 일어나는 것이 원인.

---

### Q3 (Apply)
**질문**: 피크 19,500 GFLOP/s, 피크 대역폭 2,039 GB/s에서 변곡점 산술 강도는?

**정답**: `19,500 / 2,039 ≈ 9.56 FLOP/byte`. 이 미만이면 메모리 바운드, 이상이면 컴퓨트 바운드.

---

### Q4 (Apply)
**질문**: M=N=K=4096 SGEMM이 3.2ms에 완료되면 GFLOP/s는?

**정답**: `FLOP = 2 * 4096^3 = 137,438,953,472. GFLOP/s = 137,438,953,472 / (3.2 × 10^-3 × 10^9) ≈ 42,950 GFLOP/s`.

---

### Q5 (Analyze)
**질문**: SASS에서 LDGSTS 대신 LDG + STS가 보이면 무슨 문제인가?

**정답**: cp.async가 의도대로 작동하지 않음. 데이터가 레지스터를 경유하여: (1) 레지스터 사용량 증가, (2) 비동기 파이프라인 이점 상실, (3) 복사-연산 오버랩 불완전. 원인: 컴파일러 버전, 정렬 미충족, PTX 인라인 어셈블리 오류 등.

---

### Q6 (Analyze)
**질문**: Long Scoreboard 60%, Wait 25%일 때 상태와 개선 방향은?

**정답**: 메모리 대기와 동기화 대기가 합쳐 85%로 연산 유닛 대부분 유휴. 개선: (1) 더블 버퍼링/cp.async로 Long Scoreboard 감소, (2) 트리플 버퍼링으로 Wait 분산.

---

### Q7 (Evaluate)
**질문**: 점유율 25%인 레지스터 블로킹이 점유율 62%인 타일링보다 3배 빠른 이유는?

**정답**: (1) 산술 강도 16배 증가 (0.5→8.0 FLOP/byte), (2) 외적 마이크로커널에서 128개 독립 FMA로 파이프라인을 완전히 채움(ILP), (3) 데이터 재사용 극대화로 글로벌 메모리 트래픽 감소. 메모리 바운드에서 컴퓨트 바운드로 전환되면 많은 워프가 불필요하다.

---

### Q8 (Evaluate)
**질문**: cuBLAS 90% 이후 수익 체감이 심한 이유는?

**정답**: (1) 암달의 법칙: 남은 10%는 여러 소규모 요인의 합으로 개별 효과가 작다. (2) 하드웨어 한계 접근: 80%+ 활용 중이라 물리적 개선 여지 제한. (3) cuBLAS의 비공개 최적화: 텐서 코어, TMA, 아키텍처별 자동 튜닝. (4) 최적화 간 상호작용: 타일 크기↑→점유율↓ 등 트레이드오프 복잡화.

---

## 퀴즈 통계 요약

| 모듈 | 문항 수 | Bloom 분포 | 마스터리 체크포인트 |
|------|---------|-----------|-------------------|
| 01: GPU 아키텍처 | 8 | R:2, U:3, Ap:2, An:1 | - |
| 02: CUDA 프로그래밍 | 6 | R:1, U:2, Ap:2, An:1 | - |
| 03: 수학적 배경 | 5 | R:1, U:1, Ap:2, An:1 | - |
| 04: Level 0 나이브 | 8 | R:1, U:2, Ap:2, An:2, Ev:1 | - |
| 05: Level 1 코얼레싱 | 7 | R:1, U:2, Ap:2, An:1, Ev:1 | - |
| 06: Level 2 타일링 | 8 | R:1, U:1, Ap:3, An:2, Ev:1 | ★ (80% 통과) |
| 07: Level 3 레지스터 | 8 | R:1, U:1, Ap:2, An:2, Ev:1, Cr:1 | - |
| 08: Level 4 더블 버퍼 | 7 | R:1, U:1, Ap:1, An:2, Ev:1, Cr:1 | ★ (80% 통과) |
| 09: Level 5 비동기 | 10 | R:2, U:1, Ap:2, An:2, Ev:1, Cr:2 | ★ (최종) |
| 10: 성능분석 | 8 | R:1, U:1, Ap:2, An:2, Ev:2 | - |
| **합계** | **75** | | |

> R=Remember, U=Understand, Ap=Apply, An=Analyze, Ev=Evaluate, Cr=Create
