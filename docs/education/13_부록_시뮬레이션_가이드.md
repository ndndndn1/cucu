# 부록: 시뮬레이션 상세 가이드

이 문서는 전체 교육 과정(모듈 01-10)에 포함된 10종의 시뮬레이션에 대한 상세 수행 절차, 필요 도구, 예상 결과, 토론 질문을 제공한다.

> **시뮬레이션의 목적**: 추상적 개념을 종이 위에서 직접 추적하여 직관을 구축한다. 시뮬레이션은 GPU 하드웨어 없이도 수행할 수 있으며, 스프레드시트를 활용하면 더 큰 규모로 확장할 수 있다.

---

## 시뮬레이션 1: Thread-to-Hardware 매핑

**모듈**: 01 (사전지식 -- GPU 아키텍처)
**유형**: 계산/표
**핵심 학습**: 그리드/블록 → SM 배정 과정의 이해

### 필요 도구
- 종이와 연필 또는 스프레드시트
- 계산기

### 수행 절차

**주어진 조건:**
- 행렬 크기: 256 x 256
- 스레드 블록 크기: 16 x 16 (= 256 스레드/블록)
- 각 스레드가 결과 행렬의 원소 1개를 계산
- GPU: 108 SMs (예: A100)

**Step 1: 그리드 크기 계산**
```
grid.x = ceil(256 / 16) = 16
grid.y = ceil(256 / 16) = 16
그리드 크기: (16, 16)
```

**Step 2: 총 블록 수 계산**
```
총 블록 = grid.x * grid.y = 16 * 16 = 256 블록
```

**Step 3: 블록당 워프 수 계산**
```
스레드/블록 = 16 * 16 = 256
워프/블록 = 256 / 32 = 8 워프
```

**Step 4: SM 배정 계산**
```
SM 수: 108
총 블록: 256
초기 배정: 256 / 108 = 2.37...
  → 대부분 SM에 2블록, 일부에 3블록

정확한 분배:
  108 * 2 = 216 블록 → 108개 SM에 각 2개
  나머지 40 블록 → 40개 SM에 1개 추가
  → 40개 SM: 3블록, 68개 SM: 2블록
```

**Step 5: 블록-SM 매핑표 작성 (처음 10개)**

| 블록 ID | 블록 좌표 (bx, by) | 배정 SM (라운드로빈 가정) | 블록 내 워프 수 |
|---------|-------------------|--------------------------|----------------|
| 0 | (0, 0) | SM 0 | 8 |
| 1 | (1, 0) | SM 1 | 8 |
| 2 | (2, 0) | SM 2 | 8 |
| ... | ... | ... | 8 |
| 9 | (9, 0) | SM 9 | 8 |

> 참고: 실제 하드웨어의 블록-SM 매핑 순서는 드라이버가 결정하며 반드시 라운드로빈은 아니다.

**Step 6: SM당 점유율 추정**
```
SM에 2블록:
  활성 스레드 = 2 * 256 = 512
  활성 워프 = 16
  점유율 = 16 / 64 = 25% (SM당 최대 64 워프 가정)

SM에 3블록:
  활성 스레드 = 3 * 256 = 768
  활성 워프 = 24
  점유율 = 24 / 64 = 37.5%
```

### 예상 결과
256x256 행렬은 108-SM GPU에서 비교적 작아 점유율이 낮다. 4096x4096 행렬이면 65,536 블록으로 SM당 수백 블록이 할당 가능하여 점유율이 크게 향상된다.

### 토론 질문
1. 행렬 크기를 4096x4096으로 늘리면 SM당 블록 수는 어떻게 변하는가?
2. 블록 크기를 32x32로 변경하면 총 블록 수와 점유율에 어떤 영향이 있는가?
3. 점유율이 25%일 때 성능이 반드시 낮은 것은 아닌 이유는? (모듈 07의 레지스터 블로킹과 연결)

---

## 시뮬레이션 2: Driver API 호출 시퀀스

**모듈**: 02 (사전지식 -- CUDA 프로그래밍 기초)
**유형**: 플로차트
**핵심 학습**: Runtime API vs Driver API의 차이, 자원 관리 생명주기

### 필요 도구
- 종이와 연필 또는 플로차트 도구
- 프로젝트 코드 참조: `src/driver/cuda_driver.cpp`, `include/cuda_driver_wrapper.hpp`

### 수행 절차

**과제**: 벡터 덧셈(`C[i] = A[i] + B[i]`, N개 원소)을 수행하기 위한 Driver API 호출 시퀀스를 나열하라.

**Step 1: 종이에 먼저 작성한 뒤, 아래 정답과 대조한다.**

**Driver API 호출 시퀀스 (17개 호출):**
```
[초기화]
  1. cuInit(0)                                    -- 드라이버 초기화
  2. cuDeviceGet(&device, 0)                      -- 디바이스 핸들 획득
  3. cuCtxCreate(&ctx, 0, device)                  -- 컨텍스트 생성

[모듈 로딩]
  4. cuModuleLoad(&module, "vecadd.ptx")           -- PTX 로드 + JIT 컴파일
  5. cuModuleGetFunction(&func, module, "vecadd")  -- 함수 핸들 획득

[메모리 할당]
  6. cuMemAlloc(&d_A, N * sizeof(float))
  7. cuMemAlloc(&d_B, N * sizeof(float))
  8. cuMemAlloc(&d_C, N * sizeof(float))

[데이터 전송: Host → Device]
  9. cuMemcpyHtoD(d_A, h_A, N * sizeof(float))
 10. cuMemcpyHtoD(d_B, h_B, N * sizeof(float))

[커널 실행]
 11. cuLaunchKernel(func, gridDim, 1, 1, blockDim, 1, 1, 0, NULL, args, NULL)

[데이터 전송: Device → Host]
 12. cuMemcpyDtoH(h_C, d_C, N * sizeof(float))

[정리]
 13. cuMemFree(d_A)
 14. cuMemFree(d_B)
 15. cuMemFree(d_C)
 16. cuModuleUnload(module)
 17. cuCtxDestroy(ctx)
```

**Step 2: Runtime API 동등 코드와 비교한다.**

```
[초기화]       (없음 -- 암묵적)
[모듈 로딩]    (없음 -- 빌드 시 링크)

[메모리 할당]
  1. cudaMalloc(&d_A, N * sizeof(float))
  2. cudaMalloc(&d_B, N * sizeof(float))
  3. cudaMalloc(&d_C, N * sizeof(float))

[전송 Host→Device]
  4. cudaMemcpy(d_A, h_A, ..., cudaMemcpyHostToDevice)
  5. cudaMemcpy(d_B, h_B, ..., cudaMemcpyHostToDevice)

[커널]
  6. vecadd<<<gridDim, blockDim>>>(d_A, d_B, d_C, N)

[전송 Device→Host]
  7. cudaMemcpy(h_C, d_C, ..., cudaMemcpyDeviceToHost)

[정리]
  8-10. cudaFree(d_A/d_B/d_C)
```

**총 10개 호출** -- Driver API의 약 절반.

### 예상 결과
Driver API는 호출 수가 약 1.7배 많지만, 초기화, 컨텍스트 관리, JIT 컴파일 옵션, 에러 로그 접근 등에 대한 명시적 제어가 가능하다. 이 프로젝트가 Driver API를 사용하는 이유이다.

### 토론 질문
1. RAII 패턴을 적용하면 정리(cleanup) 단계의 호출이 어떻게 바뀌는가?
2. PTX JIT 컴파일이 실패했을 때, Driver API에서만 얻을 수 있는 정보는 무엇인가?
3. 이 프로젝트의 `CudaContext` 클래스가 위 시퀀스의 어느 부분을 래핑하는지 코드에서 찾아보라.

---

## 시뮬레이션 3: FLOP 계산기

**모듈**: 03 (사전지식 -- 행렬곱셈 수학적 배경)
**유형**: 계산
**핵심 학습**: FLOP, GFLOP/s, 산술 강도 메트릭 계산

### 필요 도구
- 계산기 또는 스프레드시트
- 공식 참조: `include/sgemm_kernels.hpp:195-204`

### 수행 절차

#### 예제 1: 직사각 행렬
**주어진 조건**: M=1024, N=2048, K=512

| 단계 | 계산 | 결과 |
|------|------|------|
| 1. 총 FLOP | 2 × 1024 × 2048 × 512 | 2,147,483,648 ≈ 2.15 GFLOP |
| 2. GFLOP/s (0.5ms 가정) | 2,147,483,648 / (0.0005 × 10⁹) | ~4,295 GFLOP/s |
| 3. 최소 메모리 트래픽 | 4 × (1024×512 + 512×2048 + 2×1024×2048) | ~23.07 MB |
| 4. 이론적 산술 강도 | FLOP / Bytes | 93.1 FLOP/byte |

#### 예제 2: 정사각 행렬
**주어진 조건**: M=N=K=4096

| 단계 | 계산 | 결과 |
|------|------|------|
| 1. 총 FLOP | 2 × 4096³ | ~137.44 GFLOP |
| 2. 최소 바이트 | 4 × 4 × 4096² | ~268.44 MB |
| 3. 산술 강도 | 137.44G / 268.44M | 512 FLOP/byte |
| 4. 검증: n/8 | 4096 / 8 | 512 (일치!) |

**다양한 실행 시간에서의 성능:**

| 실행 시간 | GFLOP/s | 피크 19,500 대비 |
|----------|---------|-----------------|
| 50.0 ms | 2,749 | 14.1% |
| 10.0 ms | 13,744 | 70.5% |
| 7.05 ms | 19,496 | ~100% (이론 한계) |

### 예상 결과
정사각 행렬에서 산술 강도는 `n/8`이라는 간단한 공식으로 검증 가능하다. 4096 크기에서 이론적 산술 강도 512 FLOP/byte는 릿지 포인트(~10)을 훨씬 초과하므로, 충분한 최적화가 적용되면 컴퓨트 바운드 영역에 도달할 수 있다.

### 토론 질문
1. M=N=K=4096에서 실행 시간 2.0ms는 왜 비현실적인가? (힌트: 피크 GFLOP/s 초과)
2. 산술 강도가 높다고 항상 빠른 것은 아니다. 나이브 커널에서 이론적 산술 강도와 실제 산술 강도가 다른 이유는?
3. 직사각 행렬(M≠N≠K)에서 산술 강도를 최대화하려면 어느 차원을 키워야 하는가?

---

## 시뮬레이션 4: 메모리 접근 패턴 시각화

**모듈**: 04 (Level 0 -- 나이브 구현)
**유형**: 그리드 다이어그램
**핵심 학습**: 코얼레싱 여부 판별, 캐시 라인 트랜잭션 카운팅

### 필요 도구
- 종이(모눈종이 권장)와 색연필/형광펜
- 워프 구성 이해

### 수행 절차

**설정**: 8×8 행렬, 블록 16×16, lda = ldb = 8. 워프 0 = 처음 32개 스레드.

```
워프 0 배치 (16×16 블록):
tx:  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
ty=0: T0 T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 ... T15
ty=1: T16 T17 T18 ... T31
```

**Step 1: B 행렬 접근 패턴 그리기 (k=0)**

종이에 B 행렬의 메모리 레이아웃을 그리고, 각 스레드가 접근하는 주소를 표시한다.

```
T0(col=0): B[0]  → 주소 0
T1(col=1): B[1]  → 주소 4
T2(col=2): B[2]  → 주소 8
...
T7(col=7): B[7]  → 주소 28
→ 연속 주소! → 코얼레싱 가능
```

**Step 2: A 행렬 접근 패턴 그리기 (k=0)**

```
T0 (row=0): A[0*8+0] = A[0]   → 주소 0
T1 (row=0): A[0*8+0] = A[0]   → 주소 0 (동일! 브로드캐스트)
...
T16(row=1): A[1*8+0] = A[8]   → 주소 32 (다른 행!)
→ ty=0은 주소 0, ty=1은 주소 32 → 서로 다른 캐시 라인
```

**Step 3: 트랜잭션 수 비교표 작성**

| 행렬 | 워프 0의 접근 (k=0) | 필요한 32B 섹터 수 | 비고 |
|------|---------------------|-------------------|------|
| B | B[0]~B[7], 연속 32바이트 | 1 | 코얼레싱: 효율적 |
| A | A[0]과 A[8], 32바이트 간격 | 2 | 비연속: 비효율적 |

### 예상 결과
B 접근은 코얼레싱되지만, A 접근은 비코얼레싱이다. 실제 4096×4096 행렬에서는 A의 간격이 16,384바이트가 되어 비효율이 극대화된다.

### 토론 질문
1. 같은 분석을 k=1에 대해 반복하면 패턴이 어떻게 변하는가?
2. 행렬 A의 접근 패턴을 개선하려면 어떤 방법이 있는가? (힌트: 전치, 또는 접근 방향 변경)
3. "브로드캐스트"가 가능한 조건은 무엇인가?

---

## 시뮬레이션 5: 코얼레싱 패턴 검사기

**모듈**: 05 (Level 1 -- 메모리 코얼레싱)
**유형**: 주소 추적
**핵심 학습**: float4 벡터화의 효과, 트랜잭션 카운팅

### 필요 도구
- 종이 또는 스프레드시트
- 주소 계산용 계산기

### 수행 절차

**설정**: M=N=K=1024, lda=ldb=1024, 블록 16×16, 워프 = ty=0..1, tx=0..15

**Step 1: A 행렬 접근 분석 (k=0)**
```
sgemm_coalesced 커널에서 A 접근:
T0 (row=0): A[0 * 1024 + 0] = A[0]      → 0x0000
T1 (row=0): A[0 * 1024 + 0] = A[0]      → 0x0000  (동일!)
...
T16(row=1): A[1 * 1024 + 0] = A[1024]   → 0x1000  (다른 행)

→ 같은 ty의 스레드는 동일 주소 (브로드캐스트)
→ ty=0과 ty=1은 4096바이트 간격
→ A 접근: 2 트랜잭션/워프
```

**Step 2: B 행렬 접근 분석 (4개 스칼라 로드)**
```
b0: B[0 * 1024 + col]  → col=0..15 연속 → 2 섹터
b1: B[1 * 1024 + col]  → 다른 캐시 라인  → 2 섹터
b2: B[2 * 1024 + col]  → 다른 캐시 라인  → 2 섹터
b3: B[3 * 1024 + col]  → 다른 캐시 라인  → 2 섹터
→ B 접근: 8 섹터 = 4 트랜잭션/워프
```

**Step 3: 비교 요약표**

| 행렬 | 접근 방식 | 고유 주소 | 캐시 라인 | 트랜잭션 |
|------|----------|----------|----------|---------|
| A | float4 x 2 (브로드캐스트) | 2 | 2 | 2 |
| B | 스칼라 x 4행 | 16 × 4행 | 4 | 4 |
| **합계** | | | | **6** |

### 예상 결과
나이브 커널 대비 트랜잭션 수가 크게 감소하지만, B 행렬의 행 간 점프(ldb 간격)는 여전히 남아 있다. 이 문제는 전치(BT) 변형으로 해결하거나, 다음 레벨의 타일링으로 근본적으로 해결한다.

### 토론 질문
1. B 행렬에도 float4를 적용할 수 있는가? 왜 안 되는가?
2. B를 전치하면 트랜잭션 수가 어떻게 변하는가?
3. float4 로드에 16바이트 정렬이 필요한 이유는?

---

## 시뮬레이션 6: 뱅크 충돌 검출기

**모듈**: 06 (Level 2 -- 공유 메모리 타일링)
**유형**: 뱅크 그리드
**핵심 학습**: 패딩의 효과, 뱅크 충돌 메커니즘

### 필요 도구
- 종이(뱅크 번호 계산표) 또는 스프레드시트
- 공식: `bank = (row * width + col) % 32`

### 수행 절차

#### 문제: 패딩 없는 경우

`float smem[16][16]`. 16개 스레드(ty=0..15)가 동시에 `smem[ty][5]` (k=5)를 읽는다.

**뱅크 번호 계산 (col = 5):**

| 스레드 ty | 주소 계산 | 뱅크 = (ty*16+5)%32 | 충돌? |
|-----------|----------|---------------------|-------|
| 0 | (0*16+5) | 5 | |
| 1 | (1*16+5) | 21 | |
| 2 | (2*16+5) | **5** | ★ ty=0과 충돌 |
| 3 | (3*16+5) | **21** | ★ ty=1과 충돌 |
| 4 | (4*16+5) | **5** | ★ 충돌 |
| 5 | (5*16+5) | **21** | ★ 충돌 |
| ... | ... | 5와 21 교대 | ... |
| 14 | (14*16+5) | **5** | ★ 충돌 |
| 15 | (15*16+5) | **21** | ★ 충돌 |

**결과: 8-way 뱅크 충돌**이 두 뱅크(5, 21)에서 동시 발생. 접근이 8배 직렬화된다.

#### 해결: +1 패딩 적용

`float smem[16][17]`. 행 폭 = 17.

**뱅크 번호 계산 (col = 5):**

| 스레드 ty | 뱅크 = (ty*17+5)%32 | 충돌? |
|-----------|---------------------|-------|
| 0 | 5 | |
| 1 | 22 | |
| 2 | 7 | |
| 3 | 24 | |
| 4 | 9 | |
| 5 | 26 | |
| 6 | 11 | |
| 7 | 28 | |
| 8 | 13 | |
| 9 | 30 | |
| 10 | 15 | |
| 11 | 0 | |
| 12 | 17 | |
| 13 | 2 | |
| 14 | 19 | |
| 15 | 4 | |

**16개 스레드가 모두 서로 다른 뱅크에 접근!** 충돌 완전 제거.

### 예상 결과
+1 패딩의 핵심 원리: 17과 32는 서로소(coprime)이므로, `ty * 17 mod 32`는 ty=0..31에서 32개의 서로 다른 나머지를 생성한다.

### 토론 질문
1. +2 패딩(`smem[16][18]`)은 충돌을 제거하는가? 18과 32가 서로소인가?
2. col 값을 바꾸면(예: col=0) 패딩 없는 경우의 충돌 패턴이 어떻게 변하는가?
3. B_tile에서도 같은 분석을 해 보라. `B_tile[k][tx]` 접근에서 k를 바꿀 때 뱅크 충돌이 발생하는가?

---

## 시뮬레이션 7: 외적 연산 추적기

**모듈**: 07 (Level 3 -- 레지스터 블로킹)
**유형**: 행렬 그리드
**핵심 학습**: 레지스터 재사용, 산술 강도 향상 원리

### 필요 도구
- 종이(행렬 그리드) 또는 스프레드시트
- 계산기

### 수행 절차

#### Part 1: 축소 버전 (TM=4, TN=4)

**주어진 값:**
```
reg_A = [a0, a1, a2, a3]   (공유 메모리에서 로드)
reg_B = [b0, b1, b2, b3]   (공유 메모리에서 로드)
```

**외적 결과 reg_C를 표로 채우기:**

```
          b0       b1       b2       b3
    ┌────────┬────────┬────────┬────────┐
a0  │ a0*b0  │ a0*b1  │ a0*b2  │ a0*b3  │
    ├────────┼────────┼────────┼────────┤
a1  │ a1*b0  │ a1*b1  │ a1*b2  │ a1*b3  │
    ├────────┼────────┼────────┼────────┤
a2  │ a2*b0  │ a2*b1  │ a2*b2  │ a2*b3  │
    ├────────┼────────┼────────┼────────┤
a3  │ a3*b0  │ a3*b1  │ a3*b2  │ a3*b3  │
    └────────┴────────┴────────┴────────┘
```

**산술 강도 계산:**
```
로드: 4 + 4 = 8 floats = 32 bytes
FMA: 4 * 4 = 16 → 32 FLOPs
AI = 32 / 32 = 1.0 FLOP/byte
```

#### Part 2: 실제 크기 (TM=8, TN=8)
```
로드: 8 + 8 = 16 floats = 64 bytes
FMA: 8 * 8 = 64 → 128 FLOPs
AI = 128 / 64 = 2.0 FLOP/byte
```

#### Part 3: 크기별 비교표

| TM=TN | 로드(floats) | 로드(bytes) | FMA 수 | FLOPs | AI (FLOP/byte) | 레지스터(C만) |
|-------|-------------|-------------|--------|-------|----------------|-------------|
| 1 | 2 | 8 | 1 | 2 | 0.25 | 1 |
| 2 | 4 | 16 | 4 | 8 | 0.50 | 4 |
| 4 | 8 | 32 | 16 | 32 | 1.00 | 16 |
| **8** | **16** | **64** | **64** | **128** | **2.00** | **64** |
| 16 | 32 | 128 | 256 | 512 | 4.00 | 256 (스필링!) |

**일반 공식**: TM=TN=T일 때, `AI = T / 4`

#### Part 4: 비대칭 타일 연습

TM=4, TN=16:
```
로드: 4+16 = 20 floats = 80 bytes
FMA: 4*16 = 64 → 128 FLOPs
AI = 128/80 = 1.6 FLOP/byte (TM=TN=8의 2.0보다 20% 낮음)
```

### 예상 결과
TM=TN=8이 최적 지점이다. TM=16이면 reg_C만 256개로 스필링이 발생하고, TM=TN=4이면 AI가 1.0으로 불충분하다. AM-GM 부등식에 의해 정사각형 타일이 산술 강도를 최대화한다.

### 토론 질문
1. TM=8, TN=8에서 K 내부 루프 전체(BK=8 반복)의 총 FMA 수와 총 로드 수는?
2. `reg_C[m][n] += reg_A[m] * reg_B[n]`에서 `reg_A[m]`은 몇 번 재사용되는가?
3. 레지스터 수가 255를 초과하면 어떤 SASS 명령어가 나타나는가?

---

## 시뮬레이션 8: 파이프라인 타임라인 빌더

**모듈**: 08 (Level 4 -- 더블 버퍼링)
**유형**: 간트 차트
**핵심 학습**: 레이턴시 중첩, 단일 버퍼 vs 이중 버퍼 비교

### 필요 도구
- 모눈종이 또는 스프레드시트 (가로축 = 시간, 세로 = Load/Compute 행)

### 수행 절차

**설정**: 로드 시간 = 100 사이클, 연산 시간 = 10 사이클, 동기화 무시, 타일 4개

#### 단일 버퍼 타임라인

```
시간  0        100 110      210 220      310 320      420 430
Load  [===0====]    [===1====]   [===2====]   [===3====]
Comp            [0]          [1]          [2]          [3]
```

- 타일당: Load(100) + Compute(10) = **110 사이클**
- 총합: 4 × 110 = **440 사이클**
- 연산 활용률: 4 × 10 / 440 = **9.1%**

#### 더블 버퍼 타임라인

```
시간  0        100 110      210 220      310 320 330
Load  [===0====][===1====][===2====][===3====]
Comp            [0]       [1]       [2]       [3]
```

- 프롤로그: 100 사이클
- 이후 3 타일: 각 max(100, 10) = 100 사이클
- 마지막 연산: 10 사이클
- 총합: 100 + 3 × 100 + 10 = **410 사이클**

#### 속도 향상 공식

```
단일 버퍼: N × (T_load + T_comp)
이중 버퍼: T_load + N × max(T_load, T_comp)

속도 향상 = 단일 / 이중
```

대규모 N일 때: 속도 향상 ≈ `(T_load + T_comp) / max(T_load, T_comp)`

### 연습 문제

**문제 1**: T_load=400, T_comp=2048, N=512 (실제 SGEMM에 가까운 값)
```
단일: 512 × (400 + 2048) = 1,253,376
이중: 400 + 512 × max(400, 2048) = 400 + 512 × 2048 = 1,048,976
속도 향상: 1,253,376 / 1,048,976 = 1.19 (19% 개선)
```

**문제 2**: T_load=400, T_comp=200, N=512 (연산이 로드보다 짧은 경우)
```
단일: 512 × (400 + 200) = 307,200
이중: 400 + 512 × max(400, 200) = 400 + 512 × 400 = 205,200
속도 향상: 307,200 / 205,200 = 1.50 (50% 개선)
```

### 예상 결과
연산 시간이 로드 시간보다 긴 경우(문제 1) 개선 폭이 제한적이고, 로드 시간이 더 긴 경우(문제 2) 개선 폭이 크다. 이는 더블 버퍼링이 **로드 레이턴시를 숨기는** 기법이기 때문이다.

### 토론 질문
1. T_comp > T_load인 경우, 더블 버퍼링의 이론적 속도 향상 상한은?
2. 트리플 버퍼링(3개 버퍼)으로 확장하면 타임라인이 어떻게 변하는가?
3. 실제 SGEMM에서 T_load와 T_comp의 비율은 BK 값에 어떻게 의존하는가?

---

## 시뮬레이션 9: 비동기 파이프라인 상태 기계

**모듈**: 09 (Level 5 -- 비동기 복사)
**유형**: 상태 다이어그램
**핵심 학습**: 버퍼 순환, commit_group/wait_group 메커니즘

### 필요 도구
- 종이(상태 추적표) 또는 스프레드시트
- 상태 기호: L(Loading), R(Ready), C(Computing), _(Idle)

### 수행 절차

#### 프롤로그 추적

```
초기:          Buf0=_  Buf1=_  Buf2=_    미완료: {}

s=0: async_load(stage=0, tile=0) + commit → G0
              Buf0=L  Buf1=_  Buf2=_    미완료: {G0}

s=1: async_load(stage=1, tile=1) + commit → G1
              Buf0=L  Buf1=L  Buf2=_    미완료: {G0, G1}
```

#### 메인 루프 추적 (6회 반복)

| kt | curr_stage | 프리페치 | 새 그룹 | wait_group(2) 효과 | 연산 버퍼 | 상태 요약 |
|----|-----------|----------|--------|-------------------|----------|---------|
| 0 | 0 | tile2→Buf2 | G2 | G0 완료 보장 | Buf0 | Buf0=C, Buf1=L/R, Buf2=L |
| 1 | 1 | tile3→Buf0 | G3 | G1 완료 보장 | Buf1 | Buf0=L, Buf1=C, Buf2=L/R |
| 2 | 2 | tile4→Buf1 | G4 | G2 완료 보장 | Buf2 | Buf0=L/R, Buf1=L, Buf2=C |
| 3 | 0 | tile5→Buf2 | G5 | G3 완료 보장 | Buf0 | Buf0=C, Buf1=L/R, Buf2=L |
| 4 | 1 | tile6→Buf0 | G6 | G4 완료 보장 | Buf1 | Buf0=L, Buf1=C, Buf2=L/R |
| 5 | 2 | tile7→Buf1 | G7 | G5 완료 보장 | Buf2 | Buf0=L/R, Buf1=L, Buf2=C |

#### 링 버퍼 순환 패턴

```
kt:      0    1    2    3    4    5    6    7    ...
연산:  Buf0 Buf1 Buf2 Buf0 Buf1 Buf2 Buf0 Buf1  ...
프리:  tile2 tile3 tile4 tile5 tile6 tile7 tile8 tile9 ...
목적:  Buf2 Buf0 Buf1 Buf2 Buf0 Buf1 Buf2 Buf0  ...
```

관찰:
1. 연산 버퍼: 0 → 1 → 2 → 0 → 1 → 2 → ... (3단계 순환)
2. 프리페치 목적지는 연산 버퍼보다 항상 2단계 앞선 버퍼
3. 각 버퍼의 생명주기: 로드 → 대기 → 연산 → 다음 로드에 재사용

#### commit/wait 시점 추적

```
프롤로그: commit(G0), commit(G1)

kt=0: commit(G2) → wait(2) → G0 보장
kt=1: commit(G3) → wait(2) → G1 보장
kt=2: commit(G4) → wait(2) → G2 보장
...
kt=6: (프리페치 없음) → wait(2) → G6 보장
kt=7: (프리페치 없음) → wait(2) → G7 보장
```

### 예상 결과
`wait_group(NUM_STAGES-1)`은 항상 정확히 가장 오래된 1개 그룹의 완료를 보장한다. 일반화: `발행된 그룹 수 - wait_group 인자 = 완료 보장 그룹 수`. `NUM_STAGES - (NUM_STAGES - 1) = 1`.

### 토론 질문
1. NUM_STAGES=4로 변경하면 프롤로그에서 몇 개의 타일을 미리 로드해야 하는가?
2. 마지막 타일 근처에서 프리페치할 타일이 없을 때 wait_group(2)의 동작은?
3. `wait_group(0)`은 어떤 의미이며, 언제 사용해야 하는가?

---

## 시뮬레이션 10: 프로파일 데이터 해석

**모듈**: 10 (종합 성능분석 및 프로파일링)
**유형**: 데이터 분석
**핵심 학습**: Nsight Compute 리포트 읽기, 병목 진단, 최적화 방향 결정

### 필요 도구
- 종이 또는 텍스트 에디터
- SOL 해석 매트릭스 (모듈 10 참조)
- 성능 튜닝 의사결정 트리 (모듈 10 참조)

### 수행 절차

모듈 10의 시뮬레이션 섹션에 제시된 3개의 가상 프로파일(커널 A, B, C)을 분석한다.

각 커널에 대해:

**Step 1**: 6개 레벨 중 어느 것에 해당하는가? 판별 근거 3가지 이상 제시.

**판별 지표 가이드:**

| 지표 | Level 0 | Level 1 | Level 2 | Level 3 | Level 4 | Level 5 |
|------|---------|---------|---------|---------|---------|---------|
| 공유 메모리 | 0 KB | 0 KB | ~4-8 KB | ~8-9 KB | ~17 KB | ~26 KB |
| 레지스터/스레드 | 16 | 16-24 | 24-32 | 64-96 | 64-96 | 64-96 |
| SASS 주요 명령 | LDG.E | LDG.E.128 | LDG.E.128+STS | FFMA 지배 | FFMA 지배 | LDGSTS |
| 점유율 | 100% | 75-100% | 50-75% | 25-50% | 25-40% | 25-50% |
| SM SOL | ~5% | ~10% | ~30% | ~60% | ~75% | ~85% |
| 주 스톨 | LongScoreboard | LongScoreboard | LongScoreboard | Mixed | MathPipe↑ | MathPipe↑↑ |

**Step 2**: SOL 매트릭스에서 4가지 상태 중 어디에 해당하는가?

```
SM Low + Mem Low   = Latency Bound
SM Low + Mem High  = Memory Bound
SM High + Mem Low  = Compute Bound (드물다)
SM High + Mem High = Balanced (최적)
```

**Step 3**: 의사결정 트리를 적용하여 다음 최적화 방향을 결정하라.

```
SM Throughput 낮음?
├─ Yes → Memory 높음?
│        ├─ Yes → 데이터 재사용 증가 (타일링, 레지스터 블로킹)
│        └─ No → 레이턴시 숨기기 (프리페치, 더블 버퍼링, cp.async)
└─ No → 최적 상태, micro-optimization
```

### 예상 결과

정답은 모듈 10의 시뮬레이션 섹션에서 `<details>` 태그 내에 제공된다:
- **커널 A**: Level 2 (타일링) → 다음: 레지스터 블로킹
- **커널 B**: Level 4 (더블 버퍼링) → 다음: cp.async
- **커널 C**: Level 0 (나이브) → 다음: 메모리 코얼레싱

### 토론 질문
1. 커널 B에서 Math Pipe Throttle이 1위 스톨인 것은 긍정적 신호인가 부정적 신호인가?
2. 커널 C의 점유율이 100%인데 왜 성능이 최악인가?
3. 실제 프로파일링 시, 이 시뮬레이션과 다르게 나올 수 있는 요인은 무엇인가? (예: L2 캐시 효과, TLB 미스)

---

## 시뮬레이션 난이도 및 의존성 맵

```
Module 01: [시뮬1] Thread-to-Hardware 매핑
                ↓
Module 02: [시뮬2] Driver API 호출 시퀀스
                ↓
Module 03: [시뮬3] FLOP 계산기
                ↓
Module 04: [시뮬4] 메모리 접근 패턴 시각화  ←─── 시뮬1(워프 구조)
                ↓
Module 05: [시뮬5] 코얼레싱 패턴 검사기     ←─── 시뮬4(접근 패턴)
                ↓
Module 06: [시뮬6] 뱅크 충돌 검출기
                ↓
Module 07: [시뮬7] 외적 연산 추적기          ←─── 시뮬3(산술 강도)
                ↓
Module 08: [시뮬8] 파이프라인 타임라인        ←─── 시뮬7(연산/로드 비율)
                ↓
Module 09: [시뮬9] 비동기 파이프라인          ←─── 시뮬8(파이프라이닝)
                ↓
Module 10: [시뮬10] 프로파일 데이터 해석      ←─── 시뮬1-9 전체
```

---

## 추천 학습 순서

1. **필수 경로**: 시뮬레이션 1 → 3 → 4 → 6 → 7 → 8 → 9 → 10 (핵심 최적화 개념)
2. **보조 경로**: 시뮬레이션 2 → 5 (CUDA 프로그래밍 및 코얼레싱 세부사항)
3. **복습 시**: 시뮬레이션 10(프로파일 해석)으로 종합 점검 후, 부족한 부분의 시뮬레이션으로 돌아가 반복
